# Add this service to your existing docker-compose.yml
# (shown here as a standalone file for clarity)

version: "3.9"

services:

  chat:
    build:
      context: ./chainlit-medkg
      dockerfile: Dockerfile
    ports:
      - "8002:8002"
    environment:
      # ── MCP server ─────────────────────────────────────────────────────────
      MCP_SSE_URL: ${MCP_SSE_URL:-http://mcp-server/mcp/sse}

      # ── LLM selection: anthropic | openai | ollama ──────────────────────
      LLM_PROVIDER: ${LLM_PROVIDER:-anthropic}

      # Anthropic
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      ANTHROPIC_MODEL: ${ANTHROPIC_MODEL:-claude-sonnet-4-6}

      # OpenAI
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      OPENAI_MODEL: ${OPENAI_MODEL:-gpt-4o}

      # Ollama (assumes an 'ollama' service on the same network)
      OLLAMA_MODEL: ${OLLAMA_MODEL:-llama3.2}
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL:-http://ollama:11434}

      # Optional: path to custom examples file inside the container
      EXAMPLES_FILE: /app/examples.yaml

    volumes:
      # Mount a custom examples file without rebuilding the image
      - ./examples.yaml:/app/examples.yaml:ro

    networks:
      - medkg-net

    restart: unless-stopped

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/"]
      interval: 30s
      timeout: 10s
      retries: 3


  # ── Uncomment if you don't already have an Ollama service ────────────────
  # ollama:
  #   image: ollama/ollama:latest
  #   volumes:
  #     - ollama-models:/root/.ollama
  #   networks:
  #     - medkg-net
  #   restart: unless-stopped


networks:
  medkg-net:
    external: true   # use your existing network name here


# volumes:
#   ollama-models:
