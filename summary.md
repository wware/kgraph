# Project summary (generated by extract_summary.py)

This file is a concatenated dump of project documentation, config snippets, and extracted docstrings from Python modules. It is intended for search and context (e.g. one place to grep or to feed as context to an AI assistant). Use it to orient and find code; for full text of any file, open the canonical source in the repository. Markdown and config snippets below are truncated (first ~20 lines).

**Start here for an overview:** [CLAUDE.md](CLAUDE.md), [docs/architecture.md](docs/architecture.md), [docs/bundle.md](docs/bundle.md).

---

## Contents

- [CLAUDE.md](#user-content-claudemd)
- [docker-compose.yml](#user-content-docker-composeyml)
- [Dockerfile.jupyter](#user-content-dockerfilejupyter)
- [docs/api.md](#user-content-docsapimd)
- [docs/architecture.md](#user-content-docsarchitecturemd)
- [docs/bundle.md](#user-content-docsbundlemd)
- [docs/canonical_ids.md](#user-content-docscanonicalidsmd)
- [docs/determinism.md](#user-content-docsdeterminismmd)
- [docs/domains.md](#user-content-docsdomainsmd)
- [docs/graph_visualization.md](#user-content-docsgraphvisualizationmd)
- [docs/index.md](#user-content-docsindexmd)
- [docs/pipeline.md](#user-content-docspipelinemd)
- [docs/RELATIONSHIP_TRACING.md](#user-content-docsrelationshiptracingmd)
- [docs/storage.md](#user-content-docsstoragemd)
- [examples/__init__.py](#user-content-examplesinitpy)
- [examples/medlit/CANONICAL_IDS.md](#user-content-examplesmedlitcanonicalidsmd)
- [examples/medlit/documents.py](#user-content-examplesmedlitdocumentspy)
- [examples/medlit/domain.py](#user-content-examplesmedlitdomainpy)
- [examples/medlit/entities.py](#user-content-examplesmedlitentitiespy)
- [examples/medlit_golden/README.md](#user-content-examplesmedlitgoldenreadmemd)
- [examples/medlit_golden/verify.sh](#user-content-examplesmedlitgoldenverifysh)
- [examples/medlit/__init__.py](#user-content-examplesmedlitinitpy)
- [examples/medlit/pipeline/authority_lookup.py](#user-content-examplesmedlitpipelineauthoritylookuppy)
- [examples/medlit/pipeline/canonical_urls.py](#user-content-examplesmedlitpipelinecanonicalurlspy)
- [examples/medlit/pipeline/config.py](#user-content-examplesmedlitpipelineconfigpy)
- [examples/medlit/pipeline/embeddings.py](#user-content-examplesmedlitpipelineembeddingspy)
- [examples/medlit/pipeline/__init__.py](#user-content-examplesmedlitpipelineinitpy)
- [examples/medlit/pipeline/llm_client.py](#user-content-examplesmedlitpipelinellmclientpy)
- [examples/medlit/pipeline/mentions.py](#user-content-examplesmedlitpipelinementionspy)
- [examples/medlit/pipeline/parser.py](#user-content-examplesmedlitpipelineparserpy)
- [examples/medlit/pipeline/pmc_chunker.py](#user-content-examplesmedlitpipelinepmcchunkerpy)
- [examples/medlit/pipeline/pmc_streaming.py](#user-content-examplesmedlitpipelinepmcstreamingpy)
- [examples/medlit/pipeline/relationships.py](#user-content-examplesmedlitpipelinerelationshipspy)
- [examples/medlit/pipeline/resolve.py](#user-content-examplesmedlitpipelineresolvepy)
- [examples/medlit/promotion.py](#user-content-examplesmedlitpromotionpy)
- [examples/medlit/README.md](#user-content-examplesmedlitreadmemd)
- [examples/medlit/relationships.py](#user-content-examplesmedlitrelationshipspy)
- [examples/medlit_schema/base.py](#user-content-examplesmedlitschemabasepy)
- [examples/medlit_schema/DEPTH_OF_FIELDS.md](#user-content-examplesmedlitschemadepthoffieldsmd)
- [examples/medlit_schema/document.py](#user-content-examplesmedlitschemadocumentpy)
- [examples/medlit_schema/domain.py](#user-content-examplesmedlitschemadomainpy)
- [examples/medlit_schema/entity.py](#user-content-examplesmedlitschemaentitypy)
- [examples/medlit_schema/__init__.py](#user-content-examplesmedlitschemainitpy)
- [examples/medlit_schema/ONTOLOGY_GUIDE.md](#user-content-examplesmedlitschemaontologyguidemd)
- [examples/medlit_schema/PROGRESS.md](#user-content-examplesmedlitschemaprogressmd)
- [examples/medlit_schema/README.md](#user-content-examplesmedlitschemareadmemd)
- [examples/medlit_schema/relationship.py](#user-content-examplesmedlitschemarelationshippy)
- [examples/medlit_schema/storage.py](#user-content-examplesmedlitschemastoragepy)
- [examples/medlit/scripts/ingest.py](#user-content-examplesmedlitscriptsingestpy)
- [examples/medlit/scripts/__init__.py](#user-content-examplesmedlitscriptsinitpy)
- [examples/medlit/scripts/parse_pmc_xml.py](#user-content-examplesmedlitscriptsparsepmcxmlpy)
- [examples/medlit/stage_models.py](#user-content-examplesmedlitstagemodelspy)
- [examples/medlit/tests/conftest.py](#user-content-examplesmedlittestsconftestpy)
- [examples/medlit/tests/__init__.py](#user-content-examplesmedlittestsinitpy)
- [examples/medlit/tests/test_authority_lookup.py](#user-content-examplesmedlitteststestauthoritylookuppy)
- [examples/medlit/tests/test_entity_normalization.py](#user-content-examplesmedlitteststestentitynormalizationpy)
- [examples/medlit/tests/test_progress_tracker.py](#user-content-examplesmedlitteststestprogresstrackerpy)
- [examples/medlit/tests/test_promotion_lookup.py](#user-content-examplesmedlitteststestpromotionlookuppy)
- [examples/medlit/TODO.md](#user-content-examplesmedlittodomd)
- [examples/medlit/vocab.py](#user-content-examplesmedlitvocabpy)
- [examples/sherlock/data.py](#user-content-examplessherlockdatapy)
- [examples/sherlock/domain.py](#user-content-examplessherlockdomainpy)
- [examples/sherlock/__init__.py](#user-content-examplessherlockinitpy)
- [examples/sherlock/pipeline/embeddings.py](#user-content-examplessherlockpipelineembeddingspy)
- [examples/sherlock/pipeline/__init__.py](#user-content-examplessherlockpipelineinitpy)
- [examples/sherlock/pipeline/mentions.py](#user-content-examplessherlockpipelinementionspy)
- [examples/sherlock/pipeline/parser.py](#user-content-examplessherlockpipelineparserpy)
- [examples/sherlock/pipeline/relationships.py](#user-content-examplessherlockpipelinerelationshipspy)
- [examples/sherlock/pipeline/resolve.py](#user-content-examplessherlockpipelineresolvepy)
- [examples/sherlock/promotion.py](#user-content-examplessherlockpromotionpy)
- [examples/sherlock/README.md](#user-content-examplessherlockreadmemd)
- [examples/sherlock/scripts/ingest.py](#user-content-examplessherlockscriptsingestpy)
- [examples/sherlock/scripts/__init__.py](#user-content-examplessherlockscriptsinitpy)
- [examples/sherlock/scripts/query.py](#user-content-examplessherlockscriptsquerypy)
- [examples/sherlock/sources/gutenberg.py](#user-content-examplessherlocksourcesgutenbergpy)
- [examples/sherlock/sources/__init__.py](#user-content-examplessherlocksourcesinitpy)
- [.github/copilot-instructions.md](#user-content-githubcopilot-instructionsmd)
- [.github/workflows/test.yml](#user-content-githubworkflowstestyml)
- [holmes_example_plan.md](#user-content-holmesexampleplanmd)
- [IMPLEMENTATION_PLAN.md](#user-content-implementationplanmd)
- [JATS_PARSER_NOTES.md](#user-content-jatsparsernotesmd)
- [jupyter.md](#user-content-jupytermd)
- [kgbundle/kgbundle/__init__.py](#user-content-kgbundlekgbundleinitpy)
- [kgbundle/kgbundle/models.py](#user-content-kgbundlekgbundlemodelspy)
- [kgraph/builders.py](#user-content-kgraphbuilderspy)
- [kgraph/canonical_id/helpers.py](#user-content-kgraphcanonicalidhelperspy)
- [kgraph/canonical_id/__init__.py](#user-content-kgraphcanonicalidinitpy)
- [kgraph/canonical_id/json_cache.py](#user-content-kgraphcanonicalidjsoncachepy)
- [kgraph/canonical_id/lookup.py](#user-content-kgraphcanonicalidlookuppy)
- [kgraph/canonical_id/models.py](#user-content-kgraphcanonicalidmodelspy)
- [kgraph/clock.py](#user-content-kgraphclockpy)
- [kgraph/context.py](#user-content-kgraphcontextpy)
- [kgraph/export.py](#user-content-kgraphexportpy)
- [kgraph/ingest.py](#user-content-kgraphingestpy)
- [kgraph/__init__.py](#user-content-kgraphinitpy)
- [kgraph/logging.py](#user-content-kgraphloggingpy)
- [kgraph/pipeline/caching.py](#user-content-kgraphpipelinecachingpy)
- [kgraph/pipeline/embedding.py](#user-content-kgraphpipelineembeddingpy)
- [kgraph/pipeline/__init__.py](#user-content-kgraphpipelineinitpy)
- [kgraph/pipeline/interfaces.py](#user-content-kgraphpipelineinterfacespy)
- [kgraph/pipeline/streaming.py](#user-content-kgraphpipelinestreamingpy)
- [kgraph/promotion.py](#user-content-kgraphpromotionpy)
- [kgraph/query/__init__.py](#user-content-kgraphqueryinitpy)
- [kgraph/storage/__init__.py](#user-content-kgraphstorageinitpy)
- [kgraph/storage/memory.py](#user-content-kgraphstoragememorypy)
- [kgschema/canonical_id.py](#user-content-kgschemacanonicalidpy)
- [kgschema/document.py](#user-content-kgschemadocumentpy)
- [kgschema/domain.py](#user-content-kgschemadomainpy)
- [kgschema/entity.py](#user-content-kgschemaentitypy)
- [kgschema/__init__.py](#user-content-kgschemainitpy)
- [kgschema/promotion.py](#user-content-kgschemapromotionpy)
- [kgschema/relationship.py](#user-content-kgschemarelationshippy)
- [kgschema/storage.py](#user-content-kgschemastoragepy)
- [kgserver/DOCKER_COMPOSE_GUIDE.md](#user-content-kgserverdockercomposeguidemd)
- [kgserver/Dockerfile](#user-content-kgserverdockerfile)
- [kgserver/DOCKER_SETUP.md](#user-content-kgserverdockersetupmd)
- [kgserver/docs/architecture.md](#user-content-kgserverdocsarchitecturemd)
- [kgserver/GRAPHQL_VIBES.md](#user-content-kgservergraphqlvibesmd)
- [kgserver/index.md](#user-content-kgserverindexmd)
- [kgserver/lint.sh](#user-content-kgserverlintsh)
- [kgserver/LOCAL_DEV.md](#user-content-kgserverlocaldevmd)
- [kgserver/main.py](#user-content-kgservermainpy)
- [kgserver/MCP_GQL_WRAPPER.md](#user-content-kgservermcpgqlwrappermd)
- [kgserver/mcp_server/__init__.py](#user-content-kgservermcpserverinitpy)
- [kgserver/mcp_server/server.py](#user-content-kgservermcpserverserverpy)
- [kgserver/mkdocs.yml](#user-content-kgservermkdocsyml)
- [kgserver/push_main.sh](#user-content-kgserverpushmainsh)
- [kgserver/query/bundle_loader.py](#user-content-kgserverquerybundleloaderpy)
- [kgserver/query/graphql_examples.py](#user-content-kgserverquerygraphqlexamplespy)
- [kgserver/query/graphql_examples.yml](#user-content-kgserverquerygraphqlexamplesyml)
- [kgserver/query/graphql_schema.py](#user-content-kgserverquerygraphqlschemapy)
- [kgserver/query/graph_traversal.py](#user-content-kgserverquerygraphtraversalpy)
- [kgserver/query/__init__.py](#user-content-kgserverqueryinitpy)
- [kgserver/query/.ipynb_checkpoints/README-checkpoint.md](#user-content-kgserverqueryipynbcheckpointsreadme-checkpointmd)
- [kgserver/query/routers/graph_api.py](#user-content-kgserverqueryroutersgraphapipy)
- [kgserver/query/routers/graphiql_custom.py](#user-content-kgserverqueryroutersgraphiqlcustompy)
- [kgserver/query/routers/rest_api.py](#user-content-kgserverqueryroutersrestapipy)
- [kgserver/query/server.py](#user-content-kgserverqueryserverpy)
- [kgserver/query/storage_factory.py](#user-content-kgserverquerystoragefactorypy)
- [kgserver/storage/backends/__init__.py](#user-content-kgserverstoragebackendsinitpy)
- [kgserver/storage/backends/postgres.py](#user-content-kgserverstoragebackendspostgrespy)
- [kgserver/storage/backends/README.md](#user-content-kgserverstoragebackendsreadmemd)
- [kgserver/storage/backends/sqlite.py](#user-content-kgserverstoragebackendssqlitepy)
- [kgserver/storage/__init__.py](#user-content-kgserverstorageinitpy)
- [kgserver/storage/interfaces.py](#user-content-kgserverstorageinterfacespy)
- [kgserver/storage/models/bundle.py](#user-content-kgserverstoragemodelsbundlepy)
- [kgserver/storage/models/entity.py](#user-content-kgserverstoragemodelsentitypy)
- [kgserver/storage/models/evidence.py](#user-content-kgserverstoragemodelsevidencepy)
- [kgserver/storage/models/__init__.py](#user-content-kgserverstoragemodelsinitpy)
- [kgserver/storage/models/paper.py](#user-content-kgserverstoragemodelspaperpy)
- [kgserver/storage/models/README.md](#user-content-kgserverstoragemodelsreadmemd)
- [kgserver/storage/models/relationship.py](#user-content-kgserverstoragemodelsrelationshippy)
- [kgserver/storage/NEO4J_COMPATIBILITY.md](#user-content-kgserverstorageneo4jcompatibilitymd)
- [kgserver/storage/README.md](#user-content-kgserverstoragereadmemd)
- [kgserver/tests/conftest.py](#user-content-kgservertestsconftestpy)
- [kgserver/tests/test_bundle_loader.py](#user-content-kgserverteststestbundleloaderpy)
- [kgserver/tests/test_graph_api.py](#user-content-kgserverteststestgraphapipy)
- [kgserver/tests/test_graphql_schema.py](#user-content-kgserverteststestgraphqlschemapy)
- [kgserver/tests/test_rest_api.py](#user-content-kgserverteststestrestapipy)
- [kgserver/tests/test_storage_backends.py](#user-content-kgserverteststeststoragebackendspy)
- [kgserver/tests/test_storage_factory.py](#user-content-kgserverteststeststoragefactorypy)
- [LAMBDA_LABS.md](#user-content-lambdalabsmd)
- [lint.sh](#user-content-lintsh)
- [medlit_bundle/docs/README.md](#user-content-medlitbundledocsreadmemd)
- [medlit_bundle/graphql_examples.yml](#user-content-medlitbundlegraphqlexamplesyml)
- [MEDLIT_SCHEMA_SPEC.md](#user-content-medlitschemaspecmd)
- [NEXT_STEPS.md](#user-content-nextstepsmd)
- [push_main.sh](#user-content-pushmainsh)
- [README.md](#user-content-readmemd)
- [run-ingest.sh](#user-content-run-ingestsh)
- [snapshot_semantics_v1.md](#user-content-snapshotsemanticsv1md)
- [STATUS_20260212.md](#user-content-status20260212md)
- [tests/conftest.py](#user-content-testsconftestpy)
- [tests/__init__.py](#user-content-testsinitpy)
- [tests/test_caching.py](#user-content-teststestcachingpy)
- [tests/test_canonical_id.py](#user-content-teststestcanonicalidpy)
- [tests/test_context_and_builders.py](#user-content-teststestcontextandbuilderspy)
- [tests/test_entities.py](#user-content-teststestentitiespy)
- [tests/test_evidence_semantic.py](#user-content-teststestevidencesemanticpy)
- [tests/test_evidence_traceability.py](#user-content-teststestevidencetraceabilitypy)
- [tests/test_export.py](#user-content-teststestexportpy)
- [tests/test_git_hash.py](#user-content-teststestgithashpy)
- [tests/test_ingestion.py](#user-content-teststestingestionpy)
- [tests/test_logging.py](#user-content-teststestloggingpy)
- [tests/test_medlit_domain.py](#user-content-teststestmedlitdomainpy)
- [tests/test_medlit_entities.py](#user-content-teststestmedlitentitiespy)
- [tests/test_medlit_relationships.py](#user-content-teststestmedlitrelationshipspy)
- [tests/test_paper_model.py](#user-content-teststestpapermodelpy)
- [tests/test_pipeline_integration.py](#user-content-teststestpipelineintegrationpy)
- [tests/test_pmc_chunker.py](#user-content-teststestpmcchunkerpy)
- [tests/test_pmc_streaming.py](#user-content-teststestpmcstreamingpy)
- [tests/test_promotion_merge.py](#user-content-teststestpromotionmergepy)
- [tests/test_promotion.py](#user-content-teststestpromotionpy)
- [tests/test_relationships.py](#user-content-teststestrelationshipspy)
- [tests/test_relationship_swap.py](#user-content-teststestrelationshipswappy)
- [tests/test_streaming.py](#user-content-teststeststreamingpy)
- [VIBES.md](#user-content-vibesmd)

---

<span id="user-content-claudemd"></span>

# CLAUDE.md


This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Knowledge graph system for extracting entities and relationships from documents across multiple knowledge domains (medical literature, legal documents, academic CS papers, etc.). The architecture uses a two-pass ingestion process:

1. **Pass 1 (Entity Extraction)**: Extract entities from documents, assign canonical IDs where appropriate (UMLS for medical, DBPedia URIs cross-domain, etc.)
2. **Pass 2 (Relationship Extraction)**: Identify edges/relationships between entities, produce per-document JSON with edges and provisional entities

### Key Concepts

- **Canonical entities**: Assigned stable IDs from authoritative sources
- **Provisional entities**: Mentions awaiting promotion based on usage count and confidence scores
- **Entity promotion**: Provisional → canonical when usage thresholds are met
- **Entity merging**: Combining canonical entities detected as duplicates via semantic vector similarity

## Build & Test Commands

    ...


<span id="user-content-docker-composeyml"></span>

# docker-compose.yml

```yaml
services:
  api:
    build:
      context: .
      dockerfile: kgserver/Dockerfile
    stdin_open: true
    tty: true
    environment:
      DATABASE_URL: postgresql://postgres:postgres@postgres:5432/kgserver
      BUNDLE_PATH: /bundle
    #volumes:
    #  # - /home/wware/S.zip:/bundle/S.zip:ro
    #  - ./medlit_bundle:/bundle:ro
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    networks:

    ...
```


<span id="user-content-dockerfilejupyter"></span>

# Dockerfile.jupyter

```dockerfile
FROM python:3.12-slim

# Create jovyan user (matches Jupyter convention)
RUN useradd -m -s /bin/bash -u 1000 jovyan

# Install system dependencies
RUN apt-get update && apt-get install -y \
    git \
    postgresql-client \
    curl \
    && rm -rf /var/lib/apt/lists/*

RUN chmod -R ugo+w /usr/local
USER jovyan
WORKDIR /home/jovyan

# Install uv for jovyan user and add to PATH
RUN curl -LsSf https://astral.sh/uv/install.sh | sh

ENV PATH="$PATH:/home/jovyan/.local/bin"

    ...
```


<span id="user-content-docsapimd"></span>

# docs/api.md

# API Reference

## Core Classes

### kgraph.entity

#### EntityStatus

```python
class EntityStatus(str, Enum):
    CANONICAL = "canonical"    # Stable ID from authoritative source
    PROVISIONAL = "provisional" # Awaiting promotion
```

#### BaseEntity

Abstract base for all domain entities.

| Field | Type | Description |
|-------|------|-------------|

    ...


<span id="user-content-docsarchitecturemd"></span>

# docs/architecture.md

# Architecture Overview

## Two-Pass Ingestion Pipeline

The framework processes documents in two passes:

```
┌─────────────┐     ┌─────────────┐     ┌─────────────────┐
│  Raw Docs   │────▶│   Parser    │────▶│  BaseDocument   │
└─────────────┘     └─────────────┘     └────────┬────────┘
                                                 │
                        ┌────────────────────────┘
                        ▼
┌─────────────────────────────────────────────────────────┐
│                      PASS 1                             │
│  ┌─────────────────┐     ┌──────────────────┐           │
│  │ Entity Extractor│────▶│  Entity Resolver │           │
│  └─────────────────┘     └────────┬─────────┘           │
│         │                         │                     │
│         ▼                         ▼                     │
```

    ...


<span id="user-content-docsbundlemd"></span>

# docs/bundle.md

# Bundle format (kgraph → kgserver)

A **bundle** is the finalized, validated artifact produced by domain-specific pipelines (kgraph) and consumed by the domain-neutral server (kgserver).

**Bundles are a strict contract.**
- Producer pipelines must export a bundle that already matches the server schema.
- The server loads bundles as-is and should fail fast if a bundle is invalid.
- Do not rely on the server to rename fields, interpret metadata, or infer structure.

This document specifies the **bundle file layout**, **manifest schema**, and **row formats**.

## Bundle Models Package

The bundle format is defined by Pydantic models in the **kgbundle** package, a lightweight standalone package with minimal dependencies (pydantic only). This package is used by both:

- **kgraph** (producer): exports bundles using `kgbundle.EntityRow`, `kgbundle.RelationshipRow`, `kgbundle.BundleManifestV1`
- **kgserver** (consumer): loads bundles using the same models

Example:
```python
```

    ...


<span id="user-content-docscanonicalidsmd"></span>

# docs/canonical_ids.md

# Canonical IDs

Canonical IDs are stable identifiers from authoritative sources (UMLS, MeSH, HGNC, RxNorm, UniProt, DBPedia, etc.) that uniquely identify entities across different knowledge bases. The `kgraph` framework provides abstractions for working with canonical IDs throughout the ingestion pipeline.

## Overview

The canonical ID system consists of:

1. **`CanonicalId`** - A Pydantic model representing a canonical identifier with ID, URL, and synonyms
2. **`CanonicalIdCacheInterface`** - Abstract interface for caching canonical ID lookups
3. **`CanonicalIdLookupInterface`** - Abstract interface for looking up canonical IDs
4. **Helper functions** - Utilities for extracting canonical IDs from entities

These abstractions enable:
- **Consistent handling** of canonical IDs across different domains
- **Flexible caching** strategies (JSON file, database, etc.)
- **Reusable promotion logic** that works with any lookup implementation
- **URL generation** for canonical entities

## CanonicalId Model

    ...


<span id="user-content-docsdeterminismmd"></span>

# docs/determinism.md

# Determinism and Reproducibility

This document addresses concerns about non-deterministic behavior in the knowledge graph ingestion pipeline, particularly regarding LLM usage and provenance tracking.

## Current State

### LLM Temperature Settings

- **Current temperature**: `0.1` (default in `OllamaLLMClient`)
- **Impact**: Even at low temperature, LLMs can exhibit non-deterministic behavior due to:
  - Floating-point precision differences
  - Model implementation details
  - Hardware differences (CPU vs GPU)
  - Model version changes

### Non-Deterministic Sources

1. **LLM Generation** (entity extraction, relationship extraction)
   - Temperature = 0.1 (not fully deterministic)
   - No seed parameter currently set

    ...


<span id="user-content-docsdomainsmd"></span>

# docs/domains.md

# Implementing a Domain

Each knowledge domain (medical, legal, CS papers, etc.) defines its own entity types, relationship types, and validation rules by implementing `DomainSchema`.

## Step 1: Define Entity Types

Create entity classes by extending `BaseEntity`:

```python
from datetime import datetime
from kgschema.entity import BaseEntity, EntityStatus

class PersonEntity(BaseEntity):
    """A person in the legal domain."""

    # Additional domain-specific fields can be added
    bar_number: str | None = None

    def get_entity_type(self) -> str:
        return "person"
```

    ...


<span id="user-content-docsgraphvisualizationmd"></span>

# docs/graph_visualization.md

# Force-Directed Graph Visualization

Interactive graph visualization for kgserver using D3.js force simulation.

## Overview

This feature provides an interactive graph visualization accessible via a REST endpoint and static HTML page. The design separates data retrieval (API) from rendering (client-side JS) for flexibility and extensibility.

## Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                         Client Browser                          │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  /graph-viz (Static HTML + D3.js)                        │  │
│  │  - Force simulation rendering                            │  │
│  │  - Click handlers for node/edge details                  │  │
│  │  - Controls: center entity, hop depth, layout options    │  │
│  └──────────────────────────────────────────────────────────┘  │
│                              │                                  │
```

    ...


<span id="user-content-docsindexmd"></span>

# docs/index.md

# Knowledge Graph Framework

A domain-agnostic framework for building knowledge graphs from documents. Supports entity extraction, relationship mapping, and a two-pass ingestion pipeline that works across any knowledge domain (medical, legal, academic, etc.).

## Quick Start

```bash
# Install
uv venv && source .venv/bin/activate
uv pip install -e ".[dev]"

# Run tests
uv run pytest
```

## Core Concepts

### Entities

Entities are the nodes in your knowledge graph. They can be:

    ...


<span id="user-content-docspipelinemd"></span>

# docs/pipeline.md

# Pipeline Components

The knowledge graph ingestion pipeline uses a **two-pass architecture** to transform raw documents into structured knowledge:

1. **Pass 1 (Entity Extraction)**: Parse documents, extract entity mentions, and resolve them to canonical or provisional entities.
2. **Pass 2 (Relationship Extraction)**: Identify relationships (edges) between resolved entities within each document.

This separation allows the system to build a consistent entity vocabulary before attempting relationship extraction, which improves accuracy and enables cross-document entity linking.

The pipeline consists of pluggable components for parsing, extraction, resolution, and embedding generation. Each component is defined as an abstract interface, allowing domain-specific implementations.

For a complete, real-world example, see the `examples/sherlock` directory.

## Component Interfaces

Each interface is designed to be stateless and async-first, enabling parallel processing and easy testing with mock implementations.

### DocumentParserInterface

Converts raw document bytes into structured `BaseDocument` instances. This is the entry point for document ingestion—the parser handles format detection, content extraction, and metadata identification.

    ...


<span id="user-content-docsrelationshiptracingmd"></span>

# docs/RELATIONSHIP_TRACING.md

# Relationship tracing works now

The machinery for tracing relationship ingestion (and the decision to
keep or discard a relationship) is now working.

```bash
$ cd /home/wware/kgraph && rm -f /tmp/kgraph-relationship-traces/*.json && uv run python -m examples.medlit.scripts.ingest --input-dir examples/medlit/pmc_xmls/
    --limit 1 --use-ollama --ollama-timeout 1200 --stop-after relationships 2>&1 | tee /tmp/ingest_output.txt
```

Let me check the trace file. Read /tmp/kgraph-relationship-traces/PMC10759991.relationships.trace.json

The trace is working. Here's what it captured. LLM proposed 6 relationships:

  1. gene-expression profiling → treats → breast cancer (REJECTED by domain validation - procedures don't treat)
  2. luminal A → associated_with → breast cancer ✓
  3. luminal B → associated_with → breast cancer ✓
  4. HER2-enriched → associated_with → breast cancer ✓
  5. basal-like → associated_with → breast cancer ✓
  6. breast cancer → associated_with → gene-expression profiling ✓

    ...


<span id="user-content-docsstoragemd"></span>

# docs/storage.md

# Storage Backends

The framework defines storage interfaces for entities, relationships, and documents. These interfaces decouple the knowledge graph core from specific persistence technologies, enabling you to:

- Use **in-memory storage** for testing and development
- Deploy with **relational databases** (PostgreSQL, MySQL) for ACID guarantees
- Leverage **vector databases** (Pinecone, Weaviate, Qdrant) for embedding-based similarity search
- Use **graph databases** (Neo4j, ArangoDB) for optimized relationship traversal

All interfaces are async-first to support non-blocking I/O with modern database drivers.

## Storage Interfaces

Each interface is designed to support the complete entity lifecycle and knowledge graph operations.

### EntityStorageInterface

The primary persistence layer for knowledge graph nodes. Supports:
- Basic CRUD operations (add, get, update, delete)
- Batch retrieval for efficient bulk operations

    ...


<span id="user-content-examplesinitpy"></span>

# examples/__init__.py



<span id="user-content-examplesmedlitcanonicalidsmd"></span>

# examples/medlit/CANONICAL_IDS.md

# Canonical ID Lookup for Medical Entities

This document describes how the medlit pipeline acquires canonical IDs from authoritative medical ontology sources for entities like diseases, genes, drugs, and proteins.

## Overview

Medical knowledge graphs require standardized identifiers to link entities across different papers and databases. The medlit pipeline uses a two-pronged approach:

1. **During Extraction**: The LLM can call a lookup tool to find canonical IDs while extracting entities from text
2. **During Promotion**: Provisional entities without canonical IDs can be enriched via API lookup before promotion

## Supported Ontologies

| Entity Type | Ontology | ID Format | Example |
|-------------|----------|-----------|---------|
| Disease | UMLS or MeSH* | C + digits or `MeSH:D` + digits | `C0006142` or `MeSH:D001943` |
| Gene | HGNC (HUGO Gene Nomenclature Committee) | `HGNC:` + number | `HGNC:1100` (BRCA1) |
| Drug | RxNorm | `RxNorm:` + number | `RxNorm:161` (aspirin) |
| Protein | UniProt | P/Q + 5 alphanumeric | `P38398` (BRCA1 protein) |

    ...


<span id="user-content-examplesmedlitdocumentspy"></span>

# examples/medlit/documents.py

Journal article document representation for medical literature domain.

## `class JournalArticle(BaseDocument)`

A journal article (research paper) as a source document for extraction.

Maps from med-lit-schema's Paper model to kgraph's BaseDocument.
Papers are NOT the same as doc_assets.jsonl (which is for documentation assets).
Papers are the source of information for building the knowledge graph, taken
from sources like PubMed, PLOS ONE, or medical journals if available.

Key mappings:
- Paper.paper_id → BaseDocument.document_id (prefer doi:, else pmid:, else stable hash)
- Paper.title → BaseDocument.title
- Paper.abstract + (optional full text) → BaseDocument.content
- PaperMetadata → BaseDocument.metadata (study type, sample size, journal, etc.)
- Paper.extraction_provenance → BaseDocument.metadata["extraction"]

### `def JournalArticle.get_document_type(self) -> str`

Return domain-specific document type.

### `def JournalArticle.get_sections(self) -> list[tuple[str, str]]`

Return document sections as (section_name, content) tuples.

For journal articles, we typically have:
- title: The paper title
- abstract: The abstract text
- body: The full text content (if available)

### `def JournalArticle.study_type(self) -> str | None`

Convenience property for accessing study_type from metadata.

### `def JournalArticle.sample_size(self) -> int | None`

Convenience property for accessing sample_size from metadata.

### `def JournalArticle.mesh_terms(self) -> list[str]`

Convenience property for accessing mesh_terms from metadata.


<span id="user-content-examplesmedlitdomainpy"></span>

# examples/medlit/domain.py

Domain schema for medical literature knowledge graph.

## `class MedLitDomainSchema(DomainSchema)`

Domain schema for medical literature extraction.

Defines the vocabulary and validation rules for extracting medical knowledge
from journal articles. Uses canonical IDs (UMLS, HGNC, RxNorm, UniProt) for
entity identification and supports rich relationship metadata with evidence
and provenance tracking.

### `def MedLitDomainSchema.promotion_config(self) -> PromotionConfig`

Medical domain promotion configuration.

Lowered thresholds to match LLM extraction characteristics:
- min_usage_count=1: Entities appear once per paper
- min_confidence=0.4: LLM typically returns ~0.47 confidence
- require_embedding=False: Don't block promotion if embeddings not ready

### `def MedLitDomainSchema.validate_entity(self, entity: BaseEntity) -> list[ValidationIssue]`

Validate an entity against medical domain rules.

Rules:
- Entity type must be registered
- Canonical entities should have canonical IDs in entity_id or canonical_ids
- Provisional entities are allowed (they'll be promoted later)

### `def MedLitDomainSchema.get_valid_predicates(self, subject_type: str, object_type: str) -> list[str]`

Return predicates valid between two entity types.

Uses the vocabulary validation function to enforce domain-specific
constraints on which relationships are semantically valid.

### `def MedLitDomainSchema.get_promotion_policy(self, lookup: CanonicalIdLookup | None = None) -> PromotionPolicy`

Return the promotion policy for medical literature domain.

Uses MedLitPromotionPolicy which assigns canonical IDs based on
authoritative medical ontologies (UMLS, HGNC, RxNorm, UniProt).

Args:
    lookup: Optional canonical ID lookup service. If None, a new
            instance will be created (without UMLS API key unless
            set in environment).


<span id="user-content-examplesmedlitentitiespy"></span>

# examples/medlit/entities.py

Medical entity types for the knowledge graph.

## `class DiseaseEntity(BaseEntity)`

Represents medical conditions, disorders, and syndromes.

Uses UMLS as the primary identifier system with additional mappings to
MeSH and ICD-10 for interoperability with clinical systems.

Mapping from med-lit-schema:
- Disease.entity_id (UMLS ID) → BaseEntity.entity_id
- Disease.umls_id → BaseEntity.canonical_ids["umls"]
- Disease.mesh_id → BaseEntity.canonical_ids["mesh"]
- Disease.icd10_codes → BaseEntity.metadata["icd10_codes"]
- Disease.category → BaseEntity.metadata["category"]

## `class GeneEntity(BaseEntity)`

Represents genes and their genomic information.

Uses HGNC (HUGO Gene Nomenclature Committee) as the primary identifier
with additional mappings to NCBI Entrez Gene.

Mapping from med-lit-schema:
- Gene.entity_id (HGNC ID) → BaseEntity.entity_id
- Gene.hgnc_id → BaseEntity.canonical_ids["hgnc"]
- Gene.entrez_id → BaseEntity.canonical_ids["entrez"]
- Gene.symbol → BaseEntity.metadata["symbol"]
- Gene.chromosome → BaseEntity.metadata["chromosome"]

## `class DrugEntity(BaseEntity)`

Represents medications and therapeutic substances.

Uses RxNorm as the primary identifier for standardized medication naming.

Mapping from med-lit-schema:
- Drug.entity_id (RxNorm ID) → BaseEntity.entity_id
- Drug.rxnorm_id → BaseEntity.canonical_ids["rxnorm"]
- Drug.brand_names → BaseEntity.metadata["brand_names"]
- Drug.drug_class → BaseEntity.metadata["drug_class"]
- Drug.mechanism → BaseEntity.metadata["mechanism"]

## `class ProteinEntity(BaseEntity)`

Represents proteins and their biological functions.

Uses UniProt as the primary identifier for protein sequences and annotations.

Mapping from med-lit-schema:
- Protein.entity_id (UniProt ID) → BaseEntity.entity_id
- Protein.uniprot_id → BaseEntity.canonical_ids["uniprot"]
- Protein.gene_id → BaseEntity.metadata["gene_id"]
- Protein.function → BaseEntity.metadata["function"]
- Protein.pathways → BaseEntity.metadata["pathways"]

## `class SymptomEntity(BaseEntity)`

Represents clinical signs and symptoms.

## `class ProcedureEntity(BaseEntity)`

Represents medical tests, diagnostics, treatments.

## `class BiomarkerEntity(BaseEntity)`

Represents measurable indicators.

## `class PathwayEntity(BaseEntity)`

Represents biological pathways.

## `class LocationEntity(BaseEntity)`

Represents geographic locations relevant to epidemiological analysis.

Used for tracking disease prevalence by region, endemic diseases, and
geographic health disparities. Uses provisional IDs initially; canonical
IDs could come from GeoNames or ISO country codes in the future.

## `class EthnicityEntity(BaseEntity)`

Represents ethnic or population groups for epidemiological analysis.

Used for tracking genetic predispositions, health disparities, and
population-specific disease risk factors. Uses provisional IDs initially;
canonical IDs could come from standardized ethnicity codes in the future.


<span id="user-content-examplesmedlitgoldenreadmemd"></span>

# examples/medlit_golden/README.md

# MedLit Golden Example

This directory provides a "golden" example of a two-pass ingestion pipeline using the MedLit schema. It demonstrates how a small piece of text is processed to extract canonical entities, evidence, and relationships.

## Scenario

The input is a mini-abstract about the drug Olaparib and its use in treating BRCA-mutated breast cancer.

-   **Input**: `input/PMC999_abstract.txt`

## Expected Output

The ingestion process is expected to run in two passes:

1.  **Pass 1: Entity and Evidence Extraction**
    -   Extracts canonical entities for the drug, disease, and genes mentioned.
    -   Extracts a `Paper` entity representing the abstract itself.
    -   Extracts an `Evidence` entity that captures the context of the claim (e.g., that this is from an RCT with a specific sample size).
    -   **Output**: `expected/pass1_entities.jsonl` and `expected/pass1_evidence.jsonl`

    ...


<span id="user-content-examplesmedlitgoldenverifysh"></span>

# examples/medlit_golden/verify.sh

```sh
#!/bin/bash

# This script is a placeholder for the full pipeline verification.
# In a real scenario, this would trigger the ingestion pipeline and
# then compare the output with the golden files.

set -e

INPUT_DIR="input"
EXPECTED_DIR="expected"
OUTPUT_DIR="output"

echo "INFO: Running verification for MedLit golden example..."

# 1. Simulate pipeline run (in a real scenario, this would be the actual pipeline command)
echo "INFO: Simulating pipeline run..."
mkdir -p $OUTPUT_DIR
cp $EXPECTED_DIR/pass1_entities.jsonl $OUTPUT_DIR/pass1_entities.jsonl
cp $EXPECTED_DIR/pass1_evidence.jsonl $OUTPUT_DIR/pass1_evidence.jsonl
cp $EXPECTED_DIR/pass2_relationships.jsonl $OUTPUT_DIR/pass2_relationships.jsonl

    ...
```


<span id="user-content-examplesmedlitinitpy"></span>

# examples/medlit/__init__.py

Medical literature domain extension for kgraph.

This package provides domain-specific types and pipeline components for
extracting knowledge from biomedical journal articles.


<span id="user-content-examplesmedlitpipelineauthoritylookuppy"></span>

# examples/medlit/pipeline/authority_lookup.py

Canonical ID lookup from medical ontology authorities.

Provides lookup functionality for canonical IDs from various medical ontology
sources: UMLS, HGNC, RxNorm, and UniProt.

Features persistent caching to avoid repeated API calls across runs.

## `class CanonicalIdLookup(CanonicalIdLookupInterface)`

Look up canonical IDs from various medical ontology authorities.

Supports lookup from:
- UMLS (diseases, symptoms, procedures)
- HGNC (genes)
- RxNorm (drugs)
- UniProt (proteins)

Features persistent caching to disk to avoid repeated API calls across runs.

### `def CanonicalIdLookup.__init__(self, umls_api_key: Optional[str] = None, cache_file: Optional[Path] = None, embedding_generator: Any = None, similarity_threshold: float = 0.5)`

Initialize the canonical ID lookup service.

Args:
    umls_api_key: Optional UMLS API key. If not provided, will try to
                 read from UMLS_API_KEY environment variable.
    cache_file: Optional path to cache file. If not provided, defaults
               to "canonical_id_cache.json" in current directory.
    embedding_generator: Optional; if set, used to rerank multiple candidates
                        (UMLS/MeSH) by cosine similarity to the search term.
                        Must have async generate(text: str) -> tuple[float, ...].
    similarity_threshold: Min cosine similarity when using embedding rerank (0-1).

### `def CanonicalIdLookup._save_cache(self, force: bool = False) -> None`

Save cache to disk.

Args:
    force: If True, save even if cache is not marked dirty (for emergency saves).

### `def CanonicalIdLookup._normalize_mesh_search_terms(self, term: str) -> list[str]`

Generate normalized search terms for MeSH lookup.

MeSH uses formal terminology, so we normalize common informal terms.
Returns a list of search terms to try, in order of preference.

Args:
    term: Original search term

Returns:
    List of normalized search terms (original first, then normalized variants)

### `def CanonicalIdLookup._extract_mesh_id_from_results(self, data: list, search_terms: str | list[str]) -> Optional[str]`

Extract MeSH descriptor ID from API results, preferring best matches.

Scores results based on how well they match any of the provided search terms.
This allows normalized terms (e.g., "breast neoplasms") to score well even
when the original search was "breast cancer".

Scoring strategy:
1. Exact match (case-insensitive) gets highest score
2. Exact word match (all words present) gets high score
3. Prefer shorter labels (more general terms) over longer ones (complications)
4. Prefer matches where term is at the start of the label
5. Penalize matches that are much longer than the search term (likely complications)
6. Prefer matches to earlier search terms (original > normalized)

Args:
    data: List of result dictionaries from MeSH API
    search_terms: Single search term (str) or list of search terms tried
                 (original first, then normalized variants). If a single string
                 is provided, it's treated as the only search term.

### `def CanonicalIdLookup._dbpedia_label_matches(self, term: str, label: str) -> bool`

Check if a DBPedia label is a good match for the search term.

### `def CanonicalIdLookup._extract_authoritative_id_from_dbpedia_sync(self, client: 'httpx.Client', dbpedia_id: str, entity_type: str, original_term: str) -> Optional[str]`

Synchronous version of authoritative ID extraction from DBPedia.

Args:
    client: Synchronous HTTP client
    dbpedia_id: DBPedia ID in format "DBPedia:ResourceName"
    entity_type: Type of entity (disease, gene, drug, protein, etc.)
    original_term: Original search term for caching

Returns:
    Authoritative canonical ID if found, None otherwise

### `def CanonicalIdLookup._lookup_mesh_by_id_sync(self, mesh_id: str) -> Optional[str]`

Sync version: Look up MeSH ID by known ID.

### `def CanonicalIdLookup._lookup_umls_by_id_sync(self, umls_id: str) -> Optional[str]`

Sync version: Look up UMLS CUI by known ID.

### `def CanonicalIdLookup._lookup_hgnc_by_id_sync(self, hgnc_id: str) -> Optional[str]`

Sync version: Look up HGNC ID by known ID.

### `def CanonicalIdLookup._lookup_rxnorm_by_id_sync(self, rxnorm_id: str) -> Optional[str]`

Sync version: Look up RxNorm ID by known ID.

### `def CanonicalIdLookup._lookup_uniprot_by_id_sync(self, uniprot_id: str) -> Optional[str]`

Sync version: Look up UniProt ID by known ID.

### `def CanonicalIdLookup.lookup_sync(self, term: str, entity_type: str) -> Optional[CanonicalId]`

Synchronous lookup (interface method).

Args:
    term: The entity name/mention text
    entity_type: Type of entity (disease, gene, drug, protein, etc.)

Returns:
    CanonicalId if found, None otherwise

### `def CanonicalIdLookup.lookup_canonical_id_sync(self, term: str, entity_type: str) -> Optional[str]`

Synchronous wrapper for use as Ollama tool.

This is needed because Ollama tool functions must be synchronous.
Uses the cache first, then makes synchronous HTTP calls if needed.

Args:
    term: The entity name/mention text
    entity_type: Type of entity (disease, gene, drug, protein, etc.)

Returns:
    Canonical ID string if found, None otherwise

### `def CanonicalIdLookup._lookup_umls_sync(self, client: 'httpx.Client', term: str) -> Optional[str]`

Synchronous UMLS lookup with MeSH fallback.

### `def CanonicalIdLookup._lookup_mesh_sync(self, client: 'httpx.Client', term: str) -> Optional[str]`

Synchronous MeSH lookup with term normalization.

Uses the same multi-term approach as async version.

### `def CanonicalIdLookup._lookup_hgnc_sync(self, client: 'httpx.Client', term: str) -> Optional[str]`

Synchronous HGNC lookup with alias fallback.

### `def CanonicalIdLookup._lookup_rxnorm_sync(self, client: 'httpx.Client', term: str) -> Optional[str]`

Synchronous RxNorm lookup.

### `def CanonicalIdLookup._lookup_uniprot_sync(self, client: 'httpx.Client', term: str) -> Optional[str]`

Synchronous UniProt lookup.

### `def CanonicalIdLookup._lookup_dbpedia_sync(self, client: 'httpx.Client', term: str) -> Optional[str]`

Synchronous DBPedia lookup as fallback with validation.


<span id="user-content-examplesmedlitpipelinecanonicalurlspy"></span>

# examples/medlit/pipeline/canonical_urls.py

Utility functions for constructing canonical URLs from entity canonical IDs.

### `def build_canonical_url(canonical_id: str, entity_type: Optional[str] = None) -> Optional[str]`

Build a canonical URL for an entity based on its canonical ID.

Supports:
- DBPedia: https://dbpedia.org/page/{entity_name}
- MeSH: https://meshb.nlm.nih.gov/record/ui?ui={ID}
- UniProt: https://www.uniprot.org/uniprotkb/{ID}
- HGNC: https://www.genenames.org/data/gene-symbol-report/#!/hgnc_id/{ID}
- UMLS: https://uts.nlm.nih.gov/uts/umls/concept/{ID}
- RxNorm: https://www.nlm.nih.gov/research/umls/rxnorm/overview.html (no direct link, returns None)

Args:
    canonical_id: The canonical ID (e.g., "MeSH:D000570", "UniProt:P38398", "HGNC:1100")
    entity_type: Optional entity type hint (e.g., "disease", "gene", "protein")

Returns:
    URL string if a link can be constructed, None otherwise.

### `def build_canonical_urls_from_dict(canonical_ids: dict[str, str], entity_type: Optional[str] = None) -> dict[str, str]`

Build canonical URLs for all canonical IDs in a dictionary.

Args:
    canonical_ids: Dictionary mapping source names to canonical IDs
        (e.g., {"umls": "C0006142", "mesh": "MeSH:D000570"})
    entity_type: Optional entity type hint

Returns:
    Dictionary mapping source names to URLs (e.g., {"umls": "https://...", "mesh": "https://..."})


<span id="user-content-examplesmedlitpipelineconfigpy"></span>

# examples/medlit/pipeline/config.py

Load medlit pipeline config from TOML (e.g. medlit.toml).

Config file is looked up in order:
  1. Path in MEDLIT_CONFIG env var (if set)
  2. medlit.toml in the examples/medlit package directory
  3. medlit.toml in the current working directory

If no file is found, built-in defaults are used (window_size=1536, overlap=400).

### `def _default_config_paths() -> list[Path]`

Return paths to check for medlit.toml (first existing wins).

### `def load_medlit_config() -> dict[str, Any]`

Load medlit config from TOML file.

Returns:
    Config dict with at least "chunker" key containing window_size and overlap.
    Uses DEFAULT_WINDOW_SIZE and DEFAULT_OVERLAP if no file or [chunker] section.


<span id="user-content-examplesmedlitpipelineembeddingspy"></span>

# examples/medlit/pipeline/embeddings.py

Embedding generation for medical entities.

Simple hash-based embedding generator for now. Can be enhanced with
biomedical embedding models (BioBERT, etc.) later.

## `class OllamaMedLitEmbeddingGenerator(EmbeddingGeneratorInterface)`

Real embedding generator using Ollama.

Uses Ollama's embedding API to generate vectors for medical entities.
Default model is nomic-embed-text which performs well on medical text.

### `def OllamaMedLitEmbeddingGenerator.dimension(self) -> int`

Return embedding dimension for the model.

Common dimensions:
- nomic-embed-text: 768
- mxbai-embed-large: 1024
- bge-large: 1024


<span id="user-content-examplesmedlitpipelineinitpy"></span>

# examples/medlit/pipeline/__init__.py

Pipeline components for medical literature extraction.


<span id="user-content-examplesmedlitpipelinellmclientpy"></span>

# examples/medlit/pipeline/llm_client.py

LLM client abstraction for entity and relationship extraction.

Provides a unified interface for Ollama LLM integration with tool calling support.

## `class LLMTimeoutError(TimeoutError)`

Raised when an LLM request (e.g. Ollama) exceeds the configured timeout.

Ingestion should treat this as a hard failure: abort the run, do not save
bundle or caches, and exit loudly.

## `class LLMClientInterface(ABC)`

Abstract interface for LLM clients.

## `class OllamaLLMClient(LLMClientInterface)`

Ollama LLM client implementation.

### `def OllamaLLMClient.__init__(self, model: str = 'llama3.1:8b', host: str = 'http://localhost:11434', timeout: float = 300.0)`

Initialize Ollama client.

Args:
    model: Ollama model name (e.g., "llama3.1:8b", "llama3.1:8b")
    host: Ollama server URL
    timeout: Request timeout in seconds (default: 300)

### `def OllamaLLMClient._parse_json_from_text(self, response_text: str) -> dict[str, Any] | list[Any]`

Extract and parse JSON from response text.

Handles markdown code blocks and finds the first complete JSON structure.

Args:
    response_text: Raw text response from the LLM.

Returns:
    Parsed JSON object (dict or list).

Raises:
    ValueError: If no valid JSON found in response.

### `def OllamaLLMClient.find_matching_bracket(text: str, start: int, open_char: str, close_char: str) -> int`

Find the matching closing bracket for an opening bracket.


<span id="user-content-examplesmedlitpipelinementionspy"></span>

# examples/medlit/pipeline/mentions.py

Entity mention extraction from journal articles.

Extracts entity mentions from Paper JSON format (from med-lit-schema).
Since the papers already have extracted entities, we convert those to EntityMention objects.
Can also use Ollama LLM for NER extraction from text.

### `def _normalize_mention_key(name: str, entity_type: str) -> tuple[str, str]`

Normalized key for deduping mentions: (alphanumeric lower name, type).

## `class MedLitEntityExtractor(EntityExtractorInterface)`

Extract entity mentions from journal articles.

This extractor works with Paper JSON format from med-lit-schema, which
already contains extracted entities. We convert those to EntityMention objects.

Can also use Ollama LLM to extract entities directly from text if llm_client is provided.
Note: Canonical ID lookup is handled during the promotion phase, not during extraction.

### `def MedLitEntityExtractor.__init__(self, llm_client: LLMClientInterface | None = None, domain: DomainSchema | None = None)`

Initialize entity extractor.

Args:
    llm_client: Optional LLM client for extracting entities from text.
                If None, only uses pre-extracted entities from Paper JSON.
    domain: Domain schema for entity type validation (needed for normalization).

### `def MedLitEntityExtractor._normalize_entity_type(self, entity_type_raw: str) -> str | None`

Normalize LLM entity types to schema types.

Handles:
- Multi-type format (drug|protein) → takes first valid type
- Common mistakes (test → procedure)
- Invalid types → returns None (skip entity)

Args:
    entity_type_raw: Raw entity type string from LLM

Returns:
    Normalized type string if valid, None if invalid/non-medical


<span id="user-content-examplesmedlitpipelineparserpy"></span>

# examples/medlit/pipeline/parser.py

Document parser for journal articles.

Converts raw paper input (PMC XML, JSON, etc.) into JournalArticle documents.

## `class JournalArticleParser(DocumentParserInterface)`

Parse raw journal article content into JournalArticle documents.

This parser handles various input formats (PMC XML, JSON from med-lit-schema,
etc.) and converts them to kgraph's JournalArticle format.

For now, this is a minimal implementation. A full implementation would:
1. Parse PMC XML (using existing med-lit-schema parser logic)
2. Parse JSON from med-lit-schema's Paper format
3. Extract metadata and map to JournalArticle fields

### `def JournalArticleParser._parse_xml_to_dict(self, root: Any, source_uri: str | None) -> dict[str, Any]`

Converts a PMC XML structure into a dictionary.

This method traverses the XML element tree of a PubMed Central article
and extracts key information, mapping it to a dictionary that loosely
conforms to the `med-lit-schema` Paper format. This intermediate
dictionary is then passed to `_parse_from_dict`.

Args:
    root: The root element of the parsed XML tree.
    source_uri: An optional source URI, used as a fallback to derive
                the paper's ID from its filename.

Returns:
    A dictionary containing the extracted title, abstract, authors,
    and other metadata.

### `def JournalArticleParser._parse_from_dict(self, data: dict[str, Any], source_uri: str | None) -> JournalArticle`

Constructs a `JournalArticle` from a dictionary.

This method takes a dictionary (conforming to `med-lit-schema`'s Paper
format or the output of `_parse_xml_to_dict`) and maps its fields to
the `JournalArticle` document model.

Key mapping logic:
-   `document_id` is chosen in order of preference: DOI, then PMID,
    then the original `paper_id`.
-   `content` is created by combining the abstract and full text.
-   Pre-existing `entities` and `relationships` from the input data
    are moved into the `metadata` dictionary, so that downstream
    extractors in the kgraph pipeline can find them.
-   Other fields like authors, publication date, and journal are
    mapped directly.

Args:
    data: A dictionary containing the paper's data.
    source_uri: The original source URI of the document.

Returns:
    A fully populated `JournalArticle` object.

Raises:
    ValueError: If no valid identifier (paper_id, doi, or pmid)
                can be found in the input data.


<span id="user-content-examplesmedlitpipelinepmcchunkerpy"></span>

# examples/medlit/pipeline/pmc_chunker.py

PMC-specific document chunker using iter_pmc_windows for memory-efficient streaming.

Produces DocumentChunks from raw PMC/JATS XML bytes without loading the full
document into memory. Implements DocumentChunkerInterface with chunk_from_raw()
for the streaming path and chunk(document) as a fallback for parsed documents.

### `def _content_type_is_xml(content_type: str) -> bool`

Return True if content_type is XML (strip parameters like ; charset=utf-8).

## `class PMCStreamingChunker(DocumentChunkerInterface)`

Chunker for PMC/JATS XML that uses iter_pmc_windows for memory-efficient chunking.

When chunk_from_raw() is used with XML content type, yields overlapping
windows from raw bytes without parsing the full document. For chunk(document)
(e.g. already-parsed document or non-XML), delegates to a windowed chunker
over document.content.

### `def PMCStreamingChunker.__init__(self, window_size: int = DEFAULT_WINDOW_SIZE, overlap: int = DEFAULT_OVERLAP, include_abstract_separately: bool = True, document_chunk_config: ChunkingConfig | None = None)`

Initialize the PMC streaming chunker.

Args:
    window_size: Target characters per window (used for chunk_from_raw).
    overlap: Overlap between consecutive windows.
    include_abstract_separately: If True, first window is abstract alone.
    document_chunk_config: Config for chunk(document) fallback. If None,
        uses window_size and overlap for the windowed chunker.

### `def document_id_from_source_uri(source_uri: str | None) -> str`

Derive a document ID from source_uri (e.g. file stem). Used when parsing is deferred.


<span id="user-content-examplesmedlitpipelinepmcstreamingpy"></span>

# examples/medlit/pipeline/pmc_streaming.py

Streaming PMC XML chunker for full-paper extraction.

Yields overlapping text windows from a PMC/JATS XML document without loading
the entire body into a single string. Uses iterparse so the parser does not
hold the full tree in memory; sections are yielded and then cleared.

Use for:
- Entity extraction: run NER on each window, then merge/dedupe mentions.
- Relationship extraction: run relationship extraction on each window with
  the full entity list, then merge relationships.

### `def _local_tag(tag: str) -> str`

Strip XML namespace from tag for comparison.

### `def iter_pmc_sections(raw_content: bytes) -> Iterator[tuple[str, str]]`

Yield (section_id, text) for abstract and each body section.

Uses iterparse so we do not build a full DOM for the body. After
yielding each element's text we clear it to free memory. Yields:
- ("abstract", abstract_text) first if present
- ("sec_<id>", section_text) for each <sec> (full text of that section,
  including nested secs and paragraphs)

Namespaces in JATS are stripped so we match "abstract", "body", "sec".

### `def iter_overlapping_windows(sections: Iterator[tuple[str, str]], window_size: int = DEFAULT_WINDOW_SIZE, overlap: int = DEFAULT_OVERLAP) -> Iterator[tuple[int, str]]`

Turn a stream of (section_id, text) into overlapping windows.

Concatenates section texts. When accumulated length reaches window_size,
yields (window_index, text). Then slides by (window_size - overlap) so
consecutive windows overlap by `overlap` characters. This helps the LLM
see context across boundaries and avoids splitting entities.

If include_abstract_separately is True, the first yielded window is
always the abstract alone (if any section has section_id == "abstract").
Subsequent windows are from body content only. Only one window-sized
buffer is kept in memory.

Args:
    sections: Iterator of (section_id, text).
    window_size: Target size of each window in characters.
    overlap: Number of characters to overlap between consecutive windows.
    include_abstract_separately: If True, yield abstract as window 0.

Yields:
    (window_index, text) for each window.

### `def iter_pmc_windows(raw_content: bytes, window_size: int = DEFAULT_WINDOW_SIZE, overlap: int = DEFAULT_OVERLAP, include_abstract_separately: bool = True) -> Iterator[tuple[int, str]]`

Yield overlapping text windows from PMC XML for full-paper extraction.

Convenience generator: iter_pmc_sections(raw_content) -> iter_overlapping_windows(...).
Use when you have raw PMC/XML bytes and want a sequence of prompts (e.g. for
entity or relationship extraction) without loading the whole paper into one string.

Args:
    raw_content: Raw bytes of the PMC/JATS XML document.
    window_size: Target characters per window.
    overlap: Overlap between consecutive windows.
    include_abstract_separately: If True, first window is the abstract alone.

Yields:
    (window_index, text) for each window.


<span id="user-content-examplesmedlitpipelinerelationshipspy"></span>

# examples/medlit/pipeline/relationships.py

Relationship extraction from journal articles.

Extracts relationships from Paper JSON format (from med-lit-schema).
Since the papers already have extracted relationships, we convert those to BaseRelationship objects.
Can also use Ollama LLM for relationship extraction from text.

### `def _normalize_evidence_for_match(text: str) -> str`

Normalize evidence text for substring matching: lowercase, strip, collapse whitespace.

### `def _evidence_has_disease_context(evidence: str) -> bool`

Return True if evidence text suggests disease/marker context (IHC, tumor, etc.).

### `def _evidence_contains_both_entities(evidence: str, subject_name: str, object_name: str, subject_entity: BaseEntity | None, object_entity: BaseEntity | None) -> tuple[bool, str | None, dict[str, Any]]`

Check that both subject and object (or synonyms) appear in the evidence text.

Returns:
    (ok, drop_reason, detail): ok is True only if both entities appear;
    drop_reason is set when ok is False; detail has subject_in_evidence, object_in_evidence.

## `class MedLitRelationshipExtractor(RelationshipExtractorInterface)`

Extract relationships from journal articles.

This extractor works with Paper JSON format from med-lit-schema, which
already contains extracted relationships. We convert those to BaseRelationship objects.

Can also use Ollama LLM to extract relationships directly from text if llm_client is provided.

### `def MedLitRelationshipExtractor.__init__(self, llm_client: Optional[LLMClientInterface] = None, domain: Optional['MedLitDomainSchema'] = None, trace_dir: Optional[Path] = None, embedding_generator: Any = None, evidence_similarity_threshold: float = 0.5)`

Initialize relationship extractor.

Args:
    llm_client: Optional LLM client for extracting relationships from text.
                If None, only uses pre-extracted relationships from Paper JSON.
    domain: Optional domain schema for type validation and predicate constraints.
            If provided, will attempt to swap subject/object on type mismatches.
    trace_dir: Optional directory for writing trace files. If None, uses default.
    embedding_generator: Optional embedding generator for semantic evidence validation.
                        When set, failed string evidence check is retried with cosine similarity.
    evidence_similarity_threshold: Minimum cosine similarity (0-1) for semantic evidence pass. Default 0.5.

### `def MedLitRelationshipExtractor.trace_dir(self) -> Path`

Get the trace directory.

### `def MedLitRelationshipExtractor.trace_dir(self, value: Path) -> None`

Set the trace directory.

### `def MedLitRelationshipExtractor._should_swap_subject_object(self, predicate: str, subject_entity: BaseEntity, object_entity: BaseEntity) -> bool`

Check if subject and object should be swapped based on type constraints.

Args:
    predicate: The relationship predicate
    subject_entity: The subject entity
    object_entity: The object entity

Returns:
    True if swapping subject and object would satisfy type constraints, False otherwise.

### `def MedLitRelationshipExtractor._validate_predicate_semantics(self, predicate: str, evidence: str) -> bool`

Validate that predicate semantics match the evidence text.

Checks for semantic mismatches like:
- "increases_risk" with positive therapeutic language
- "treats" with negative/harmful language

Args:
    predicate: The relationship predicate (e.g., "treats", "increases_risk")
    evidence: The evidence text supporting the relationship

Returns:
    True if predicate matches evidence semantics, False if there's a mismatch.

### `def MedLitRelationshipExtractor._build_llm_prompt(self, text_sample: str, entity_list: str) -> str`

Build the prompt for the LLM.

Notes:
- This prompt is driven by the domain schema:
  - self._domain.relationship_types (vocabulary)
  - self._domain.predicate_constraints (allowed subject/object types)
- Predicates must be returned in *lowercase* (e.g. "treats"), because downstream
  code currently normalizes and stores predicates as lowercase. :contentReference[oaicite:3]{index=3}

### `def MedLitRelationshipExtractor.write_skip_trace(self, document_id: str, reason: str, entity_count: int) -> None`

Write a minimal trace file when a window is skipped (e.g. fewer than 2 entities).

Uses the same path convention as _write_trace so skip and full traces
appear in the same directory. Call from WindowedRelationshipExtractor
when a chunk is skipped so --trace-all still produces a trace per window.

### `def MedLitRelationshipExtractor._write_trace(self, document_id: str, trace: dict[str, Any]) -> None`

Write trace file for debugging relationship extraction.

When document_id is from a windowed run (e.g. PMC12770061_window_5),
the filename includes the window index so each window gets its own file
(e.g. PMC12770061.5.relationships.trace.json) and earlier windows are
not overwritten.


<span id="user-content-examplesmedlitpipelineresolvepy"></span>

# examples/medlit/pipeline/resolve.py

Entity resolution for medical literature domain.

Resolves entity mentions to canonical entities using UMLS, HGNC, RxNorm, UniProt IDs.

## `class MedLitEntityResolver(BaseModel, EntityResolverInterface)`

Resolve medical entity mentions to canonical or provisional entities.

Resolution strategy (hybrid approach):
1. If mention has canonical_id_hint (from pre-extracted entities), use that
2. Check if entity with that ID already exists in storage
3. If not, create new canonical entity (since we have authoritative IDs)
4. For mentions without canonical IDs:
   a. Try embedding-based semantic matching against existing entities (if embedding_generator provided)
   b. If no match found, create provisional entities

The embedding-based matching acts as a semantic cache, catching variations like:
- "BRCA-1" vs "BRCA1"
- "breast cancer 1 gene" vs "BRCA1"
- "TP53" vs "p53"

**Fields:**

```python
domain: DomainSchema
embedding_generator: EmbeddingGeneratorInterface | None = None
similarity_threshold: float = 0.85
```


### `def MedLitEntityResolver._parse_canonical_id(self, entity_id: str, entity_type: str) -> dict[str, str]`

Parses a canonical ID string into a structured dictionary.

This utility function takes a raw ID string (e.g., "HGNC:1100") and
converts it into a `canonical_ids` dictionary (e.g.,
`{"hgnc": "HGNC:1100"}`). It handles both prefixed IDs and attempts to
infer the authority for non-prefixed IDs based on the entity type.

Args:
    entity_id: The canonical ID string to parse.
    entity_type: The entity's type, used to infer the authority for
                 non-prefixed IDs.

Returns:
    A dictionary mapping the authority name (e.g., "hgnc") to the
    full canonical ID.


<span id="user-content-examplesmedlitpromotionpy"></span>

# examples/medlit/promotion.py

Promotion policy for medical literature domain.

Promotes provisional entities to canonical status when they have authoritative
identifiers (UMLS, HGNC, RxNorm, UniProt) or meet usage/confidence thresholds.

## `class MedLitPromotionPolicy(PromotionPolicy)`

Promotion policy for medical literature domain.

Assigns canonical IDs based on authoritative medical ontologies:
- Diseases: UMLS IDs (e.g., "C0006142")
- Genes: HGNC IDs (e.g., "HGNC:1100")
- Drugs: RxNorm IDs (e.g., "RxNorm:1187832")
- Proteins: UniProt IDs (e.g., "UniProt:P38398")

Promotion strategy:
1. If entity already has canonical_id in canonical_ids dict, use that
2. If entity_id is already a canonical ID format, use it directly
3. Otherwise, look up canonical ID from authority APIs (UMLS, HGNC, RxNorm, UniProt)

### `def MedLitPromotionPolicy.__init__(self, config, lookup: Optional[CanonicalIdLookupInterface] = None)`

Initialize promotion policy.

Args:
    config: Promotion configuration with thresholds.
    lookup: Optional canonical ID lookup service. If None, will create
            a new CanonicalIdLookup instance (without UMLS API key unless set in env).

### `def MedLitPromotionPolicy.should_promote(self, entity: BaseEntity) -> bool`

Check if entity meets promotion thresholds.

Force-promote rules (bypass standard thresholds):
- If confidence >= 0.7, ignore usage count requirement
- If canonical ID is found (checked in run_promotion), promote regardless

Standard thresholds:
- usage_count >= min_usage_count (default: 1)
- confidence >= min_confidence (default: 0.4)
- embedding required only if require_embedding=True (default: False)


<span id="user-content-examplesmedlitreadmemd"></span>

# examples/medlit/README.md

# Medical Literature Domain Extension

This package provides a kgraph domain extension for extracting knowledge from biomedical journal articles. It rewrites the med-lit-schema project as a kgraph domain package, following the same pattern as the Sherlock example.

## Architecture

### Key Design Decisions

1. **Papers are NOT doc_assets.jsonl**: Source papers are `JournalArticle(BaseDocument)` instances used for extraction, not documentation assets. The `doc_assets.jsonl` in bundles is for human-readable documentation only.

2. **Canonical IDs**: Entities use authoritative identifiers (UMLS, HGNC, RxNorm, UniProt) directly in `entity_id`, with additional mappings in `canonical_ids`.

3. **Pattern A Relationships**: All medical predicates (treats, causes, increases_risk, etc.) use a single `MedicalClaimRelationship` class. The `predicate` field distinguishes the relationship type.

4. **Rich Metadata**: Paper metadata (study type, sample size, MeSH terms) and extraction provenance are preserved in `BaseDocument.metadata` and `BaseRelationship.metadata`.

## Domain Components

### Documents

    ...


<span id="user-content-examplesmedlitrelationshipspy"></span>

# examples/medlit/relationships.py

Medical relationship types for the knowledge graph.

Following Pattern A (simple, scalable): many predicates → one relationship class.
This allows fast implementation without class explosion, while the predicate
still stays in the `predicate` field for clear queries.

## `class MedicalClaimRelationship(BaseRelationship)`

Base class for all medical claim relationships.

This single class handles all medical predicates (treats, causes,
increases_risk, etc.). The predicate field distinguishes the relationship
type, and domain-specific metadata can be stored in the metadata dict.

Mapping from med-lit-schema:
- AssertedRelationship.subject_id → BaseRelationship.subject_id
- AssertedRelationship.predicate → BaseRelationship.predicate
- AssertedRelationship.object_id → BaseRelationship.object_id
- AssertedRelationship.confidence → BaseRelationship.confidence
- AssertedRelationship.evidence → BaseRelationship.metadata["evidence"]
- AssertedRelationship.section → BaseRelationship.metadata["section"]
- AssertedRelationship.metadata → BaseRelationship.metadata (merged)

For multi-paper aggregation:
- source_documents includes all paper IDs that assert this relationship
- metadata["assertions"][paper_id] = {"evidence": "...", "section": "...", ...}

### `def MedicalClaimRelationship.get_edge_type(self) -> str`

Return edge type category.

For Pattern A, we return a generic "medical_claim" since all
predicates use the same class. If we later split into typed classes,
each would return its specific type.


<span id="user-content-examplesmedlitschemabasepy"></span>

# examples/medlit_schema/base.py

Base models for the medlit schema.

## `class ModelInfo(BaseModel)`

Information about the model used for extraction.

**Fields:**

```python
name: str
version: str
```


## `class ExtractionProvenance(BaseModel)`

Complete provenance metadata for an extraction.

This is the complete audit trail of how extraction was performed.
Enables:
- Reproducing exact extraction with same code/models/prompts
- Comparing outputs from different pipeline versions
- Debugging quality issues
- Tracking pipeline evolution over time
- Meeting reproducibility requirements for research

Example queries enabled by provenance:
- "Find all papers extracted with prompt v1 so I can re-extract with v2"
- "Which papers were extracted with uncommitted code changes?"
- "Compare entity extraction quality between llama3.1:70b and claude-4"

Attributes:
    extraction_pipeline: Pipeline version info
    models: Models used, keyed by role (e.g., 'llm', 'embeddings')
    prompt: Prompt version info
    execution: Execution environment info
    entity_resolution: Entity resolution details if applicable

**Fields:**

```python
extraction_pipeline: Optional['ExtractionPipelineInfo'] = None
models: dict[str, ModelInfo] = Field(default_factory=dict)
prompt: Optional['PromptInfo'] = None
execution: Optional['ExecutionInfo'] = None
entity_resolution: Optional['EntityResolutionInfo'] = None
model_info: Optional[ModelInfo] = None
```


## `class SectionType(str, Enum)`

Type of section in a paper.

## `class TextSpanRef(BaseModel)`

A structural locator for text within a parsed document.

This is a parser/segmentation address that uses structural coordinates
(section type, paragraph index, sentence index) to locate text. It is
distinct from TextSpan (entity.py), which is a graph entity anchor with
precise character offsets.

Use this for:
- Intermediate parsing stages before final offsets are computed
- Structural navigation within documents
- Creating TextSpan entities once offsets are finalized

Attributes:
    paper_id: The ID of the paper this span belongs to.
    section_type: The type of section (abstract, introduction, etc.).
    paragraph_idx: Zero-based paragraph index within the section.
    sentence_idx: Optional sentence index within the paragraph.
    text_span: Optional text snippet for reference.
    start_offset: Optional character offset (for when computed).
    end_offset: Optional character offset (for when computed).

**Fields:**

```python
paper_id: str
section_type: SectionType
paragraph_idx: int
sentence_idx: Optional[int] = None
text_span: Optional[str] = None
start_offset: Optional[int] = None
end_offset: Optional[int] = None
```


## `class ExtractionMethod(str, Enum)`

Method used for extraction.

## `class StudyType(str, Enum)`

Type of study.

## `class PredicateType(str, Enum)`

All possible predicates (relationship types) in the medical literature knowledge graph.

This enum provides type safety for relationship categorization and enables
validation of entity-relationship compatibility.

## `class EntityType(str, Enum)`

All possible entity types in the knowledge graph.

This enum provides type safety for entity categorization and enables
validation of entity-relationship compatibility.

## `class ClaimPredicate(BaseModel)`

Describes the nature of a claim made in a paper.

Examples:
    - "Olaparib significantly improved progression-free survival" (TREATS)
    - "BRCA1 mutations increase breast cancer risk by 5-fold" (INCREASES_RISK)
    - "Warfarin and aspirin interact synergistically" (INTERACTS_WITH)

Attributes:
    predicate_type: The type of relationship asserted in the claim
    description: A natural language description of the predicate as it appears in the text

**Fields:**

```python
predicate_type: PredicateType
description: str
```


## `class Provenance(BaseModel)`

Information about the origin of a piece of data.

Attributes:
    source_type: The type of source (e.g., 'paper', 'database', 'model_extraction')
    source_id: An identifier for the source (e.g., DOI, database record ID)
    source_version: The version of the source, if applicable
    notes: Additional notes about the provenance

**Fields:**

```python
source_type: str
source_id: str
source_version: Optional[str] = None
notes: Optional[str] = None
```


## `class EvidenceType(BaseModel)`

The type of evidence supporting a relationship, linked to evidence ontologies.

Examples:
    - RCT: ontology_id="ECO:0007673", ontology_label="randomized controlled trial evidence"
    - Observational: ontology_id="ECO:0000203", ontology_label="observational study evidence"
    - Case report: ontology_id="ECO:0006016", ontology_label="case study evidence"

Attributes:
    ontology_id: Identifier from an evidence ontology (ECO, SEPIO)
    ontology_label: Human-readable label for the ontology term
    description: A fuller description of the evidence type

**Fields:**

```python
ontology_id: str
ontology_label: str
description: Optional[str] = None
```


## `class EntityReference(BaseModel)`

Reference to an entity in the knowledge graph.

Lightweight pointer to a canonical entity (Disease, Drug, Gene, etc.)
with the name as it appeared in this specific paper.

Attributes:
    id: Canonical entity ID
    name: Entity name as mentioned in paper
    type: Entity type (drug, disease, gene, protein, etc.)

**Fields:**

```python
id: str = ...
name: str = ...
type: EntityType = ...
```


## `class Polarity(str, Enum)`

Polarity of evidence relative to a claim.

## `class Edge(BaseModel)`

Base edge in the knowledge graph.

**Fields:**

```python
id: EdgeId
subject: EntityReference
object: EntityReference
provenance: Provenance
```


## `class ExtractionEdge(Edge)`

Edge from automated extraction.

## `class ClaimEdge(Edge)`

Edge representing a claim from a paper.

## `class EvidenceEdge(Edge)`

Edge representing evidence for a claim.

## `class ExtractionPipelineInfo(BaseModel)`

Information about the extraction pipeline version.

Tracks the exact code version that performed entity/relationship extraction.
Essential for reproducibility and debugging extraction quality issues.

Attributes:
    name: Pipeline name (e.g., 'ollama_langchain_ingest')
    version: Semantic version of the pipeline
    git_commit: Full git commit hash
    git_commit_short: Short git commit hash (7 chars)
    git_branch: Git branch name
    git_dirty: Whether working directory had uncommitted changes
    repo_url: Repository URL

**Fields:**

```python
name: str
version: str
git_commit: str
git_commit_short: str
git_branch: str
git_dirty: bool
repo_url: str
```


## `class PromptInfo(BaseModel)`

Information about the prompt used.

Tracks prompt evolution. Critical for understanding extraction behavior changes.

Attributes:
    version: Prompt version identifier
    template: Prompt template name
    checksum: SHA256 of actual prompt text for exact reproduction

**Fields:**

```python
version: str
template: str
checksum: Optional[str] = None
```


## `class ExecutionInfo(BaseModel)`

Information about when and where extraction was performed.

Useful for debugging issues related to specific machines or time periods.

Attributes:
    timestamp: ISO 8601 UTC timestamp
    hostname: Hostname of machine that ran extraction
    python_version: Python version
    duration_seconds: Extraction duration in seconds

**Fields:**

```python
timestamp: str
hostname: str
python_version: str
duration_seconds: Optional[float] = None
```


## `class EntityResolutionInfo(BaseModel)`

Information about entity resolution process.

Tracks how entities were matched to canonical IDs. Helps identify when
entity deduplication is working poorly.

Attributes:
    canonical_entities_matched: Number of entities matched to existing canonical IDs
    new_entities_created: Number of new canonical entities created
    similarity_threshold: Similarity threshold used for matching
    embedding_model: Embedding model used for similarity

**Fields:**

```python
canonical_entities_matched: int
new_entities_created: int
similarity_threshold: float
embedding_model: str
```


## `class Measurement(BaseModel)`

Quantitative measurements associated with relationships.

Stores numerical data with appropriate metadata for statistical
analysis and evidence quality assessment.

Attributes:
    value: The numerical value
    unit: Unit of measurement (if applicable)
    value_type: Type of measurement (effect_size, p_value, etc.)
    p_value: Statistical significance
    confidence_interval: 95% confidence interval
    study_population: Description of study population
    measurement_context: Additional context about the measurement

Example:
    >>> measurement = Measurement(
    ...     value=0.59,
    ...     value_type="response_rate",
    ...     p_value=0.001,
    ...     confidence_interval=(0.52, 0.66),
    ...     study_population="BRCA-mutated breast cancer patients"
    ... )

**Fields:**

```python
value: float
unit: Optional[str] = None
value_type: str
p_value: Optional[float] = None
confidence_interval: Optional[tuple[float, float]] = None
study_population: Optional[str] = None
measurement_context: Optional[str] = None
```



<span id="user-content-examplesmedlitschemadepthoffieldsmd"></span>

# examples/medlit_schema/DEPTH_OF_FIELDS.md

# DEPTH_OF_FIELDS: Enrich examples/medlit_schema to Production-Ready Depth

## Executive Summary

**Goal**: Transform `examples/medlit_schema/` from a minimal teaching example (~400 LOC) into a production-ready, reusable schema package (~2,500+ LOC) with the full richness of `med-lit-schema` while maintaining the clean separation of definitions vs. implementation.

**Scope**: Schema definitions only (no functional infrastructure code like pipelines, storage backends, or servers).

**Architectural Principle**:
```
kgschema/          → examples/medlit_schema/    (definitions only)
kgraph/            → examples/medlit/           (implementations)
```

**Current State**:
- `entity.py`: 171 lines → **Target: ~1,100 lines**
- `relationship.py`: 153 lines → **Target: ~600 lines**
- `base.py`: 76 lines → **Target: ~300 lines**
- `domain.py`: 141 lines → **Target: ~170 lines**
- **Total**: ~540 lines → **Target: ~2,170 lines** of pure schema definitions

    ...


<span id="user-content-examplesmedlitschemadocumentpy"></span>

# examples/medlit_schema/document.py

Medlit document definitions.


<span id="user-content-examplesmedlitschemadomainpy"></span>

# examples/medlit_schema/domain.py

Domain schema for the Medical Literature domain.

## `class MedlitDomain(DomainSchema)`

Domain schema for medical literature.


<span id="user-content-examplesmedlitschemaentitypy"></span>

# examples/medlit_schema/entity.py

Medlit entity definitions.

## `class BaseMedicalEntity(BaseEntity)`

Base for all medical entities.

**Fields:**

```python
name: str
synonyms: tuple[str, ...] = ()
abbreviations: List[str] = []
embedding: Optional[tuple[float, ...]] = None
source: Literal['umls', 'mesh', 'rxnorm', 'hgnc', 'uniprot', 'extracted']
```


## `class Disease(BaseMedicalEntity)`

Represents medical conditions, disorders, and syndromes.

Uses UMLS as the primary identifier system with additional mappings to
MeSH and ICD-10 for interoperability with clinical systems.

Attributes:
    umls_id: UMLS Concept ID (e.g., "C0006142" for Breast Cancer)
    mesh_id: Medical Subject Heading ID for literature indexing
    icd10_codes: List of ICD-10 diagnostic codes
    category: Disease classification (genetic, infectious, autoimmune, etc.)

Example:
    >>> breast_cancer = Disease(
    ...     entity_id="C0006142",
    ...     name="Breast Cancer",
    ...     synonyms=("Breast Carcinoma", "Mammary Cancer"),
    ...     umls_id="C0006142",
    ...     mesh_id="D001943",
    ...     icd10_codes=["C50.9"],
    ...     category="genetic",
    ...     source="umls"
    ... )

**Fields:**

```python
umls_id: Optional[str] = None
mesh_id: Optional[str] = None
icd10_codes: List[str] = []
category: Optional[str] = None
```


## `class Gene(BaseMedicalEntity)`

Represents human genes.

Uses HGNC (HUGO Gene Nomenclature Committee) as the primary identifier
with additional mappings to Entrez Gene for cross-reference.

Attributes:
    symbol: Official gene symbol (e.g., "BRCA1")
    hgnc_id: HGNC identifier (e.g., "HGNC:1100")
    chromosome: Chromosomal location (e.g., "17q21.31")
    entrez_id: NCBI Entrez Gene ID

Example:
    >>> brca1 = Gene(
    ...     entity_id="HGNC:1100",
    ...     name="BRCA1",
    ...     symbol="BRCA1",
    ...     hgnc_id="HGNC:1100",
    ...     chromosome="17q21.31",
    ...     entrez_id="672",
    ...     source="hgnc"
    ... )

**Fields:**

```python
symbol: Optional[str] = None
hgnc_id: Optional[str] = None
chromosome: Optional[str] = None
entrez_id: Optional[str] = None
```


## `class Drug(BaseMedicalEntity)`

Represents pharmaceutical drugs and medications.

Uses RxNorm as the primary identifier system for standardized drug names.

Attributes:
    rxnorm_id: RxNorm concept identifier (e.g., "1187832" for Olaparib)
    brand_names: Commercial brand names (e.g., ["Lynparza"])
    drug_class: Pharmacological class (e.g., "PARP inhibitor")
    mechanism: Mechanism of action description

Example:
    >>> olaparib = Drug(
    ...     entity_id="RxNorm:1187832",
    ...     name="Olaparib",
    ...     rxnorm_id="1187832",
    ...     brand_names=["Lynparza"],
    ...     drug_class="PARP inhibitor",
    ...     mechanism="Inhibits PARP enzymes",
    ...     source="rxnorm"
    ... )

**Fields:**

```python
rxnorm_id: Optional[str] = None
brand_names: List[str] = []
drug_class: Optional[str] = None
mechanism: Optional[str] = None
```


## `class Protein(BaseMedicalEntity)`

Represents proteins and protein complexes.

Uses UniProt as the primary identifier system.

Attributes:
    uniprot_id: UniProt accession (e.g., "P38398" for BRCA1 protein)
    gene_id: Associated gene identifier
    function: Protein function description
    pathways: List of pathway IDs this protein participates in

Example:
    >>> brca1_protein = Protein(
    ...     entity_id="UniProt:P38398",
    ...     name="BRCA1",
    ...     uniprot_id="P38398",
    ...     gene_id="HGNC:1100",
    ...     function="DNA repair",
    ...     pathways=["R-HSA-5685942"],
    ...     source="uniprot"
    ... )

**Fields:**

```python
uniprot_id: Optional[str] = None
gene_id: Optional[str] = None
function: Optional[str] = None
pathways: List[str] = []
```


## `class Mutation(BaseMedicalEntity)`

Represents genetic mutations and variants.

Attributes:
    variant_notation: HGVS notation (e.g., "c.68_69delAG")
    consequence: Effect of mutation (e.g., "frameshift", "missense")
    clinical_significance: ClinVar significance (pathogenic, benign, etc.)

Example:
    >>> brca1_mutation = Mutation(
    ...     entity_id="BRCA1_c.68_69delAG",
    ...     name="BRCA1 c.68_69delAG",
    ...     variant_notation="c.68_69delAG",
    ...     consequence="frameshift",
    ...     clinical_significance="pathogenic",
    ...     source="extracted"
    ... )

**Fields:**

```python
variant_notation: Optional[str] = None
consequence: Optional[str] = None
clinical_significance: Optional[str] = None
```


## `class Symptom(BaseMedicalEntity)`

Represents clinical signs and symptoms.

Attributes:
    severity_scale: Measurement scale if applicable (e.g., "0-10", "mild/moderate/severe")
    onset_pattern: Typical onset (acute, chronic, intermittent)

Example:
    >>> pain = Symptom(
    ...     entity_id="C0030193",
    ...     name="Pain",
    ...     umls_id="C0030193",
    ...     severity_scale="0-10",
    ...     onset_pattern="varies",
    ...     source="umls"
    ... )

**Fields:**

```python
severity_scale: Optional[str] = None
onset_pattern: Optional[str] = None
```


## `class Biomarker(BaseMedicalEntity)`

Represents biological markers used for diagnosis or prognosis.

Attributes:
    loinc_code: LOINC code for lab tests
    measurement_type: Type of measurement (protein, metabolite, imaging, etc.)
    clinical_use: Primary clinical application

Example:
    >>> ca125 = Biomarker(
    ...     entity_id="LOINC:10334-1",
    ...     name="CA-125",
    ...     loinc_code="10334-1",
    ...     measurement_type="protein",
    ...     clinical_use="ovarian cancer screening",
    ...     source="extracted"
    ... )

**Fields:**

```python
loinc_code: Optional[str] = None
measurement_type: Optional[str] = None
clinical_use: Optional[str] = None
```


## `class Pathway(BaseMedicalEntity)`

Represents biological pathways.

Attributes:
    kegg_id: KEGG pathway identifier
    reactome_id: Reactome pathway identifier
    pathway_type: Type of pathway (signaling, metabolic, etc.)

Example:
    >>> dna_repair = Pathway(
    ...     entity_id="R-HSA-5685942",
    ...     name="HDR through Homologous Recombination",
    ...     reactome_id="R-HSA-5685942",
    ...     pathway_type="DNA repair",
    ...     source="extracted"
    ... )

**Fields:**

```python
kegg_id: Optional[str] = None
reactome_id: Optional[str] = None
pathway_type: Optional[str] = None
```


## `class Procedure(BaseMedicalEntity)`

Represents medical tests, diagnostics, treatments.

Attributes:
    type: Procedure category (diagnostic, therapeutic, preventive)
    invasiveness: Invasiveness level (non-invasive, minimally invasive, invasive)

**Fields:**

```python
type: Optional[str] = None
invasiveness: Optional[str] = None
```


## `class PaperMetadata(BaseModel)`

Extended metadata about the research paper.

Combines study characteristics (for evidence quality assessment) with
bibliographic information (for citations and filtering).

This is MORE than just storage - these fields enable critical queries:
- "Show me only RCT evidence for this drug-disease relationship"
- "What's the sample size distribution for studies on this topic?"
- "Find papers from high-impact journals on this mutation"

Attributes:
    study_type: Type of study (observational, rct, meta_analysis, case_report, review)
    sample_size: Study sample size - larger = more reliable
    study_population: Description of study population
    primary_outcome: Primary outcome measured
    clinical_phase: Clinical trial phase if applicable
    mesh_terms: Medical Subject Headings - NLM's controlled vocabulary for indexing

Example:
    >>> metadata = PaperMetadata(
    ...     study_type="rct",
    ...     sample_size=302,
    ...     study_population="Women with BRCA1/2-mutated metastatic breast cancer",
    ...     primary_outcome="Progression-free survival",
    ...     clinical_phase="III",
    ...     mesh_terms=["Breast Neoplasms", "BRCA1 Protein", "PARP Inhibitors"]
    ... )

**Fields:**

```python
study_type: Optional[str] = None
sample_size: Optional[int] = None
study_population: Optional[str] = None
primary_outcome: Optional[str] = None
clinical_phase: Optional[str] = None
mesh_terms: List[str] = []
```


## `class TextSpan(BaseEntity)`

Represents a specific span of text within a document, acting as an anchor for evidence.

This entity provides fine-grained provenance for assertions by linking them
to exact locations within a source paper. It serves as a first-class entity
that can be referenced by Evidence.

TextSpan is canonical-only (not promotable) because:
- Character offsets are stable only relative to a specific text representation
- The combination of paper_id + section + offsets provides a natural canonical ID
- There is no meaningful "provisional" state for a text location

Note: This is distinct from TextSpanRef (base.py), which is a structural locator
using paragraph/sentence indices for parsing stages before final offsets are computed.

Attributes:
    paper_id: The ID of the paper this text span belongs to.
    section: The section of the paper (e.g., "abstract", "introduction", "results").
    start_offset: The character offset where the span starts in the section content (required).
    end_offset: The character offset where the span ends in the section content (required).
    text_content: The actual text content of the span (optional, for convenience and caching).

**Fields:**

```python
promotable: bool = False
status: EntityStatus = EntityStatus.CANONICAL
paper_id: str
section: str
start_offset: int
end_offset: int
text_content: Optional[str] = None
```


### `def TextSpan.end_must_be_greater_than_start(cls, v, info)`

Validate that end_offset > start_offset.

## `class Paper(BaseEntity)`

A research paper with extracted entities, relationships, and full provenance.

This is the COMPLETE representation of a paper in the knowledge graph, combining:

1. Bibliographic metadata (authors, journal, identifiers)
2. Text content (title, abstract)
3. Study metadata (study type, sample size, etc.)
4. Extraction provenance (how extraction was performed)

Design philosophy:

- Top-level fields are FREQUENTLY QUERIED (paper_id, title, authors, publication_date)
- Nested objects group related data (paper_metadata for study info, extraction_provenance for pipeline info)

Why certain fields are top-level:

- paper_id: Primary key, referenced everywhere
- title, abstract: Core content, always displayed
- authors: Essential for citations, frequently filtered
- publication_date: Frequently used for filtering by recency
- journal: Frequently used for quality filtering

Why other fields are nested:

- paper_metadata: Study details, accessed together for evidence assessment
- extraction_provenance: Technical details, only for debugging/reproducibility

Attributes:
    paper_id: Unique identifier - PMC ID preferred, but can be DOI or PMID
    pmid: PubMed ID - different from PMC ID
    doi: Digital Object Identifier
    title: Full paper title
    abstract: Complete abstract text
    authors: List of author names in citation order
    publication_date: Publication date in ISO format (YYYY-MM-DD)
    journal: Journal name
    paper_metadata: Extended metadata including study type, sample size, MeSH terms
    extraction_provenance: Complete provenance of how extraction was performed

Example:
    >>> paper = Paper(
    ...     entity_id="PMC8437152",
    ...     paper_id="PMC8437152",
    ...     pmid="34567890",
    ...     doi="10.1234/nejm.2023.001",
    ...     title="Efficacy of Olaparib in BRCA-Mutated Breast Cancer",
    ...     abstract="Background: PARP inhibitors have shown promise...",
    ...     authors=["Smith J", "Johnson A", "Williams K"],
    ...     publication_date=datetime(2023, 6, 15),
    ...     journal="New England Journal of Medicine",
    ...     paper_metadata=PaperMetadata(
    ...         study_type="rct",
    ...         sample_size=302,
    ...         mesh_terms=["Breast Neoplasms", "PARP Inhibitors"]
    ...     )
    ... )

**Fields:**

```python
paper_id: str
pmid: Optional[str] = None
doi: Optional[str] = None
title: Optional[str] = None
abstract: Optional[str] = None
authors: List[str] = []
publication_date: Optional[datetime] = None
journal: Optional[str] = None
paper_metadata: PaperMetadata = PaperMetadata()
extraction_provenance: Optional[ExtractionProvenance] = None
```


## `class Author(BaseEntity)`

Represents a researcher or author of scientific publications.

Attributes:
    orcid: ORCID identifier (unique researcher ID)
    affiliations: List of institutional affiliations
    h_index: Citation metric indicating research impact

Example:
    >>> author = Author(
    ...     entity_id="0000-0001-2345-6789",
    ...     name="Jane Smith",
    ...     orcid="0000-0001-2345-6789",
    ...     affiliations=["Harvard Medical School", "Massachusetts General Hospital"],
    ...     h_index=45,
    ...     source="orcid",
    ...     created_at=datetime.now()
    ... )

**Fields:**

```python
orcid: Optional[str] = None
affiliations: List[str] = []
h_index: Optional[int] = None
```


## `class ClinicalTrial(BaseEntity)`

Represents a clinical trial registered on ClinicalTrials.gov.

Attributes:
    nct_id: ClinicalTrials.gov identifier (e.g., "NCT01234567")
    title: Official trial title
    phase: Trial phase (I, II, III, IV)
    trial_status: Current status (recruiting, completed, terminated, etc.)
    intervention: Description of treatment being tested

Example:
    >>> trial = ClinicalTrial(
    ...     entity_id="NCT01234567",
    ...     name="Study of Drug X in Patients with Disease Y",
    ...     nct_id="NCT01234567",
    ...     title="Study of Drug X in Patients with Disease Y",
    ...     phase="III",
    ...     trial_status="completed",
    ...     intervention="Drug X 100mg daily",
    ...     source="clinicaltrials.gov",
    ...     created_at=datetime.now()
    ... )

**Fields:**

```python
nct_id: Optional[str] = None
title: Optional[str] = None
phase: Optional[str] = None
trial_status: Optional[str] = None
intervention: Optional[str] = None
```


## `class Institution(BaseEntity)`

Represents research institutions and affiliations.

Attributes:
    country: Country location
    department: Department or division

Example:
    >>> institution = Institution(
    ...     entity_id="INST:harvard_med",
    ...     name="Harvard Medical School",
    ...     country="USA",
    ...     department="Oncology",
    ...     source="extracted",
    ...     created_at=datetime.now()
    ... )

**Fields:**

```python
country: Optional[str] = None
department: Optional[str] = None
```


## `class Hypothesis(BaseEntity)`

Represents a scientific hypothesis tracked across the literature.

Uses IAO (Information Artifact Ontology) for standardized representation
of hypotheses as information content entities. Enables tracking of
hypothesis evolution: from proposal through testing to acceptance/refutation.

Attributes:
    iao_id: IAO identifier (typically IAO:0000018 for hypothesis)
    sepio_id: SEPIO identifier for assertions (SEPIO:0000001)
    proposed_by: Paper ID where hypothesis was first proposed
    proposed_date: Date when hypothesis was first proposed
    hypothesis_status: Current status (proposed, supported, controversial, refuted)
    description: Natural language description of the hypothesis
    predicts: List of entity IDs that this hypothesis predicts outcomes for

Example:
    >>> hypothesis = Hypothesis(
    ...     entity_id="HYPOTHESIS:amyloid_cascade_alzheimers",
    ...     name="Amyloid Cascade Hypothesis",
    ...     iao_id="IAO:0000018",
    ...     sepio_id="SEPIO:0000001",
    ...     proposed_by="PMC123456",
    ...     proposed_date="1992",
    ...     hypothesis_status="controversial",
    ...     description="Beta-amyloid accumulation drives Alzheimer's disease pathology",
    ...     predicts=["C0002395"],
    ...     source="extracted",
    ...     created_at=datetime.now()
    ... )

**Fields:**

```python
iao_id: Optional[str] = None
sepio_id: Optional[str] = None
proposed_by: Optional[str] = None
proposed_date: Optional[str] = None
hypothesis_status: Optional[str] = None
description: Optional[str] = None
predicts: List[str] = []
```


## `class StudyDesign(BaseEntity)`

Represents a study design or experimental protocol.

Uses OBI (Ontology for Biomedical Investigations) to standardize
study design classifications. Enables filtering by evidence quality
based on study design.

Attributes:
    obi_id: OBI identifier for study design type
    stato_id: STATO identifier for study design (if applicable)
    design_type: Human-readable design type
    description: Description of the study design
    evidence_level: Quality level (1-5, where 1 is highest quality)

Example:
    >>> rct = StudyDesign(
    ...     entity_id="OBI:0000008",
    ...     name="Randomized Controlled Trial",
    ...     obi_id="OBI:0000008",
    ...     stato_id="STATO:0000402",
    ...     design_type="interventional",
    ...     evidence_level=1,
    ...     source="obi",
    ...     created_at=datetime.now()
    ... )

**Fields:**

```python
obi_id: Optional[str] = None
stato_id: Optional[str] = None
design_type: Optional[str] = None
description: Optional[str] = None
evidence_level: Optional[int] = None
```


## `class StatisticalMethod(BaseEntity)`

Represents a statistical method or test used in analysis.

Uses STATO (Statistics Ontology) to standardize statistical method
classifications. Enables tracking of analytical approaches across studies.

Attributes:
    stato_id: STATO identifier for the statistical method
    method_type: Category of method (hypothesis_test, regression, etc.)
    description: Description of the method
    assumptions: Key assumptions of the method

Example:
    >>> ttest = StatisticalMethod(
    ...     entity_id="STATO:0000288",
    ...     name="Student's t-test",
    ...     stato_id="STATO:0000288",
    ...     method_type="hypothesis_test",
    ...     description="Parametric test comparing means of two groups",
    ...     source="stato",
    ...     created_at=datetime.now()
    ... )

**Fields:**

```python
stato_id: Optional[str] = None
method_type: Optional[str] = None
description: Optional[str] = None
assumptions: List[str] = []
```


## `class EvidenceLine(BaseEntity)`

Represents a line of evidence using SEPIO framework.

Uses SEPIO (Scientific Evidence and Provenance Information Ontology)
to represent structured evidence chains. Links evidence items to
assertions they support or refute.

Attributes:
    sepio_type: SEPIO evidence line type ID
    eco_type: ECO evidence type ID
    assertion_id: ID of the assertion this evidence supports
    supports_ids: List of hypothesis IDs this evidence supports
    refutes_ids: List of hypothesis IDs this evidence refutes
    evidence_items: List of paper IDs providing evidence
    strength: Evidence strength classification
    provenance_info: Provenance information

Example:
    >>> evidence = EvidenceLine(
    ...     entity_id="EVIDENCE_LINE:olaparib_brca_001",
    ...     name="Clinical evidence for Olaparib in BRCA-mutated breast cancer",
    ...     sepio_type="SEPIO:0000084",
    ...     eco_type="ECO:0007673",
    ...     assertion_id="ASSERTION:olaparib_brca",
    ...     supports_ids=["HYPOTHESIS:parp_inhibitor_synthetic_lethality"],
    ...     evidence_items=["PMC999888", "PMC888777"],
    ...     strength="strong",
    ...     source="extracted",
    ...     created_at=datetime.now()
    ... )

**Fields:**

```python
sepio_type: Optional[str] = None
eco_type: Optional[str] = None
assertion_id: Optional[str] = None
supports_ids: List[str] = []
refutes_ids: List[str] = []
evidence_items: List[str] = []
strength: Optional[str] = None
provenance_info: Optional[str] = None
```


## `class Evidence(BaseEntity)`

Evidence for a relationship, treated as a first-class entity.

Evidence entities have immediate canonical ID promotion using format:
{paper_id}:{section}:{paragraph}:{method}

Example canonical ID: "PMC8437152:results:5:llm"

This format enables:
- Immediate promotion (no provisional state needed)
- Efficient lookups by paper/section
- Deduplication across extraction runs
- Database indexing for queries like "all evidence from Section 2"

Attributes:
    entity_id: Canonical ID in format {paper_id}:{section}:{paragraph}:{method}
    paper_id: PMC ID of source paper
    text_span_id: Reference to TextSpan entity (for exact location)
    confidence: Confidence score 0.0-1.0
    extraction_method: Method used (scispacy_ner, llm, table_parser, pattern_match, manual)
    study_type: Type of study (observational, rct, meta_analysis, case_report, review)
    sample_size: Number of subjects in the study
    eco_type: ECO evidence type ID (e.g., "ECO:0007673" for RCT)
    obi_study_design: OBI study design ID (e.g., "OBI:0000008" for RCT)
    stato_methods: List of STATO statistical method IDs used

Schema Rules:
- entity_id MUST follow canonical ID format
- paper_id and text_span_id MUST be non-empty
- Evidence entities are immediately promotable (no usage threshold)

Example:
    >>> evidence = Evidence(
    ...     entity_id="PMC999888:results:3:llm",
    ...     name="Evidence from Olaparib RCT results",
    ...     paper_id="PMC999888",
    ...     text_span_id="PMC999888:results:3",
    ...     confidence=0.92,
    ...     extraction_method=ExtractionMethod.LLM,
    ...     study_type=StudyType.RCT,
    ...     sample_size=302,
    ...     eco_type="ECO:0007673",
    ...     obi_study_design="OBI:0000008",
    ...     stato_methods=["STATO:0000288"],
    ...     source="extracted",
    ...     created_at=datetime.now()
    ... )

**Fields:**

```python
promotable: bool = False
status: EntityStatus = EntityStatus.CANONICAL
paper_id: str
text_span_id: str
confidence: float
extraction_method: 'ExtractionMethod'
study_type: 'StudyType'
sample_size: Optional[int] = None
eco_type: Optional[str] = None
obi_study_design: Optional[str] = None
stato_methods: List[str] = []
```



<span id="user-content-examplesmedlitschemainitpy"></span>

# examples/medlit_schema/__init__.py

Medical Literature Domain Schema for kgraph.

This package provides production-ready schema definitions for medical
literature knowledge graphs, with full provenance tracking, ontology
integration, and evidence-based relationships.

Schema version: 1.0.0
Compatible with: kgschema >=0.2.0
Ontologies: UMLS, HGNC, RxNorm, UniProt, ECO, OBI, STATO, SEPIO


<span id="user-content-examplesmedlitschemaontologyguidemd"></span>

# examples/medlit_schema/ONTOLOGY_GUIDE.md

# MedLit Schema Ontology Integration Guide

This document outlines how the MedLit schema integrates with standard biomedical ontologies to ensure data quality, interoperability, and semantic richness.

## Core Principle: Canonicalization

A central goal of the schema is to move from provisional, text-extracted entities to canonical entities linked to established ontology identifiers. This process is managed through the `source` field in `BaseMedicalEntity` and entity-specific identifier fields.

-   **Provisional Entities**: When an entity is first extracted from text, it is considered "provisional." Its `source` is set to `"extracted"`, and it may not have a canonical ID.
-   **Canonical Entities**: Once the entity is resolved to a specific ontology concept, its `source` is updated (e.g., to `"umls"`, `"hgnc"`), and the corresponding ID field is populated.

The schema includes a Pydantic validator that enforces this: a canonical entity *must* have an associated ontology ID.

## Key Ontologies Used

The schema leverages several key ontologies, each tailored to a specific type of entity.

### For Core Biomedical Entities

| Entity    | Primary Ontology | ID Field(s)                             | Description                                                                                             |

    ...


<span id="user-content-examplesmedlitschemaprogressmd"></span>

# examples/medlit_schema/PROGRESS.md

# DEPTH_OF_FIELDS Implementation Progress

**Schema Version**: 1.0.0
**Started**: 2026-02-03
**Target**: ~3,175 LOC across 6 phases

---

## Phase 1: Enhance Base Models & Types (base.py) ✅ COMPLETE
**Target**: ~300 lines | **Actual**: +216 lines | **Time**: ~1 hour

- [x] Task 1.1: Expand PredicateType Enum (~40 predicates) - DONE
- [x] Task 1.2: Add EntityType Enum - DONE
- [x] Task 1.3: Add Supporting Models (ClaimPredicate, Provenance, EvidenceType, EntityReference, Polarity, Edge hierarchy) - DONE

---

## Phase 2: Enrich Entity Definitions (entity.py) ✅ COMPLETE
**Target**: ~940 lines | **Actual**: ~700 lines | **Time**: ~3 hours

    ...


<span id="user-content-examplesmedlitschemareadmemd"></span>

# examples/medlit_schema/README.md

# MedLit Schema

**Version**: 1.0.0

This directory contains the domain-specific schema for representing knowledge graphs of medical literature, serving as a `definitions-only` package. It extends the core `kgschema` with rich, domain-specific types for entities and relationships tailored to the biomedical field.

## Core Design Principles

1.  **Evidence as a First-Class Entity**: All medical claims (relationships) must be backed by evidence. The `Evidence` entity is a cornerstone of this schema, providing traceability from a claim back to its source in the literature.
2.  **Rich, Composable Models**: Entities and relationships are modeled as Pydantic classes, enabling validation, type safety, and easy composition.
3.  **Ontology Integration**: The schema is designed to integrate with major biomedical ontologies (UMLS, HGNC, RxNorm, UniProt, etc.) for entity normalization and canonicalization.
4.  **Traceability**: A primary goal is to ensure that any piece of information can be traced back to its source paper, section, and paragraph. The linkage `Relationship -> Evidence -> TextSpan -> Paper` is central to this.
5.  **Separation of Concerns**: This package contains only schema definitions (Pydantic models and domain registration). All implementation logic (ingestion, storage, querying) resides in other parts of the `kgraph` framework, such as `examples/medlit`.

## Schema Structure

The schema is organized into the following key files:

-   `entity.py`: Defines all entity types, from core biomedical concepts like `Disease`, `Gene`, and `Drug` to bibliographic entities like `Paper` and `Author`, and scientific concepts like `Hypothesis` and `Evidence`.
-   `relationship.py`: Defines the connections between entities. These range from medical relationships (`TREATS`, `CAUSES`) to biological (`ENCODES`) and bibliographic (`AUTHORED_BY`) ones. It also includes a factory function `create_relationship` for creating typed relationship instances.

    ...


<span id="user-content-examplesmedlitschemarelationshippy"></span>

# examples/medlit_schema/relationship.py

Medlit relationship definitions.

## `class EvidenceItem(BaseModel)`

Lightweight evidence reference for relationships.

Attributes:
    paper_id: PMC ID of source paper
    study_type: Type of study (observational, rct, meta_analysis, case_report, review)
    sample_size: Number of subjects in the study
    confidence: Confidence score (0.0-1.0)

**Fields:**

```python
paper_id: str
study_type: str
sample_size: Optional[int] = None
confidence: float = 0.5
```


## `class BaseMedicalRelationship(BaseRelationship)`

Base class for all medical relationships with comprehensive provenance tracking.

All medical relationships inherit from this class and include evidence-based
provenance fields to support confidence scoring, contradiction detection,
and temporal tracking of medical knowledge.

Combines lightweight tracking (just paper IDs) with optional rich provenance
(detailed Evidence objects) and quantitative measurements.

Schema Rules:
- Medical assertion relationships MUST have non-empty evidence_ids
- Bibliographic relationships (AuthoredBy, Cites) do NOT require evidence

Attributes:
    subject_id: Entity ID of the subject (source node)
    predicate: Relationship type
    object_id: Entity ID of the object (target node)
    evidence_ids: REQUIRED list of Evidence entity IDs (for medical assertions)
    confidence: Confidence score (0.0-1.0) based on evidence strength
    source_papers: List of PMC IDs supporting this relationship (lightweight)
    evidence_count: Number of papers providing supporting evidence
    contradicted_by: List of PMC IDs with contradicting findings
    first_reported: Date when this relationship was first observed
    last_updated: Date of most recent supporting evidence
    evidence: List of detailed EvidenceItem objects (optional, for rich provenance)
    measurements: List of quantitative measurements (optional)
    properties: Flexible dict for relationship-specific properties

Example (lightweight):
    >>> relationship = Treats(
    ...     subject_id="RxNorm:1187832",
    ...     predicate="TREATS",
    ...     object_id="C0006142",
    ...     evidence_ids=["PMC123:results:5:llm", "PMC456:abstract:2:llm"],
    ...     source_papers=["PMC123", "PMC456"],
    ...     confidence=0.85,
    ...     evidence_count=2,
    ...     response_rate=0.59
    ... )

Example (rich provenance):
    >>> relationship = Treats(
    ...     subject_id="RxNorm:1187832",
    ...     predicate="TREATS",
    ...     object_id="C0006142",
    ...     evidence_ids=["PMC123:results:5:rct"],
    ...     confidence=0.85,
    ...     evidence=[EvidenceItem(paper_id="PMC123", study_type="rct", sample_size=302)],
    ...     measurements=[Measurement(value=0.59, value_type="response_rate")],
    ...     response_rate=0.59
    ... )

### `def BaseMedicalRelationship.evidence_required_for_medical_assertions(cls, v)`

Medical assertion relationships must include evidence.

This validator is overridden in non-medical relationship classes
(like ResearchRelationship) that don't require evidence.

## `class Treats(BaseMedicalRelationship)`

Represents a therapeutic relationship between a drug and a disease.

Direction: Drug → Disease

Attributes:
    efficacy: Effectiveness measure or description
    response_rate: Percentage of patients responding (0.0-1.0)
    line_of_therapy: Treatment sequence (first-line, second-line, etc.)
    indication: Specific approved use or condition

Example:
    >>> treats = Treats(
    ...     subject_id="RxNorm:1187832",  # Olaparib
    ...     object_id="C0006142",  # Breast Cancer
    ...     predicate="TREATS",
    ...     evidence_ids=["PMC999:results:5:rct", "PMC888:abstract:2:rct"],
    ...     efficacy="significant improvement in PFS",
    ...     response_rate=0.59,
    ...     line_of_therapy="second-line",
    ...     indication="BRCA-mutated breast cancer",
    ...     source_papers=["PMC999", "PMC888"],
    ...     confidence=0.85
    ... )

## `class Causes(BaseMedicalRelationship)`

Represents a causal relationship between a disease and a symptom.

Direction: Disease → Symptom (or Gene/Mutation → Disease)

Attributes:
    frequency: How often the symptom occurs (always, often, sometimes, rarely)
    onset: When the symptom typically appears (early, late)
    severity: Typical severity of the symptom (mild, moderate, severe)

Example:
    >>> causes = Causes(
    ...     subject_id="C0006142",  # Breast Cancer
    ...     object_id="C0030193",  # Pain
    ...     predicate="CAUSES",
    ...     evidence_ids=["PMC123:results:3:llm"],
    ...     frequency="often",
    ...     onset="late",
    ...     severity="moderate",
    ...     source_papers=["PMC123"],
    ...     confidence=0.75
    ... )

## `class Prevents(BaseMedicalRelationship)`

Drug prevents disease relationship.

Direction: Drug → Disease

Attributes:
    efficacy: Effectiveness measure or description
    risk_reduction: Risk reduction percentage (0.0-1.0)

## `class IncreasesRisk(BaseMedicalRelationship)`

Represents genetic risk factors for diseases.

Direction: Gene/Mutation → Disease

Attributes:
    risk_ratio: Numeric risk increase (e.g., 2.5 means 2.5x higher risk)
    penetrance: Percentage who develop condition (0.0-1.0)
    age_of_onset: Typical age when disease manifests
    population: Studied population or ethnic group

Example:
    >>> risk = IncreasesRisk(
    ...     subject_id="HGNC:1100",  # BRCA1
    ...     object_id="C0006142",  # Breast Cancer
    ...     predicate="INCREASES_RISK",
    ...     evidence_ids=["PMC123:results:7:llm", "PMC456:discussion:2:llm"],
    ...     risk_ratio=5.0,
    ...     penetrance=0.72,
    ...     age_of_onset="40-50 years",
    ...     population="Ashkenazi Jewish",
    ...     source_papers=["PMC123", "PMC456"],
    ...     confidence=0.92
    ... )

## `class SideEffect(BaseMedicalRelationship)`

Represents adverse effects of medications.

Direction: Drug → Symptom

Attributes:
    frequency: How often it occurs (common, uncommon, rare)
    severity: Severity level (mild, moderate, severe)
    reversible: Whether the side effect resolves after stopping the drug

Example:
    >>> side_effect = SideEffect(
    ...     subject_id="RxNorm:1187832",  # Olaparib
    ...     object_id="C0027497",  # Nausea
    ...     predicate="SIDE_EFFECT",
    ...     evidence_ids=["PMC999:results:8:llm"],
    ...     frequency="common",
    ...     severity="mild",
    ...     reversible=True,
    ...     source_papers=["PMC999"],
    ...     confidence=0.75
    ... )

## `class AssociatedWith(BaseMedicalRelationship)`

Represents a general association between entities.

This is used for relationships where causality is not established but
statistical association exists.

Valid directions:
    - Disease → Disease (comorbidities)
    - Gene → Disease
    - Biomarker → Disease

Attributes:
    association_type: Nature of association (positive, negative, neutral)
    strength: Association strength (strong, moderate, weak)
    statistical_significance: p-value from statistical tests

Example:
    >>> assoc = AssociatedWith(
    ...     subject_id="C0011849",  # Diabetes
    ...     object_id="C0020538",  # Hypertension
    ...     predicate="ASSOCIATED_WITH",
    ...     evidence_ids=["PMC111:results:4:llm"],
    ...     association_type="positive",
    ...     strength="strong",
    ...     statistical_significance=0.001,
    ...     source_papers=["PMC111"],
    ...     confidence=0.80
    ... )

## `class InteractsWith(BaseMedicalRelationship)`

Represents drug-drug interactions.

Direction: Drug ↔ Drug (bidirectional)

Attributes:
    interaction_type: Nature of interaction (synergistic, antagonistic, additive)
    severity: Clinical severity (major, moderate, minor)
    mechanism: Pharmacological mechanism of interaction
    clinical_significance: Description of clinical implications

Example:
    >>> interaction = InteractsWith(
    ...     subject_id="RxNorm:123",  # Warfarin
    ...     object_id="RxNorm:456",  # Aspirin
    ...     predicate="INTERACTS_WITH",
    ...     evidence_ids=["PMC789:discussion:3:llm"],
    ...     interaction_type="synergistic",
    ...     severity="major",
    ...     mechanism="Additive anticoagulant effect",
    ...     clinical_significance="Increased bleeding risk",
    ...     source_papers=["PMC789"],
    ...     confidence=0.90
    ... )

## `class ContraindicatedFor(BaseMedicalRelationship)`

Drug -[CONTRAINDICATED_FOR]-> Disease/Condition

Attributes:
    severity: Contraindication severity (absolute, relative)
    reason: Why contraindicated

## `class DiagnosedBy(BaseMedicalRelationship)`

Represents diagnostic tests or biomarkers used to diagnose a disease.

Direction: Disease → Procedure/Biomarker

Attributes:
    sensitivity: True positive rate (0.0-1.0)
    specificity: True negative rate (0.0-1.0)
    standard_of_care: Whether this is standard clinical practice

Example:
    >>> diagnosis = DiagnosedBy(
    ...     subject_id="C0006142",  # Breast Cancer
    ...     object_id="LOINC:123",  # Mammography
    ...     predicate="DIAGNOSED_BY",
    ...     evidence_ids=["PMC555:methods:2:llm"],
    ...     sensitivity=0.87,
    ...     specificity=0.91,
    ...     standard_of_care=True,
    ...     source_papers=["PMC555"],
    ...     confidence=0.88
    ... )

## `class ParticipatesIn(BaseMedicalRelationship)`

Gene/Protein -[PARTICIPATES_IN]-> Pathway

Attributes:
    role: Function in pathway
    regulatory_effect: Type of regulation (activates, inhibits, modulates)

## `class SubtypeOf(BaseMedicalRelationship)`

When one disease is a subtype of another disease

## `class ResearchRelationship(BaseRelationship)`

Base class for research metadata relationships.

These relationships connect papers, authors, and clinical trials.
Unlike medical relationships, they don't require provenance tracking
since they represent bibliographic metadata rather than medical claims.

Attributes:
    subject_id: ID of the subject entity
    predicate: Relationship type
    object_id: ID of the object entity
    properties: Flexible dict for relationship-specific properties

## `class Cites(ResearchRelationship)`

Represents a citation from one paper to another.

Direction: Paper → Paper (citing → cited)

Attributes:
    context: Section where citation appears
    sentiment: How the citation is used (supports, contradicts, mentions)

## `class StudiedIn(ResearchRelationship)`

Links medical entities to papers that study them.

Direction: Any medical entity → Paper

Attributes:
    role: Importance in the paper (primary_focus, secondary_finding, mentioned)
    section: Where discussed (results, methods, discussion, introduction)

## `class AuthoredBy(ResearchRelationship)`

Paper -[AUTHORED_BY]-> Author

Attributes:
    position: Author position (first, last, corresponding, middle)

## `class PartOf(ResearchRelationship)`

Paper -[PART_OF]-> ClinicalTrial

Attributes:
    publication_type: Type of publication (protocol, results, analysis)

## `class Predicts(BaseMedicalRelationship)`

Represents a hypothesis predicting an observable outcome.

Direction: Hypothesis → Entity (Disease, Outcome, etc.)

Attributes:
    prediction_type: Nature of prediction (positive, negative, conditional)
    conditions: Conditions under which prediction holds
    testable: Whether the prediction is empirically testable

## `class Refutes(BaseMedicalRelationship)`

Represents evidence that refutes a hypothesis.

Direction: Evidence/Paper → Hypothesis

Attributes:
    refutation_strength: Strength of refutation (strong, moderate, weak)
    alternative_explanation: Alternative explanation for observations
    limitations: Limitations of the refuting evidence

## `class TestedBy(BaseMedicalRelationship)`

Represents a hypothesis being tested by a study or clinical trial.

Direction: Hypothesis → Paper/ClinicalTrial

Attributes:
    test_outcome: Result of the test (supported, refuted, inconclusive)
    methodology: Study methodology used
    study_design_id: OBI study design ID

## `class Supports(BaseMedicalRelationship)`

Evidence supports a hypothesis or claim.

Direction: Evidence → Hypothesis

Attributes:
    support_strength: Strength of support (strong, moderate, weak)

## `class Generates(BaseMedicalRelationship)`

Represents a study generating evidence for analysis.

Direction: ClinicalTrial/Paper → Evidence

Attributes:
    evidence_type: Type of evidence generated (experimental, observational, etc.)
    eco_type: ECO evidence type ID
    quality_score: Quality assessment score

### `def create_relationship(predicate: str, subject_id: str, object_id: str, **kwargs) -> BaseRelationship`

Factory function for creating typed relationship instances.

Provides type-safe relationship creation with predicate validation.
Returns the appropriate relationship subclass based on predicate.

Args:
    predicate: Relationship type (must match RELATIONSHIP_TYPE_MAP keys)
    subject_id: Entity ID of the subject
    object_id: Entity ID of the object
    **kwargs: Relationship-specific fields (evidence_ids, confidence, etc.)

Returns:
    Typed relationship instance (Treats, Causes, Cites, etc.)

Raises:
    ValueError: If predicate is not recognized

Example:
    >>> rel = create_relationship(
    ...     predicate="TREATS",
    ...     subject_id="RxNorm:1187832",
    ...     object_id="C0006142",
    ...     evidence_ids=["PMC123:results:5:rct"],
    ...     response_rate=0.59,
    ...     confidence=0.85
    ... )
    >>> isinstance(rel, Treats)
    True


<span id="user-content-examplesmedlitschemastoragepy"></span>

# examples/medlit_schema/storage.py



<span id="user-content-examplesmedlitscriptsingestpy"></span>

# examples/medlit/scripts/ingest.py

Ingestion script for medical literature knowledge graph.

Processes Paper JSON files (from med-lit-schema) and generates a kgraph bundle.

The pipeline has three stages:
    1. Entity Extraction (per-paper): Extract entities, most provisional initially
    2. Promotion (batch): De-duplicate and promote provisionals to canonical
    3. Relationship Extraction (per-paper): Extract relationships using canonical entities

Use --stop-after to halt at any stage and dump JSON to stdout for debugging/testing.

Usage:
    # Full pipeline
    python -m examples.medlit.scripts.ingest --input-dir /path/to/papers --output-dir medlit_bundle --use-ollama

    # Stop after entity extraction and dump JSON
    python -m examples.medlit.scripts.ingest --input-dir /path/to/papers --use-ollama --stop-after entities

    # Stop after promotion and dump JSON
    python -m examples.medlit.scripts.ingest --input-dir /path/to/papers --use-ollama --stop-after promotion

## `class TraceCollector`

Collects paths to trace files written during ingestion.

Each ingestion run gets a unique UUID, and trace files are organized as:
/tmp/kgraph-traces/{run_id}/entities/{doc_id}.entities.trace.json
/tmp/kgraph-traces/{run_id}/promotion/promotions.trace.json
/tmp/kgraph-traces/{run_id}/relationships/{doc_id}.relationships.trace.json

### `def TraceCollector.trace_dir(self) -> Path`

Get the trace directory for this run.

### `def TraceCollector.entity_trace_dir(self) -> Path`

Get the entity trace directory for this run.

### `def TraceCollector.promotion_trace_dir(self) -> Path`

Get the promotion trace directory for this run.

### `def TraceCollector.relationship_trace_dir(self) -> Path`

Get the relationship trace directory for this run.

### `def TraceCollector.add(self, path: Path) -> None`

Add a trace file path.

### `def TraceCollector.collect_from_directory(self, directory: Path, pattern: str = '*.trace.json') -> None`

Collect all trace files matching pattern from a directory.

### `def TraceCollector.print_summary(self) -> None`

Print summary of all trace files written.

## `class ProgressTracker`

Track and report progress during long-running operations.

### `def ProgressTracker.increment(self) -> None`

Increment completed count and report if interval elapsed.

### `def ProgressTracker.report(self) -> None`

Print progress report to stderr.

### `def build_orchestrator(use_ollama: bool = False, ollama_model: str = 'llama3.1:8b', ollama_host: str = 'http://localhost:11434', ollama_timeout: float = 300.0, cache_file: Path | None = None, relationship_trace_dir: Path | None = None, embeddings_cache_file: Path | None = None, evidence_validation_mode: str = 'hybrid', evidence_similarity_threshold: float = 0.5) -> tuple[IngestionOrchestrator, CanonicalIdLookup | None, CachedEmbeddingGenerator | None]`

Builds and configures the ingestion orchestrator and its components.

This function sets up the entire pipeline, including storage,
extractors, resolvers, and the main orchestrator instance.

Args:
    use_ollama: If True, initializes the Ollama LLM client for extraction tasks.
                This is mandatory for the current entity and relationship
                extraction strategies.
    ollama_model: The name of the Ollama model to use (e.g., "llama3.1:8b").
    ollama_host: The URL of the Ollama server.
    ollama_timeout: The timeout in seconds for requests to the Ollama server.
    cache_file: An optional path to a file for caching canonical ID lookups.
                This is not used during initialization but passed for later use.
    relationship_trace_dir: Optional directory for writing relationship trace files.
                            If None, uses the default location.
    embeddings_cache_file: Optional path for a persistent embeddings cache (JSON).
                           If set, wraps the embedding generator with
                           CachedEmbeddingGenerator + FileBasedEmbeddingsCache.

Returns:
    A tuple containing:
    - An instance of `IngestionOrchestrator` configured for the pipeline.
    - `None`, as the `CanonicalIdLookup` service is initialized later,
      just before the promotion phase.
    - The CachedEmbeddingGenerator if embeddings_cache_file was set, else None
      (caller should await cache.load() before use and save_cache() when done).

### `def parse_arguments() -> argparse.Namespace`

Parses and validates command-line arguments for the ingestion script.

Returns:
    An `argparse.Namespace` object containing the parsed arguments.

### `def find_input_files(input_dir: Path, limit: int | None, input_papers: str | None = None) -> list[tuple[Path, str]]`

Finds all processable JSON and XML files in the input directory.

Args:
    input_dir: The directory to search for input files.
    limit: An optional integer to limit the number of files returned.
    input_papers: Optional comma-separated glob patterns to filter files,
                  e.g. 'PMC1234*.xml,PMC56*.xml'

Returns:
    A sorted list of tuples, where each tuple contains:
    - A `Path` object for a found file.
    - A string with the file's MIME content type.

### `def _initialize_lookup(use_ollama: bool, cache_file: Path | None, quiet: bool, embedding_generator: Any = None) -> CanonicalIdLookup | None`

Initializes the canonical ID lookup service.

### `def _build_promoted_records(promoted: list) -> list[PromotedEntityRecord]`

Builds a list of promoted entity records from a list of promoted entities.

### `def _handle_keyboard_interrupt(lookup: CanonicalIdLookup | None) -> None`

Handles graceful shutdown on KeyboardInterrupt (Ctrl+C).

This function is registered as an exception handler to ensure that the
canonical ID lookup cache is saved before the program exits, preventing
loss of work.

Args:
    lookup: The `CanonicalIdLookup` instance, which contains the cache
            to be saved.

### `def _output_stage_result(result: BaseModel, stage_name: str, quiet: bool) -> None`

Output stage result as JSON to stdout.

### `def _initialize_pipeline(args: argparse.Namespace) -> tuple`

Initializes the pipeline and returns necessary components.


<span id="user-content-examplesmedlitscriptsinitpy"></span>

# examples/medlit/scripts/__init__.py

Scripts for medical literature ingestion.


<span id="user-content-examplesmedlitscriptsparsepmcxmlpy"></span>

# examples/medlit/scripts/parse_pmc_xml.py

Parse PMC JATS-XML files directly to Paper schema JSON format.

This script combines XML parsing and schema conversion into a single step,
converting JATS-XML files directly to the format expected by JournalArticleParser.

### `def parse_pmc_xml_to_paper_schema(xml_path: Path) -> dict`

Parse PMC XML file directly into Paper schema JSON format.

Args:
    xml_path: Path to the PMC XML file

Returns:
    Dictionary in Paper schema format with:
    - paper_id: PMC ID (from filename)
    - title: Article title
    - abstract: Dict with "text" key containing abstract
    - full_text: Full body text (if available)
    - authors: List of author names
    - metadata: Dict with keywords (if available)


<span id="user-content-examplesmedlitstagemodelspy"></span>

# examples/medlit/stage_models.py

Pydantic models for ingestion pipeline stage outputs.

These models capture the state of the pipeline at each stage, enabling:
1. Validation of intermediate results
2. JSON serialization for debugging and testing
3. Stopping the pipeline at any stage and dumping state to stdout

Stage Flow:
    Stage 1 (entities): Per-paper entity extraction
    Stage 2 (promotion): Batch de-duplication and promotion across all papers
    Stage 3 (relationships): Per-paper relationship extraction

Each stage produces a model that can be serialized to JSON for inspection.

## `class IngestionStage(str, Enum)`

Pipeline stages where ingestion can be stopped.

## `class ExtractedEntityRecord(BaseModel)`

Record of a single extracted entity.

**Fields:**

```python
entity_id: str = Field(...)
name: str = Field(...)
entity_type: str = Field(...)
status: str = Field(...)
confidence: float = Field(...)
source: str = Field(...)
canonical_ids: dict[str, str] = Field(default_factory=dict)
synonyms: tuple[str, ...] = ()
metadata: dict[str, Any] = Field(default_factory=dict)
```


## `class PaperEntityExtractionResult(BaseModel)`

Result of entity extraction from a single paper.

**Fields:**

```python
document_id: str = Field(...)
source_uri: str | None = None
extracted_at: datetime = Field(...)
entities_extracted: int = Field(...)
entities_new: int = Field(...)
entities_existing: int = Field(...)
entities: tuple[ExtractedEntityRecord, ...] = ()
errors: tuple[str, ...] = ()
```


## `class EntityExtractionStageResult(BaseModel)`

Complete result of Stage 1: Entity Extraction across all papers.

This model captures the state after all papers have been processed
for entity extraction, but before promotion.

**Fields:**

```python
stage: str = 'entities'
completed_at: datetime = Field(...)
papers_processed: int = Field(...)
papers_failed: int = Field(...)
total_entities_extracted: int = Field(...)
total_entities_new: int = Field(...)
total_entities_existing: int = Field(...)
paper_results: tuple[PaperEntityExtractionResult, ...] = ()
entity_type_counts: dict[str, int] = Field(default_factory=dict)
provisional_count: int = 0
canonical_count: int = 0
```


## `class PromotedEntityRecord(BaseModel)`

Record of an entity that was promoted to canonical status.

**Fields:**

```python
old_entity_id: str = Field(...)
new_entity_id: str = Field(...)
name: str = Field(...)
entity_type: str = Field(...)
canonical_source: str = Field(...)
canonical_url: str | None = None
```


## `class PromotionStageResult(BaseModel)`

Complete result of Stage 2: Entity Promotion.

This model captures the state after provisional entities have been
de-duplicated and promoted to canonical status.

**Fields:**

```python
stage: str = 'promotion'
completed_at: datetime = Field(...)
candidates_evaluated: int = Field(...)
entities_promoted: int = Field(...)
entities_skipped_no_canonical_id: int = 0
entities_skipped_policy: int = 0
entities_skipped_storage_failure: int = 0
promoted_entities: tuple[PromotedEntityRecord, ...] = ()
total_canonical_entities: int = Field(...)
total_provisional_entities: int = Field(...)
```


## `class ExtractedRelationshipRecord(BaseModel)`

Record of a single extracted relationship.

**Fields:**

```python
subject_id: str = Field(...)
subject_name: str = Field(...)
subject_type: str = Field(...)
predicate: str = Field(...)
object_id: str = Field(...)
object_name: str = Field(...)
object_type: str = Field(...)
confidence: float = Field(...)
source_document: str = Field(...)
evidence_quote: str | None = None
metadata: dict[str, Any] = Field(default_factory=dict)
```


## `class PaperRelationshipExtractionResult(BaseModel)`

Result of relationship extraction from a single paper.

**Fields:**

```python
document_id: str = Field(...)
source_uri: str | None = None
extracted_at: datetime = Field(...)
relationships_extracted: int = Field(...)
relationships: tuple[ExtractedRelationshipRecord, ...] = ()
errors: tuple[str, ...] = ()
```


## `class RelationshipExtractionStageResult(BaseModel)`

Complete result of Stage 3: Relationship Extraction.

This model captures the final state after relationship extraction.

**Fields:**

```python
stage: str = 'relationships'
completed_at: datetime = Field(...)
papers_processed: int = Field(...)
papers_with_relationships: int = Field(...)
total_relationships_extracted: int = Field(...)
paper_results: tuple[PaperRelationshipExtractionResult, ...] = ()
predicate_counts: dict[str, int] = Field(default_factory=dict)
```


## `class IngestionPipelineResult(BaseModel)`

Complete result of the full ingestion pipeline.

Combines results from all three stages for final output.

**Fields:**

```python
pipeline_version: str = '1.0.0'
started_at: datetime = Field(...)
completed_at: datetime = Field(...)
stopped_at_stage: str | None = None
entity_extraction: EntityExtractionStageResult | None = None
promotion: PromotionStageResult | None = None
relationship_extraction: RelationshipExtractionStageResult | None = None
total_documents: int = Field(...)
total_entities: int = Field(...)
total_relationships: int = Field(...)
```



<span id="user-content-examplesmedlittestsconftestpy"></span>

# examples/medlit/tests/conftest.py

Conftest for medlit tests - imports fixtures from main conftest.


<span id="user-content-examplesmedlittestsinitpy"></span>

# examples/medlit/tests/__init__.py

Tests for the medlit example application.


<span id="user-content-examplesmedlitteststestauthoritylookuppy"></span>

# examples/medlit/tests/test_authority_lookup.py

Tests for canonical ID authority lookup.

Tests the matching logic for DBPedia and other ontology lookups.

## `class TestDBPediaLabelMatching`

Test the DBPedia label matching logic.

### `def TestDBPediaLabelMatching.lookup(self)`

Create a CanonicalIdLookup instance for testing.

### `def TestDBPediaLabelMatching.test_exact_match(self, lookup)`

Exact match should succeed.

### `def TestDBPediaLabelMatching.test_term_contained_in_label(self, lookup)`

Term contained in label should succeed.

### `def TestDBPediaLabelMatching.test_label_contained_in_term(self, lookup)`

Label contained in term should succeed.

### `def TestDBPediaLabelMatching.test_label_starts_with_term(self, lookup)`

Label starting with term should succeed.

### `def TestDBPediaLabelMatching.test_common_prefix_singular_plural(self, lookup)`

Common 6-char prefix should succeed (handles singular/plural).

### `def TestDBPediaLabelMatching.test_html_tags_stripped(self, lookup)`

HTML bold tags should be stripped from labels.

### `def TestDBPediaLabelMatching.test_case_insensitive(self, lookup)`

Matching should be case-insensitive.

### `def TestDBPediaLabelMatching.test_garbage_match_insect(self, lookup)`

Garbage match 'HER2-enriched' → 'Insect' should fail.

### `def TestDBPediaLabelMatching.test_garbage_match_animal(self, lookup)`

Garbage match 'basal-like' → 'Animal' should fail.

### `def TestDBPediaLabelMatching.test_unrelated_terms(self, lookup)`

Completely unrelated terms should fail.

### `def TestDBPediaLabelMatching.test_substring_match_allowed(self, lookup)`

Substring matching is allowed (term in label).

### `def TestDBPediaLabelMatching.test_no_overlap_fails(self, lookup)`

Terms with no overlap should fail.

## `class TestMeSHTermNormalization`

Test MeSH term normalization (cancer → neoplasms).

### `def TestMeSHTermNormalization.lookup(self)`

Create a CanonicalIdLookup instance for testing.

### `def TestMeSHTermNormalization.test_mesh_id_extraction(self, lookup)`

Test extracting MeSH ID from API results.

### `def TestMeSHTermNormalization.test_mesh_id_extraction_word_order(self, lookup)`

Test MeSH extraction handles word order differences.

### `def TestMeSHTermNormalization.test_mesh_id_extraction_no_match(self, lookup)`

Test MeSH extraction returns None for no match.

### `def TestMeSHTermNormalization.test_mesh_id_extraction_empty_data(self, lookup)`

Test MeSH extraction handles empty data.

### `def TestMeSHTermNormalization.test_mesh_id_extraction_prefers_general_over_complication(self, lookup)`

Test that general terms are preferred over complications.

"breast cancer" should match "Breast Neoplasms" (D001943)
rather than "Breast Cancer Lymphedema" (D000072656).

### `def TestMeSHTermNormalization.test_mesh_id_extraction_exact_match_priority(self, lookup)`

Test that exact matches get highest priority.


<span id="user-content-examplesmedlitteststestentitynormalizationpy"></span>

# examples/medlit/tests/test_entity_normalization.py

Tests for entity type normalization in MedLitEntityExtractor.

Tests the _normalize_entity_type() method which handles:
- Pipe-separated types from LLM output
- Common LLM mistakes (test → procedure)
- Invalid type filtering

## `class TestTypeNormalizationWithDomain`

Test entity type normalization with domain schema validation.

### `def TestTypeNormalizationWithDomain.extractor(self)`

Create extractor with domain for full validation.

### `def TestTypeNormalizationWithDomain.test_valid_type_passes_through(self, extractor)`

Valid entity types should pass through unchanged.

### `def TestTypeNormalizationWithDomain.test_case_normalization(self, extractor)`

Types should be normalized to lowercase.

### `def TestTypeNormalizationWithDomain.test_whitespace_stripped(self, extractor)`

Whitespace should be stripped from types.

### `def TestTypeNormalizationWithDomain.test_pipe_separated_takes_first_valid(self, extractor)`

Pipe-separated types should return first valid type.

### `def TestTypeNormalizationWithDomain.test_pipe_separated_skips_invalid(self, extractor)`

Pipe-separated types should skip invalid types.

### `def TestTypeNormalizationWithDomain.test_pipe_separated_all_invalid_returns_none(self, extractor)`

Pipe-separated with all invalid types should return None.

### `def TestTypeNormalizationWithDomain.test_common_mistake_test_to_procedure(self, extractor)`

'test' should be normalized to 'procedure'.

### `def TestTypeNormalizationWithDomain.test_common_mistake_diagnostic_to_procedure(self, extractor)`

'diagnostic' should be normalized to 'procedure'.

### `def TestTypeNormalizationWithDomain.test_common_mistake_imaging_to_procedure(self, extractor)`

'imaging' should be normalized to 'procedure'.

### `def TestTypeNormalizationWithDomain.test_common_mistake_assay_to_biomarker(self, extractor)`

'assay' should be normalized to 'biomarker'.

### `def TestTypeNormalizationWithDomain.test_common_mistake_marker_to_biomarker(self, extractor)`

'marker' should be normalized to 'biomarker'.

### `def TestTypeNormalizationWithDomain.test_skip_system_type(self, extractor)`

'system' should be skipped (returns None).

### `def TestTypeNormalizationWithDomain.test_skip_organization_type(self, extractor)`

'organization' should be skipped (returns None).

### `def TestTypeNormalizationWithDomain.test_invalid_type_returns_none(self, extractor)`

Unknown types should return None.

## `class TestTypeNormalizationWithoutDomain`

Test entity type normalization without domain (basic mode).

### `def TestTypeNormalizationWithoutDomain.extractor(self)`

Create extractor without domain for basic normalization.

### `def TestTypeNormalizationWithoutDomain.test_basic_type_passes_through(self, extractor)`

Types pass through in basic mode (no validation).

### `def TestTypeNormalizationWithoutDomain.test_basic_pipe_takes_first(self, extractor)`

Pipe-separated takes first part in basic mode.

### `def TestTypeNormalizationWithoutDomain.test_basic_mapping_applied(self, extractor)`

TYPE_MAPPING is still applied in basic mode.

## `class TestTypeMappingConstants`

Test the TYPE_MAPPING constant has expected entries.

### `def TestTypeMappingConstants.test_procedure_mappings_exist(self)`

Procedure mappings should exist.

### `def TestTypeMappingConstants.test_biomarker_mappings_exist(self)`

Biomarker mappings should exist.

### `def TestTypeMappingConstants.test_skip_mappings_exist(self)`

Skip mappings (None values) should exist.


<span id="user-content-examplesmedlitteststestprogresstrackerpy"></span>

# examples/medlit/tests/test_progress_tracker.py

Tests for ProgressTracker in the ingestion script.

Tests progress tracking and reporting functionality.

## `class TestProgressTrackerBasics`

Test basic ProgressTracker functionality.

### `def TestProgressTrackerBasics.test_initial_state(self)`

Tracker should start with zero completed.

### `def TestProgressTrackerBasics.test_increment_increases_completed(self)`

Increment should increase completed count.

### `def TestProgressTrackerBasics.test_percentage_calculation(self)`

Report should calculate correct percentage.

### `def TestProgressTrackerBasics.test_percentage_zero_total(self)`

Report should handle zero total gracefully.

### `def TestProgressTrackerBasics.test_report_shows_progress_count(self)`

Report should show completed/total count.

## `class TestProgressTrackerTiming`

Test ProgressTracker timing-related functionality.

### `def TestProgressTrackerTiming.test_rate_calculation(self)`

Report should calculate processing rate.

### `def TestProgressTrackerTiming.test_elapsed_time_shown(self)`

Report should show elapsed time.

### `def TestProgressTrackerTiming.test_estimated_remaining_shown(self)`

Report should show estimated remaining time when not complete.

## `class TestProgressTrackerAutoReport`

Test automatic reporting based on interval.

### `def TestProgressTrackerAutoReport.test_no_auto_report_before_interval(self)`

Should not auto-report before interval elapses.

### `def TestProgressTrackerAutoReport.test_auto_report_after_interval(self)`

Should auto-report when interval elapses.

## `class TestProgressTrackerEdgeCases`

Test edge cases for ProgressTracker.

### `def TestProgressTrackerEdgeCases.test_large_total(self)`

Should handle large totals.

### `def TestProgressTrackerEdgeCases.test_custom_report_interval(self)`

Should respect custom report interval.

### `def TestProgressTrackerEdgeCases.test_completed_equals_total(self)`

Should handle 100% completion.


<span id="user-content-examplesmedlitteststestpromotionlookuppy"></span>

# examples/medlit/tests/test_promotion_lookup.py

Tests for promotion with canonical ID lookup service integration.

Tests that verify the lookup parameter is correctly passed through the promotion chain:
- run_promotion(lookup=...) → get_promotion_policy(lookup=...) → MedLitPromotionPolicy(lookup=...)

### `def mock_lookup()`

Create a mock CanonicalIdLookupInterface for testing.

## `class TestPromotionLookupIntegration`

Test that lookup service is passed through the promotion chain.


<span id="user-content-examplesmedlittodomd"></span>

# examples/medlit/TODO.md

# Medical Literature Domain - Enhancement TODO

This document tracks enhancements to the med-lit domain extension for kgraph. The current implementation works with pre-extracted entities/relationships from Paper JSON format. These enhancements will enable extraction directly from raw text.

## 1. Integrate NER Models for Entity Extraction

**Status**: Not Started
**Priority**: High
**Component**: `MedLitEntityExtractor`

### Current State
- `MedLitEntityExtractor` only extracts from pre-extracted entities in Paper JSON
- Returns empty list if no pre-extracted entities found

### Implementation Options

#### Option A: BioBERT (HuggingFace)
**Model**: `d4data/biomedical-ner-all` or `dmis-lab/biobert-base-cased-v1.1`

**Pros**:

    ...


<span id="user-content-examplesmedlitvocabpy"></span>

# examples/medlit/vocab.py

Vocabulary and validation for medical literature domain.

Defines valid predicates and their constraints (which entity types
can participate in which relationships).

### `def get_valid_predicates(subject_type: str, object_type: str) -> list[str]`

Return predicates valid between two entity types.

This implements domain-specific constraints. For example:
- Drug → Disease: treats, prevents, contraindicated_for, side_effect
- Gene → Disease: increases_risk, decreases_risk, associated_with
- Gene → Protein: encodes
- Drug → Drug: interacts_with
- Disease → Symptom: causes
- Disease → Procedure: diagnosed_by

Args:
    subject_type: The entity type of the relationship subject.
    object_type: The entity type of the relationship object.

Returns:
    List of predicate names that are valid for this entity type pair.


<span id="user-content-examplessherlockdatapy"></span>

# examples/sherlock/data.py

Curated list of Sherlock Holmes characters, locations, and story metadata.

This module is intentionally “dumb data”:
- It provides canonical IDs and alias lists for pattern matching.
- Extractors use these lists to emit mentions with canonical_id_hint.
- Resolver uses hints to create canonical entities (or provisional ones).

Canonical ID scheme:
- Characters: holmes:char:<Name>
- Locations:  holmes:loc:<Name>
- Stories:    holmes:story:<Name>


<span id="user-content-examplessherlockdomainpy"></span>

# examples/sherlock/domain.py

## `class SherlockCharacter(BaseEntity)`

A character in the Sherlock Holmes stories.

**Fields:**

```python
role: Optional[str] = None
```


## `class SherlockLocation(BaseEntity)`

A location mentioned in the stories.

**Fields:**

```python
location_type: Optional[str] = None
```


## `class SherlockStory(BaseEntity)`

A story or novel in the Holmes canon.

**Fields:**

```python
collection: Optional[str] = None
publication_year: Optional[int] = None
```


## `class AppearsInRelationship(BaseRelationship)`

Character appears in a story.

## `class CoOccursWithRelationship(BaseRelationship)`

Two characters co-occur within the same textual context.

## `class LivesAtRelationship(BaseRelationship)`

Character lives at a location.

## `class AntagonistOfRelationship(BaseRelationship)`

Character is an antagonist of another character.

## `class AllyOfRelationship(BaseRelationship)`

Character is an ally of another character.

## `class SherlockDocument(BaseDocument)`

A Sherlock Holmes story document.

### `def SherlockDomainSchema.predicate_constraints(self) -> dict[str, PredicateConstraint]`

Define predicate constraints for the Sherlock domain.


<span id="user-content-examplessherlockinitpy"></span>

# examples/sherlock/__init__.py



<span id="user-content-examplessherlockpipelineembeddingspy"></span>

# examples/sherlock/pipeline/embeddings.py

## `class SimpleEmbeddingGenerator(EmbeddingGeneratorInterface)`

Deterministic hash-based embedding generator (demo only).


<span id="user-content-examplessherlockpipelineinitpy"></span>

# examples/sherlock/pipeline/__init__.py



<span id="user-content-examplessherlockpipelinementionspy"></span>

# examples/sherlock/pipeline/mentions.py

## `class SherlockEntityExtractor(EntityExtractorInterface)`

Extract character, location, and story mentions using curated alias lists.


<span id="user-content-examplessherlockpipelineparserpy"></span>

# examples/sherlock/pipeline/parser.py

## `class SherlockDocumentParser(DocumentParserInterface)`

Parse plain text Sherlock Holmes stories into SherlockDocument objects.


<span id="user-content-examplessherlockpipelinerelationshipspy"></span>

# examples/sherlock/pipeline/relationships.py

## `class SherlockRelationshipExtractor(RelationshipExtractorInterface)`

Extract relationships from resolved entities + document text.

Strategies:
- appears_in: character -> story for each character seen in doc
- co_occurs_with: character pairs co-mentioned within same paragraph


<span id="user-content-examplessherlockpipelineresolvepy"></span>

# examples/sherlock/pipeline/resolve.py

## `class SherlockEntityResolver(BaseModel, EntityResolverInterface)`

Resolve Sherlock entity mentions to canonical or provisional entities.

**Fields:**

```python
domain: DomainSchema
```



<span id="user-content-examplessherlockpromotionpy"></span>

# examples/sherlock/promotion.py

Promotion policy for Sherlock Holmes domain.

Promotes provisional entities to canonical status using curated DBPedia URI mappings.
Uses the shared canonical ID helper functions for consistency with other domains.

## `class SherlockPromotionPolicy(PromotionPolicy)`

Promotion policy for Sherlock Holmes domain using curated DBPedia mappings.

Promotion strategy:
1. If entity already has canonical_ids dict, use that
2. If entity_id is already a DBPedia URI, use it directly
3. Otherwise, look up from curated mapping


<span id="user-content-examplessherlockreadmemd"></span>

# examples/sherlock/README.md

# Sherlock Holmes Knowledge Graph Example

This directory contains a **complete, working reference example** demonstrating
how to build a domain-specific knowledge graph using **kgraph**, based on the
Sherlock Holmes canon.

It ingests *The Adventures of Sherlock Holmes* from Project Gutenberg and
constructs a queryable knowledge graph of:

* **Characters** (e.g. Sherlock Holmes, Irene Adler)
* **Locations** (e.g. 221B Baker Street, Scotland Yard)
* **Stories**
* **Relationships** (e.g. appears_in, co_occurs_with)

The example is intentionally **readable, explicit, and idiomatic**, prioritizing clarity over cleverness.

---

## Why this example exists

    ...


<span id="user-content-examplessherlockscriptsingestpy"></span>

# examples/sherlock/scripts/ingest.py



<span id="user-content-examplessherlockscriptsinitpy"></span>

# examples/sherlock/scripts/__init__.py



<span id="user-content-examplessherlockscriptsquerypy"></span>

# examples/sherlock/scripts/query.py



<span id="user-content-examplessherlocksourcesgutenbergpy"></span>

# examples/sherlock/sources/gutenberg.py

Download Sherlock Holmes stories from Project Gutenberg.

Fetches "The Adventures of Sherlock Holmes" (Gutenberg #1661) and splits
the collection into the 12 individual stories for ingestion.

Public API:
    download_adventures(force_download: bool = False) -> list[tuple[str, str]]

Returns:
    List of (story_title, story_content) tuples.

### `def download_adventures(force_download: bool = False) -> list[tuple[str, str]]`

Download and split The Adventures of Sherlock Holmes into stories.

### `def _strip_gutenberg_boilerplate(text: str) -> str`

Remove Gutenberg license/header/footer so story splits are cleaner.


<span id="user-content-examplessherlocksourcesinitpy"></span>

# examples/sherlock/sources/__init__.py



<span id="user-content-githubcopilot-instructionsmd"></span>

# .github/copilot-instructions.md

# GitHub Copilot Instructions for kgraph

This file provides guidance to GitHub Copilot when working with code in this repository.

## Project Overview

**kgraph** is a domain-agnostic framework for building knowledge graphs from documents. The system extracts entities and relationships across multiple knowledge domains (medical literature, legal documents, academic CS papers, etc.).

### Architecture

The system uses a two-pass ingestion pipeline:

1. **Pass 1 (Entity Extraction)**: Extract entities from documents, assign canonical IDs where appropriate (UMLS for medical, DBPedia URIs cross-domain, etc.)
2. **Pass 2 (Relationship Extraction)**: Identify edges/relationships between entities, produce per-document JSON with edges and provisional entities

### Key Concepts

- **Canonical entities**: Assigned stable IDs from authoritative sources
- **Provisional entities**: Mentions awaiting promotion based on usage count and confidence scores
- **Entity promotion**: Provisional → canonical when usage thresholds are met

    ...


<span id="user-content-githubworkflowstestyml"></span>

# .github/workflows/test.yml

```yaml
name: Run Tests

on:
  push:
    branches: [ main, plod ]
  pull_request:
    branches: [ main, plod ]

jobs:
  test_job:
    runs-on: ubuntu-latest

    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

    ...
```


<span id="user-content-holmesexampleplanmd"></span>

# holmes_example_plan.md

# Sherlock Holmes Knowledge Graph Example — Implementation Plan

## Overview

This example is a *reference implementation* showing how to build a domain-specific knowledge graph using **kgraph**. It ingests public-domain Sherlock Holmes stories (Project Gutenberg), extracts entities and relationships, and demonstrates basic querying.

### Purpose

Provide a complete, correct, and idiomatic example that extension authors can copy/adapt.

### Design goals

- Align closely with the `kgraph` ingestion pipeline and interfaces
- Keep pipeline responsibilities cleanly separated:
  - **Pass 1**: parse document → emit **mentions** → resolve into **entities**
  - **Pass 2**: derive **relationships** from document + resolved entities
- Demonstrate canonical vs provisional entities in a way users can understand
- Prefer readability and teachability over cleverness

---

    ...


<span id="user-content-implementationplanmd"></span>

# IMPLEMENTATION_PLAN.md

# Implementation Plan: Semantic Evidence Validation for Relationship Extraction

**Created:** 2026-02-17  
**Status:** Approved; updated per Claude review (caching, integration detail, CLI, trace fields)  
**Goal:** Replace or augment strict string-based evidence validation with cosine-similarity (embedding-based) checks so that relationships are not rejected when the evidence uses abbreviations, related terms, or partial matches (e.g. "MBC" vs "Male breast cancer", "breast cancer" vs "Male breast cancer (MBC)").

---

## 1. Problem statement

During relationship extraction, the pipeline rejects many otherwise-valid relationships because **evidence validation** requires the subject and object entity names (or synonyms) to appear **verbatim** in the evidence text. Examples of rejections that we want to accept:

| Evidence snippet | Entity name | Current result | Desired result |
|------------------|-------------|----------------|----------------|
| "...increased prevalence in **MBC**, such as BRCA2..." | Male breast cancer | Rejected (evidence_missing_object) | Accept (MBC ≈ Male breast cancer) |
| "...metastatic **pleural effusions** is **breast cancer**." | Male breast cancer (MBC), pleural effusion | Rejected (evidence_missing_subject) | Accept (breast cancer ≈ Male breast cancer; effusions ≈ effusion) |
| "...cytological **pleural metastasis** from ductal breast carcinoma..." | pleural effusion | Rejected (evidence_missing_object) | Accept or tune (pleural metastasis related to pleural effusion) |

**Current behavior:** All 23 relationships in a sample run were dropped due to `evidence_missing_subject` or `evidence_missing_object`; the pipeline produced 0 relationships for that paper.

    ...


<span id="user-content-jatsparsernotesmd"></span>

# JATS_PARSER_NOTES.md

# Notes on JATS-XML Parsing for Medlit Schema Ingestion

## Goal
To develop a robust parser that can transform JATS-XML scientific articles into structured data conforming to the `medlit_schema` in the `kgraph` framework. This involves extracting:
- `Paper` entities (bibliographic metadata)
- `TextSpan` entities (granular text locations)
- `BaseMedicalEntity` types (e.g., Disease, Gene, Drug, Protein, Mutation, Symptom, Biomarker, Pathway, Procedure, Author, ClinicalTrial, Institution, Hypothesis, StudyDesign, StatisticalMethod)
- `Evidence` entities (canonical records of observations)
- Relationships between entities, critically linked to `Evidence` entities.

## Key Challenges
1.  **Heterogeneous XML Structure**: JATS-XML can vary significantly between publishers and journals, requiring flexible parsing strategies.
2.  **Text Extraction Accuracy**: Accurately extracting text content, section titles, and character offsets is crucial for `TextSpan` creation.
3.  **Entity Identification & Normalization**: Identifying mentions of biomedical entities in text and linking them to canonical IDs (UMLS, HGNC, etc.) is complex.
4.  **Evidence Grounding**: Correctly associating asserted facts and relationships with their precise `TextSpan` and `Paper` of origin.
5.  **Relationship Inference**: Moving beyond simple co-occurrence to infer meaningful, typed relationships.

## Proposed Steps for Parser Development

### 1. XML Parsing & Document Structure

    ...


<span id="user-content-jupytermd"></span>

# jupyter.md

# Jupyter notebook

This is a quick easy way to experiment with things. I want to deal
with some quality issues in the ingestion step and this will help.

- Miscategroized entities, like bacteria being classified as diseases.
  You can have an infection of a particular bacterial strain but the
  bacterium is not itself a disease.
- Relationships need more work, too many are `associated_with` meaning
  that we didn't figure out the nature of the relationship and we
  didn't take advantage of the rich collection of different relationship
  types that we have representations for.

```bash
$ docker compose build jupyter
$ docker compose --profile jupyter up -d
```

The Jupyter notebook will appear on port 8888 of a new container.

    ...


<span id="user-content-kgbundlekgbundleinitpy"></span>

# kgbundle/kgbundle/__init__.py

Knowledge Graph Bundle Models

Lightweight Pydantic models defining the contract between bundle producers
(kgraph ingestion framework) and consumers (kgserver query server).

This package has minimal dependencies (only pydantic) and is designed to be
importable by both sides without pulling in heavy ML or web frameworks.

Example:
    # In kgraph (producer) - export bundle
    from kgbundle import BundleManifestV1, EntityRow, RelationshipRow

    entity = EntityRow(
        entity_id="char:123",
        entity_type="character",
        name="Sherlock Holmes",
        status="canonical",
        usage_count=1,
        created_at="2024-01-15T10:30:00Z",
        source="sherlock:curated"
    )

    # In kgserver (consumer) - load bundle
    from kgbundle import BundleManifestV1

    with open("manifest.json") as f:
        manifest = BundleManifestV1.model_validate_json(f.read())


<span id="user-content-kgbundlekgbundlemodelspy"></span>

# kgbundle/kgbundle/models.py

Knowledge Graph Bundle Models

Lightweight Pydantic models defining the contract between bundle producers (kgraph)
and consumers (kgserver).

This module has minimal dependencies (only pydantic) and is designed to be
importable by both sides without pulling in heavy ML or web frameworks.

## `class EntityRow(BaseModel)`

Entity row format for bundle JSONL files.

Matches the server bundle contract with proper field names and types.

**Fields:**

```python
entity_id: str = ...
entity_type: str = ...
name: Optional[str] = None
status: str = ...
confidence: Optional[float] = None
usage_count: int = ...
created_at: str = ...
source: str = ...
canonical_url: Optional[str] = None
properties: Dict[str, Any] = Field(default_factory=dict)
```


## `class RelationshipRow(BaseModel)`

Relationship row format for bundle JSONL files.

Matches the server bundle contract with proper field names and types.

**Fields:**

```python
subject_id: str = ...
object_id: str = ...
predicate: str = ...
confidence: Optional[float] = None
source_documents: List[str] = ...
created_at: str = ...
properties: Dict[str, Any] = Field(default_factory=dict)
```


## `class BundleFile(BaseModel)`

Reference to a file within the bundle.

**Fields:**

```python
path: str = ...
format: str = ...
```


## `class DocAssetRow(BaseModel)`

Documentation asset row format for bundle doc_assets.jsonl files.

Lists static assets (markdown files, images, etc.) that should be
copied from the bundle to provide documentation for the knowledge domain.

Note: This is for human-readable documentation, NOT source documents
(papers, articles) used for entity/relationship extraction.

**Fields:**

```python
path: str = ...
content_type: str = ...
```


## `class BundleManifestV1(BaseModel)`

Bundle manifest format matching the server contract.

Contains bundle identification, file references, and metadata.

**Fields:**

```python
bundle_version: str = 'v1'
bundle_id: str = ...
domain: str = ...
label: Optional[str] = None
created_at: str = ...
entities: BundleFile = ...
relationships: BundleFile = ...
doc_assets: Optional[BundleFile] = None
metadata: Dict[str, Any] = Field(default_factory=dict)
```



<span id="user-content-kgraphbuilderspy"></span>

# kgraph/builders.py

## `class EntityBuilder(BaseModel)`

**Fields:**

```python
domain: DomainSchema
clock: IngestionClock
document: BaseDocument
entity_storage: EntityStorageInterface | None = None
provisional_prefix: str = 'prov:'
```


## `class RelationshipBuilder(BaseModel)`

**Fields:**

```python
domain: DomainSchema
clock: IngestionClock
document: BaseDocument
entity_storage: EntityStorageInterface | None = None
```



<span id="user-content-kgraphcanonicalidhelperspy"></span>

# kgraph/canonical_id/helpers.py

Helper functions for working with canonical IDs in promotion logic.

This module provides generic helper functions that can be used by promotion
policies to extract canonical IDs from entity data.

### `def extract_canonical_id_from_entity(entity: BaseEntity, priority_sources: Optional[list[str]] = None) -> Optional[CanonicalId]`

Extract canonical ID from entity's canonical_ids dict.

Args:
    entity: The entity to extract canonical ID from
    priority_sources: Optional list of source keys to check in priority order.
                     If None, checks all sources in the dict.

Returns:
    CanonicalId if found, None otherwise

### `def check_entity_id_format(entity: BaseEntity, format_patterns: dict[str, tuple[str, ...]]) -> Optional[CanonicalId]`

Check if entity_id matches any known canonical ID format.

Args:
    entity: The entity to check
    format_patterns: Dict mapping entity types to tuples of format prefixes/patterns.
                    For example: {"gene": ("HGNC:",), "disease": ("C",)}

Returns:
    CanonicalId if entity_id matches a format, None otherwise


<span id="user-content-kgraphcanonicalidinitpy"></span>

# kgraph/canonical_id/__init__.py

Canonical ID system for knowledge graph ingestion.

This package provides abstractions for working with canonical IDs (stable
identifiers from authoritative sources) throughout the ingestion pipeline.

Note: The CanonicalId model has been moved to kgschema.canonical_id but is
re-exported here for backwards compatibility.


<span id="user-content-kgraphcanonicalidjsoncachepy"></span>

# kgraph/canonical_id/json_cache.py

JSON file-based implementation of CanonicalIdCacheInterface.

This implementation stores canonical ID lookups in a JSON file, with support for
backward compatibility with the old cache format (dict[str, str]).

## `class JsonFileCanonicalIdCache(CanonicalIdCacheInterface)`

JSON file-based implementation of CanonicalIdCacheInterface.

Stores canonical ID lookups in a JSON file. The cache format is:
{
    "cache_key": {
        "id": "UMLS:C12345",
        "url": "https://...",
        "synonyms": ["term1", "term2"]
    },
    ...
}

Known bad entries are stored with `"id": null` to distinguish them from
successful lookups.

Attributes:
    cache_file: Path to the JSON cache file
    _cache: In-memory cache dictionary mapping cache keys to CanonicalId objects
    _known_bad: Set of cache keys marked as "known bad"
    _cache_dirty: Whether the cache has been modified since last save
    _hits: Number of cache hits
    _misses: Number of cache misses

### `def JsonFileCanonicalIdCache.__init__(self, cache_file: Optional[Path] = None)`

Initialize the JSON file-based cache.

Args:
    cache_file: Path to the JSON cache file. If None, defaults to
               "canonical_id_cache.json" in the current directory.

### `def JsonFileCanonicalIdCache.load(self, tag: str) -> None`

Load cache from JSON file.

Args:
    tag: Path to the cache file (overrides self.cache_file if provided).
         If tag is a relative path, it's used as-is.
         If tag is an absolute path, it overrides self.cache_file.

### `def JsonFileCanonicalIdCache._migrate_old_format(self, old_data: dict[str, str]) -> None`

Migrate old cache format (dict[str, str]) to new format.

Args:
    old_data: Old cache data where values are either canonical ID strings or "NULL"

### `def JsonFileCanonicalIdCache.save(self, tag: str) -> None`

Save cache to JSON file.

Args:
    tag: Path to the cache file (overrides self.cache_file if provided).
         If tag is a path, it's used as-is.

### `def JsonFileCanonicalIdCache.store(self, term: str, entity_type: str, canonical_id: CanonicalId) -> None`

Store a canonical ID in the cache.

Args:
    term: The entity name/mention text
    entity_type: Type of entity (e.g., "disease", "gene", "drug")
    canonical_id: The CanonicalId object to store

### `def JsonFileCanonicalIdCache.fetch(self, term: str, entity_type: str) -> Optional[CanonicalId]`

Fetch a canonical ID from the cache.

Args:
    term: The entity name/mention text
    entity_type: Type of entity (e.g., "disease", "gene", "drug")

Returns:
    CanonicalId if found in cache, None if not found or marked as "known bad"

### `def JsonFileCanonicalIdCache.mark_known_bad(self, term: str, entity_type: str) -> None`

Mark a term as "known bad" (failed lookup, don't retry).

Args:
    term: The entity name/mention text
    entity_type: Type of entity (e.g., "disease", "gene", "drug")

### `def JsonFileCanonicalIdCache.is_known_bad(self, term: str, entity_type: str) -> bool`

Check if a term is marked as "known bad".

Args:
    term: The entity name/mention text
    entity_type: Type of entity (e.g., "disease", "gene", "drug")

Returns:
    True if the term is marked as "known bad", False otherwise

### `def JsonFileCanonicalIdCache.get_metrics(self) -> dict[str, int]`

Get cache performance metrics.

Returns:
    Dictionary with metrics:
    - "hits": Number of cache hits
    - "misses": Number of cache misses
    - "known_bad": Number of known bad entries
    - "total_entries": Total number of successful entries in cache


<span id="user-content-kgraphcanonicalidlookuppy"></span>

# kgraph/canonical_id/lookup.py

Canonical ID lookup interface for promotion policies.

This module provides an abstract interface for looking up canonical IDs,
which promotion policies can use to assign canonical IDs to entities.

## `class CanonicalIdLookupInterface(ABC)`

Abstract interface for looking up canonical IDs.

This interface is used by promotion policies to look up canonical IDs
for entities. It abstracts away the details of how lookups are performed
(API calls, cache, etc.) so promotion policies can work with any lookup
implementation.

### `def CanonicalIdLookupInterface.lookup_sync(self, term: str, entity_type: str) -> Optional[CanonicalId]`

Synchronous version of lookup (for use in sync contexts).

Args:
    term: The entity name/mention text
    entity_type: Type of entity (e.g., "disease", "gene", "drug")

Returns:
    CanonicalId if found, None otherwise


<span id="user-content-kgraphcanonicalidmodelspy"></span>

# kgraph/canonical_id/models.py

Canonical ID cache interface for knowledge graph ingestion.

This module provides the CanonicalIdCacheInterface for caching canonical ID
lookups across different knowledge domains.

## `class CanonicalIdCacheInterface(ABC)`

Abstract interface for caching canonical ID lookups.

This interface allows different storage backends (JSON file, database, etc.)
to be used for caching canonical ID lookups. The cache stores mappings from
(term, entity_type) pairs to CanonicalId objects.

The cache supports:
- Loading and saving with a tag/identifier (for multi-domain support)
- Storing and fetching CanonicalId objects
- Marking terms as "known bad" (failed lookups that shouldn't be retried)
- Cache metrics (hits, misses, etc.)

Implementations should be performant since the cache's purpose is to avoid
expensive external API calls.

### `def CanonicalIdCacheInterface.load(self, tag: str) -> None`

Load cache from storage.

Args:
    tag: Identifier for the cache (e.g., "medlit", "sherlock", or a file path).
         For file-based caches, this might be a file path.
         For database caches, this might be a domain identifier.

### `def CanonicalIdCacheInterface.save(self, tag: str) -> None`

Save cache to storage.

Args:
    tag: Identifier for the cache (same as used in load()).

### `def CanonicalIdCacheInterface.store(self, term: str, entity_type: str, canonical_id: CanonicalId) -> None`

Store a canonical ID in the cache.

Args:
    term: The entity name/mention text (will be normalized internally)
    entity_type: Type of entity (e.g., "disease", "gene", "drug")
    canonical_id: The CanonicalId object to store

### `def CanonicalIdCacheInterface.fetch(self, term: str, entity_type: str) -> Optional[CanonicalId]`

Fetch a canonical ID from the cache.

Args:
    term: The entity name/mention text (will be normalized internally)
    entity_type: Type of entity (e.g., "disease", "gene", "drug")

Returns:
    CanonicalId if found in cache, None if not found or marked as "known bad"

### `def CanonicalIdCacheInterface.mark_known_bad(self, term: str, entity_type: str) -> None`

Mark a term as "known bad" (failed lookup, don't retry).

This allows the cache to remember that a lookup was attempted and failed,
so we don't waste time retrying it.

Args:
    term: The entity name/mention text (will be normalized internally)
    entity_type: Type of entity (e.g., "disease", "gene", "drug")

### `def CanonicalIdCacheInterface.is_known_bad(self, term: str, entity_type: str) -> bool`

Check if a term is marked as "known bad".

Args:
    term: The entity name/mention text (will be normalized internally)
    entity_type: Type of entity (e.g., "disease", "gene", "drug")

Returns:
    True if the term is marked as "known bad", False otherwise

### `def CanonicalIdCacheInterface.get_metrics(self) -> dict[str, int]`

Get cache performance metrics.

Returns:
    Dictionary with metrics such as:
    - "hits": Number of cache hits
    - "misses": Number of cache misses
    - "known_bad": Number of known bad entries
    - "total_entries": Total number of entries in cache

### `def CanonicalIdCacheInterface._normalize_key(self, term: str, entity_type: str) -> str`

Normalize cache key for consistent lookups.

Args:
    term: The entity name/mention text
    entity_type: Type of entity

Returns:
    Normalized cache key string (e.g., "disease:breast cancer")


<span id="user-content-kgraphclockpy"></span>

# kgraph/clock.py

## `class IngestionClock(BaseModel)`

**Fields:**

```python
now: datetime
```



<span id="user-content-kgraphcontextpy"></span>

# kgraph/context.py

## `class IngestionContext(BaseModel)`

**Fields:**

```python
domain: DomainSchema
clock: IngestionClock
document: BaseDocument
entities: EntityBuilder
relationships: RelationshipBuilder
```



<span id="user-content-kgraphexportpy"></span>

# kgraph/export.py

### `def get_git_hash() -> Optional[str]`

Gets the current git commit hash in short format.

This is used to version-stamp exported bundles, providing a precise
reference to the codebase state at the time of export.

Returns:
    The short git commit hash (e.g., "6b50d25") as a string, or `None`
    if the git command fails (e.g., not in a git repository).

### `def _collect_doc_assets(docs_source: Path, bundle_path: Path) -> List[DocAssetRow]`

Copies documentation assets from a source directory into the bundle.

This function recursively walks the `docs_source` directory, copies each
file to a `docs/` subdirectory within the `bundle_path`, and generates
a `DocAssetRow` for each copied file to be included in the bundle's
`doc_assets.jsonl`.

Note: These are human-readable documentation files (markdown, images, etc.),
NOT source documents (papers, articles) used for entity extraction.

Args:
    docs_source: The source directory containing the documentation assets
                 (e.g., Markdown files).
    bundle_path: The root directory of the bundle being created.

Returns:
    A list of `DocAssetRow` objects, one for each file copied.


<span id="user-content-kgraphingestpy"></span>

# kgraph/ingest.py

Two-pass ingestion orchestrator for the knowledge graph framework.

This module provides the `IngestionOrchestrator` class, which coordinates the
complete document ingestion pipeline. The orchestrator manages the two-pass
process that transforms raw documents into structured knowledge:

**Pass 1 - Entity Extraction:**
    1. Parse raw document bytes into structured `BaseDocument`
    2. Extract entity mentions using the configured `EntityExtractorInterface`
    3. Resolve mentions to canonical or provisional entities
    4. Generate embeddings for new entities
    5. Store entities, updating usage counts for existing ones

**Pass 2 - Relationship Extraction:**
    1. Extract relationships between resolved entities
    2. Validate relationships against the domain schema
    3. Store relationships with source document references

The orchestrator also provides methods for:
    - Batch ingestion of multiple documents
    - Entity promotion (provisional → canonical)
    - Duplicate detection via embedding similarity
    - Entity merging
    - JSON export of entities and relationships

Example usage:
    ```python
    orchestrator = IngestionOrchestrator(
        domain=my_domain_schema,
        parser=my_parser,
        entity_extractor=my_extractor,
        entity_resolver=my_resolver,
        relationship_extractor=my_rel_extractor,
        embedding_generator=my_embedder,
        entity_storage=entity_store,
        relationship_storage=rel_store,
        document_storage=doc_store,
    )

    result = await orchestrator.ingest_document(
        raw_content=document_bytes,
        content_type="text/plain",
    )
    print(f"Extracted {result.entities_extracted} entities")
    ```

## `class DocumentResult(BaseModel)`

Result of processing a single document through the ingestion pipeline.

Contains statistics about the extraction process and any errors encountered.
Immutable (frozen) to ensure results can be safely shared and stored.

Attributes:
    document_id: Unique identifier assigned to the parsed document.
    entities_extracted: Total number of entity mentions found in the document.
    entities_new: Number of mentions that created new provisional entities.
    entities_existing: Number of mentions that matched existing entities.
    relationships_extracted: Number of relationships stored from this document.
    errors: Tuple of error messages encountered during processing.

**Fields:**

```python
document_id: str
entities_extracted: int
entities_new: int
entities_existing: int
relationships_extracted: int
errors: tuple[str, ...] = ()
```


## `class IngestionResult(BaseModel)`

Result of batch document ingestion.

Aggregates statistics across multiple documents and provides per-document
breakdown via the `document_results` field.

Attributes:
    documents_processed: Total number of documents in the batch.
    documents_failed: Number of documents that had errors during processing.
    total_entities_extracted: Sum of entity mentions across all documents.
    total_relationships_extracted: Sum of relationships across all documents.
    document_results: Per-document results for detailed inspection.
    errors: Top-level errors that prevented document processing.

**Fields:**

```python
documents_processed: int
documents_failed: int
total_entities_extracted: int
total_relationships_extracted: int
document_results: tuple[DocumentResult, ...] = ()
errors: tuple[str, ...] = ()
```


### `def _determine_canonical_id_source(canonical_id: str) -> str`

Determine the canonical_ids dict key from canonical_id format.

Args:
    canonical_id: The canonical ID string (e.g., "HGNC:1100", "MeSH:D001943", "C0006142")

Returns:
    Source key for canonical_ids dict (e.g., "hgnc", "mesh", "umls", "dbpedia")

## `class IngestionOrchestrator(BaseModel)`

Orchestrates two-pass document ingestion for knowledge graph construction.

The orchestrator is the main entry point for document processing. It
coordinates all pipeline components (parser, extractors, resolver,
embedding generator) and storage backends to transform raw documents
into structured knowledge graph data.

**Two-Pass Architecture:**

- **Pass 1 (Entity Extraction)**: Extracts entity mentions from documents,
  resolves them to canonical or provisional entities, generates embeddings,
  and updates storage with new entities or incremented usage counts.

- **Pass 2 (Relationship Extraction)**: Identifies relationships between
  the resolved entities and stores them with source document references.

**Additional Operations:**

- `run_promotion()`: Promotes provisional entities to canonical status
  based on usage frequency and confidence thresholds.
- `find_merge_candidates()`: Detects potential duplicate entities using
  embedding similarity.
- `merge_entities()`: Combines duplicate entities and updates references.
- `export_*()`: Exports entities and relationships to JSON files.

Attributes:
    domain: Schema defining entity types, relationship types, and validation.
    parser: Converts raw bytes to structured documents.
    entity_extractor: Extracts entity mentions from documents.
    entity_resolver: Maps mentions to canonical or provisional entities.
    relationship_extractor: Extracts relationships between entities.
    embedding_generator: Creates semantic vectors for similarity operations.
    entity_storage: Persistence backend for entities.
    relationship_storage: Persistence backend for relationships.
    document_storage: Persistence backend for source documents.

**Fields:**

```python
domain: DomainSchema
parser: DocumentParserInterface
entity_extractor: EntityExtractorInterface
entity_resolver: EntityResolverInterface
relationship_extractor: RelationshipExtractorInterface
embedding_generator: EmbeddingGeneratorInterface
entity_storage: EntityStorageInterface
relationship_storage: RelationshipStorageInterface
document_storage: DocumentStorageInterface
document_chunker: DocumentChunkerInterface | None = None
streaming_entity_extractor: StreamingEntityExtractorInterface | None = None
streaming_relationship_extractor: StreamingRelationshipExtractorInterface | None = None
```


### `def IngestionOrchestrator._serialize_entity(self, entity: BaseEntity) -> dict[str, Any]`

Serialize an entity to a JSON-compatible dictionary.

### `def IngestionOrchestrator._serialize_relationship(self, rel: BaseRelationship) -> dict[str, Any]`

Serialize a relationship to a JSON-compatible dictionary.


<span id="user-content-kgraphinitpy"></span>

# kgraph/__init__.py

Knowledge Graph Framework - Domain-Agnostic Entity and Relationship Extraction.

A flexible framework for building knowledge graphs across multiple domains
(medical, legal, CS papers, etc.) with a two-pass ingestion process.


<span id="user-content-kgraphloggingpy"></span>

# kgraph/logging.py

## `class PprintLogger`

A logger wrapper that adds pprint support to standard logging methods.

### `def PprintLogger._format_message(self, msg: Any, pprint: bool = True) -> str`

Format a message, optionally using pprint.

If the message is a Pydantic model and pprint=True, uses model_dump_json()
to show the model's internals. Otherwise uses pformat for complex objects
or str() for simple conversion.

### `def PprintLogger.debug(self, msg: Any, *args, **kwargs) -> None`

Log a debug message with optional pprint formatting.

### `def PprintLogger.info(self, msg: Any, *args, **kwargs) -> None`

Log an info message with optional pprint formatting.

### `def PprintLogger.warning(self, msg: Any, *args, **kwargs) -> None`

Log a warning message with optional pprint formatting.

### `def PprintLogger.error(self, msg: Any, *args, **kwargs) -> None`

Log an error message with optional pprint formatting.

### `def PprintLogger.critical(self, msg: Any, *args, **kwargs) -> None`

Log a critical message with optional pprint formatting.

### `def PprintLogger.exception(self, msg: Any, *args, **kwargs) -> None`

Log an exception message with optional pprint formatting.

### `def PprintLogger.__getattr__(self, name: str) -> Any`

Delegate any other attributes to the underlying logger.

### `def setup_logging(level: int = logging.INFO) -> PprintLogger`

Set up logging and return a PprintLogger instance.


<span id="user-content-kgraphpipelinecachingpy"></span>

# kgraph/pipeline/caching.py

Caching interfaces for embeddings and other computed artifacts.

This module provides abstractions for caching expensive computations, particularly
embeddings (semantic vectors). Caching is critical for:

- **Cost reduction**: Avoiding repeated API calls to embedding providers
- **Performance**: Eliminating redundant computation for frequently-seen entities
- **Consistency**: Ensuring the same text always produces the same embedding
- **Offline operation**: Working with pre-computed embeddings without API access

Key abstractions:
    - EmbeddingsCacheInterface: Generic cache for text→embedding mappings
    - InMemoryEmbeddingsCache: Fast in-memory LRU cache
    - FileBasedEmbeddingsCache: Persistent JSON-based cache

Typical usage:
    ```python
    # Create cache with persistence
    cache = FileBasedEmbeddingsCache(cache_file="embeddings.json")
    await cache.load()

    # Wrap embedding generator with caching
    generator = CachedEmbeddingGenerator(
        base_generator=ollama_embedder,
        cache=cache
    )

    # Subsequent calls with same text use cached embeddings
    emb1 = await generator.generate("aspirin")  # API call
    emb2 = await generator.generate("aspirin")  # Cached, no API call
    ```

## `class EmbeddingCacheConfig(BaseModel)`

Configuration for embedding caching strategies.

Attributes:
    max_cache_size: Maximum number of embeddings to store in memory (for LRU eviction)
    cache_file: Path to persistent cache file (for file-based caches)
    auto_save_interval: Number of cache updates before auto-saving (0 = manual save only)
    normalize_keys: Whether to normalize cache keys (lowercase, strip whitespace)

**Fields:**

```python
max_cache_size: int = 10000
cache_file: Path | None = None
auto_save_interval: int = 100
normalize_keys: bool = True
```


## `class EmbeddingsCacheInterface(ABC)`

Abstract interface for caching text embeddings.

Implementations provide different storage backends (memory, disk, database)
with consistent semantics. All implementations should:
    - Be thread-safe for concurrent access
    - Support batch operations for efficiency
    - Provide cache statistics (hits, misses, size)
    - Handle cache invalidation gracefully

Cache keys are text strings; values are embedding vectors (tuples of floats).

### `def EmbeddingsCacheInterface.get_stats(self) -> dict[str, int]`

Get cache statistics.

Returns:
    Dictionary with metrics like:
        - "hits": Number of successful cache lookups
        - "misses": Number of cache misses
        - "size": Current number of cached embeddings
        - "evictions": Number of items evicted (for LRU caches)

### `def EmbeddingsCacheInterface._normalize_key(self, text: str) -> str`

Normalize cache key for consistent lookups.

Args:
    text: The text to normalize

Returns:
    Normalized text (lowercase, stripped whitespace)

## `class InMemoryEmbeddingsCache(EmbeddingsCacheInterface)`

In-memory LRU cache for embeddings.

Uses an OrderedDict to maintain LRU semantics with O(1) lookups and updates.
When the cache exceeds max_cache_size, the least recently used items are evicted.

This implementation is fast but non-persistent. Suitable for:
    - Short-lived processes
    - Testing
    - Hot cache layer in front of persistent storage

Example:
    ```python
    cache = InMemoryEmbeddingsCache(
        config=EmbeddingCacheConfig(max_cache_size=5000)
    )

    await cache.put("aspirin", embedding_vector)
    result = await cache.get("aspirin")  # Fast O(1) lookup
    ```

### `def InMemoryEmbeddingsCache.__init__(self, config: EmbeddingCacheConfig | None = None)`

Initialize the in-memory cache.

Args:
    config: Cache configuration. If None, uses default config.

### `def InMemoryEmbeddingsCache.get_stats(self) -> dict[str, int]`

Get cache statistics.

Returns:
    Dictionary with hits, misses, size, and evictions

## `class FileBasedEmbeddingsCache(EmbeddingsCacheInterface)`

Persistent file-based embeddings cache using JSON.

Stores embeddings in a JSON file with optional in-memory LRU cache for
hot data. The file format is:
    ```json
    {
        "aspirin": [0.1, 0.2, ..., 0.9],
        "ibuprofen": [0.3, 0.4, ..., 0.8],
        ...
    }
    ```

Features:
    - Persistent storage survives process restarts
    - Optional auto-save on every N updates
    - In-memory LRU cache for hot data
    - Atomic writes to prevent corruption

Example:
    ```python
    cache = FileBasedEmbeddingsCache(
        config=EmbeddingCacheConfig(
            cache_file=Path("embeddings.json"),
            max_cache_size=5000,
            auto_save_interval=100
        )
    )

    await cache.load()  # Load existing cache
    await cache.put("aspirin", embedding)
    # Auto-saves every 100 updates
    ```

### `def FileBasedEmbeddingsCache.__init__(self, config: EmbeddingCacheConfig)`

Initialize the file-based cache.

Args:
    config: Cache configuration including cache_file path

Raises:
    ValueError: If config.cache_file is None

### `def FileBasedEmbeddingsCache.get_stats(self) -> dict[str, int]`

Get cache statistics.

Returns:
    Dictionary with hits, misses, size, and evictions

## `class CachedEmbeddingGenerator(EmbeddingGeneratorInterface)`

Wraps an embedding generator with transparent caching.

This adapter provides a cache layer in front of any EmbeddingGeneratorInterface
implementation. Cache hits avoid API calls entirely, providing:
    - Significant cost savings for frequently-seen texts
    - Faster response times
    - Consistent embeddings (no variation from API)
    - Offline operation for cached texts

The cache is transparent to callers - the interface is identical to the
underlying generator.

Example:
    ```python
    # Wrap any embedding generator with caching
    cache = FileBasedEmbeddingsCache(
        config=EmbeddingCacheConfig(cache_file=Path("cache.json"))
    )
    await cache.load()

    cached_gen = CachedEmbeddingGenerator(
        base_generator=ollama_embedder,
        cache=cache
    )

    # First call hits API and caches result
    emb1 = await cached_gen.generate("aspirin")

    # Second call uses cached result
    emb2 = await cached_gen.generate("aspirin")  # No API call!
    ```

### `def CachedEmbeddingGenerator.__init__(self, base_generator: EmbeddingGeneratorInterface, cache: EmbeddingsCacheInterface)`

Initialize the cached generator.

Args:
    base_generator: The underlying embedding generator
    cache: The cache to use for storing/retrieving embeddings

### `def CachedEmbeddingGenerator.dimension(self) -> int`

Return embedding dimension from base generator.

Returns:
    Embedding dimension (e.g., 1536, 1024)

### `def CachedEmbeddingGenerator.get_cache_stats(self) -> dict[str, int]`

Get cache statistics.

Returns:
    Dictionary with cache metrics


<span id="user-content-kgraphpipelineembeddingpy"></span>

# kgraph/pipeline/embedding.py

Embedding generation interface for the knowledge graph framework.

This module defines the interface for generating semantic vector embeddings,
which are dense numerical representations of text that capture meaning.
Embeddings enable several key knowledge graph operations:

- **Entity resolution**: Match entity mentions to existing entities with
  similar meanings but different surface forms (e.g., "heart attack" vs
  "myocardial infarction").

- **Duplicate detection**: Identify canonical entities that should be merged
  by finding pairs with high embedding similarity.

- **Semantic search**: Query the knowledge graph by meaning rather than
  exact text matching.

Implementations typically wrap embedding APIs such as:
    - OpenAI text-embedding-3-small/large
    - Cohere embed-english-v3.0
    - Sentence Transformers (local models)
    - Domain-specific embeddings (BioBERT, LegalBERT, etc.)

## `class EmbeddingGeneratorInterface(ABC)`

Generate semantic vector embeddings for text.

This interface abstracts embedding generation, allowing the knowledge
graph framework to work with any embedding provider. Implementations
should be stateless and thread-safe.

The embedding vectors are stored as immutable tuples of floats to ensure
they can be safely shared and used as dictionary keys or set members.

Typical usage:
    ```python
    embedder = OpenAIEmbedding(client, model="text-embedding-3-small")
    embedding = await embedder.generate("aspirin")
    similar_entities = await storage.find_by_embedding(embedding, threshold=0.85)
    ```

When implementing:
    - Handle empty strings gracefully (return zero vector or raise)
    - Consider rate limiting and retries for API-based implementations
    - Normalize vectors if the similarity search requires it

### `def EmbeddingGeneratorInterface.dimension(self) -> int`

Return the dimensionality of generated embeddings.

This property is essential for:
    - Storage backends that need to configure vector columns/indices
    - Validation that embeddings match expected dimensions
    - Memory estimation for batch operations

The dimension depends on the underlying model:
    - OpenAI text-embedding-3-small: 1536
    - OpenAI text-embedding-3-large: 3072
    - Sentence Transformers all-MiniLM-L6-v2: 384

Returns:
    Integer dimension of embedding vectors (e.g., 1536, 3072).


<span id="user-content-kgraphpipelineinitpy"></span>

# kgraph/pipeline/__init__.py

Pipeline interfaces for document processing and extraction.


<span id="user-content-kgraphpipelineinterfacespy"></span>

# kgraph/pipeline/interfaces.py

Pipeline interface definitions for document processing and extraction.

This module defines the abstract interfaces for the two-pass ingestion pipeline:

- **Pass 1 (Entity Extraction)**: Parse documents, extract entity mentions,
  and resolve them to canonical or provisional entities.
- **Pass 2 (Relationship Extraction)**: Identify relationships/edges between
  the resolved entities within each document.

The pipeline components are designed to be pluggable, allowing different
implementations for different domains (medical literature, legal documents,
academic papers, etc.) or different underlying technologies (LLMs, NER models,
rule-based extractors).

Typical flow:
    1. DocumentParserInterface converts raw bytes to BaseDocument
    2. EntityExtractorInterface identifies EntityMention instances
    3. EntityResolverInterface maps mentions to BaseEntity instances
    4. RelationshipExtractorInterface finds relationships between entities

## `class DocumentParserInterface(ABC)`

Parse raw documents into structured BaseDocument instances.

Implementations handle format-specific parsing (PDF, HTML, plain text, etc.)
and extract document metadata such as title, author, publication date, and
structural elements like sections and paragraphs.

This is the entry point for the ingestion pipeline. The parsed document
provides the content that subsequent extractors process.

Example implementations might use:
    - PDF parsing libraries (PyMuPDF, pdfplumber)
    - HTML parsing (BeautifulSoup, lxml)
    - LLMs for structure extraction from unstructured text

## `class EntityExtractorInterface(ABC)`

Extract entity mentions from documents (Pass 1 of ingestion).

This interface handles the first pass of document processing: identifying
spans of text that refer to entities of interest. The output is a list of
EntityMention objects representing raw extractions that have not yet been
resolved to canonical or provisional entities.

Entity mentions capture:
    - The exact text span and its position in the document
    - The inferred entity type (domain-specific, e.g., 'drug', 'gene', 'person')
    - Extraction confidence score
    - Surrounding context for disambiguation

Implementations may use:
    - Named Entity Recognition (NER) models (spaCy, Hugging Face transformers)
    - Large Language Models with structured extraction prompts
    - Rule-based pattern matching for domain-specific entities
    - Hybrid approaches combining multiple techniques

## `class EntityResolverInterface(ABC)`

Resolve entity mentions to canonical or provisional entities.

After extraction, entity mentions must be resolved to determine whether
they refer to existing known entities or represent new discoveries. This
interface handles the resolution process through multiple strategies:

1. **Name/synonym matching**: Check if the mention text matches known
   entity names or synonyms in storage.
2. **Embedding similarity**: Use semantic vector similarity to find
   entities with similar meaning but different surface forms.
3. **External authority lookup**: Query authoritative sources (UMLS for
   medical terms, DBPedia for general knowledge, etc.) to obtain
   canonical identifiers.
4. **Provisional creation**: If no match is found, create a provisional
   entity that may later be promoted to canonical status based on
   usage frequency and confidence scores.

The confidence score returned with each resolution indicates the certainty
of the match, enabling downstream filtering and quality control.

## `class RelationshipExtractorInterface(ABC)`

Extract relationships between entities from documents (Pass 2 of ingestion).

After entities have been extracted and resolved, this interface identifies
the relationships (edges) between them within the document context. This
is the second pass of the ingestion pipeline.

Relationships are typically expressed as (subject, predicate, object) triples
with additional metadata such as:
    - Source document reference for provenance
    - Confidence score for the extraction
    - Supporting evidence (text spans, context)

The predicate vocabulary is typically domain-specific:
    - Medical: 'treats', 'causes', 'interacts_with', 'inhibits'
    - Legal: 'cites', 'overrules', 'amends', 'references'
    - Academic: 'authored_by', 'cites', 'builds_upon', 'contradicts'

Implementations may use:
    - LLMs with structured prompts listing entities and requesting triples
    - Dependency parsing to identify syntactic relationships
    - Pattern-based extraction using domain-specific templates
    - Pre-trained relation extraction models


<span id="user-content-kgraphpipelinestreamingpy"></span>

# kgraph/pipeline/streaming.py

Streaming pipeline interfaces for processing large documents.

This module provides abstractions for processing documents in a streaming fashion,
breaking them into manageable chunks (windows) that can be processed incrementally.
This is essential for:

- **Large documents**: Processing documents that exceed LLM context windows
- **Memory efficiency**: Avoiding loading entire documents into memory
- **Incremental processing**: Starting extraction before full document is parsed
- **Context preservation**: Using overlapping windows to maintain entity/relationship
  context across chunk boundaries

The design follows the patterns established in the plod branch for PMC XML streaming,
providing generic abstractions that work across different document formats.

Key abstractions:
    - DocumentChunker: Splits documents into overlapping chunks/windows
    - StreamingEntityExtractor: Extracts entities from document chunks with deduplication
    - StreamingRelationshipExtractor: Extracts relationships within windows

Typical usage:
    ```python
    chunker = WindowedDocumentChunker(chunk_size=2000, overlap=200)
    chunks = await chunker.chunk(document)

    extractor = BatchingEntityExtractor(base_extractor, batch_size=10)
    all_mentions = []
    async for chunk_mentions in extractor.extract_streaming(chunks):
        all_mentions.extend(chunk_mentions)
    ```

Based on the streaming extraction patterns from:
    - examples/medlit/pipeline/pmc_streaming.py (plod branch)
    - examples/medlit/pipeline/mentions.py (windowed entity extraction)

## `class DocumentChunk(BaseModel)`

Represents a chunk/window of a document.

Chunks may overlap to preserve context across boundaries. For example,
a document split with 2000-character chunks and 200-character overlap
ensures entities mentioned near chunk boundaries aren't missed.

Attributes:
    content: The text content of this chunk
    start_offset: Character offset where this chunk starts in the original document
    end_offset: Character offset where this chunk ends in the original document
    chunk_index: Sequential index of this chunk (0-based)
    document_id: ID of the parent document
    metadata: Optional chunk-specific metadata (section name, page number, etc.)

**Fields:**

```python
content: str = ...
start_offset: int = ...
end_offset: int = ...
chunk_index: int = ...
document_id: str = ...
metadata: dict[str, str] = Field(default_factory=dict)
```


## `class ChunkingConfig(BaseModel)`

Configuration for document chunking strategies.

Attributes:
    chunk_size: Target size of each chunk in characters
    overlap: Number of characters to overlap between consecutive chunks
    respect_boundaries: Whether to respect sentence/paragraph boundaries
    min_chunk_size: Minimum size for a chunk (to avoid tiny trailing chunks)

**Fields:**

```python
chunk_size: int = 2000
overlap: int = 200
respect_boundaries: bool = True
min_chunk_size: int = 500
```


## `class DocumentChunkerInterface(ABC)`

Interface for splitting documents into processable chunks.

Implementations handle different chunking strategies:
    - Fixed-size chunks with overlap
    - Semantic chunking (paragraph/section boundaries)
    - Token-based chunking (for LLM token limits)
    - Hybrid approaches

The chunker preserves document structure and maintains metadata for
reconstructing entity positions in the original document.

Optional: implement chunk_from_raw() for memory-efficient chunking from
raw bytes (e.g. PMC XML via iterparse) without loading the full document.

## `class WindowedDocumentChunker(DocumentChunkerInterface)`

Chunks documents into overlapping fixed-size windows.

This implementation uses a simple sliding window approach with configurable
overlap. Optionally respects sentence boundaries to avoid breaking entities
mid-sentence.

Example:
    ```python
    chunker = WindowedDocumentChunker(
        config=ChunkingConfig(chunk_size=2000, overlap=200)
    )
    chunks = await chunker.chunk(document)
    ```

### `def WindowedDocumentChunker.__init__(self, config: ChunkingConfig | None = None)`

Initialize the windowed chunker.

Args:
    config: Chunking configuration. If None, uses default config.

## `class StreamingEntityExtractorInterface(ABC)`

Interface for extracting entities from document chunks in streaming fashion.

Extends EntityExtractorInterface with streaming capabilities for processing
large documents chunk by chunk. Implementations should:
    - Deduplicate entities found in overlapping chunks (by normalized key)
    - Adjust entity offsets to match the original document
    - Batch API calls for efficiency
    - Merge mentions with highest confidence when duplicates found

This follows the pattern from plod branch's windowed entity extraction which
deduplicates by (normalized_name, entity_type) and keeps the highest confidence
mention when duplicates are found across windows.

### `def StreamingEntityExtractorInterface.extract_streaming(self, chunks: Sequence[DocumentChunk]) -> AsyncIterator[list[EntityMention]]`

Extract entities from document chunks, yielding results as they're processed.

Note: This method is not async - it returns an AsyncIterator that can be
iterated with `async for`. This is the correct pattern for async generators.

Args:
    chunks: Sequence of document chunks to process

Yields:
    Lists of EntityMention objects for each processed chunk

### `def normalize_mention_key(name: str, entity_type: str) -> tuple[str, str]`

Normalize mention key for deduplication across windows.

Removes non-alphanumeric characters, collapses whitespace, and lowercases
for consistent matching. This ensures "Breast Cancer", "breast cancer",
and "BREAST  CANCER" are all treated as the same entity.

Based on _normalize_mention_key from plod branch medlit/pipeline/mentions.py.

Args:
    name: Entity name/mention text
    entity_type: Entity type (e.g., "disease", "gene", "drug")

Returns:
    Tuple of (normalized_name, entity_type) for use as dictionary key

## `class BatchingEntityExtractor(StreamingEntityExtractorInterface)`

Wraps an EntityExtractorInterface to provide streaming extraction with batching.

This adapter enables any EntityExtractorInterface implementation to work with
document chunks. It handles:
    - Converting chunks back to temporary BaseDocument objects
    - Batching extraction calls for efficiency
    - Adjusting entity mention offsets to match original document positions
    - Deduplicating mentions across overlapping windows (keeping highest confidence)

The deduplication approach follows the plod branch pattern: normalize entity names
to alphanumeric lowercase, then keep the highest confidence mention when duplicates
are found across windows.

Example:
    ```python
    base_extractor = MyEntityExtractor()
    streaming_extractor = BatchingEntityExtractor(
        base_extractor=base_extractor,
        batch_size=10,
        deduplicate=True
    )

    async for mentions in streaming_extractor.extract_streaming(chunks):
        # Process mentions as they arrive
        await process_mentions(mentions)
    ```

### `def BatchingEntityExtractor.__init__(self, base_extractor: EntityExtractorInterface, batch_size: int = 5, deduplicate: bool = True)`

Initialize the batching extractor.

Args:
    base_extractor: The underlying extractor to use for each chunk
    batch_size: Number of chunks to process in parallel (not yet implemented)
    deduplicate: Whether to deduplicate mentions across windows

### `def BatchingEntityExtractor.get_unique_mentions(self) -> list[EntityMention]`

Get all unique mentions after deduplication.

Only meaningful when deduplicate=True. Returns the highest confidence
version of each unique entity across all processed windows.

Returns:
    List of unique EntityMention objects

## `class StreamingRelationshipExtractorInterface(ABC)`

Interface for extracting relationships from document chunks in streaming fashion.

Extends RelationshipExtractorInterface with windowed processing. This is useful for:
    - Large documents that exceed LLM context windows
    - Processing relationships as entities are discovered
    - Limiting relationship extraction to relevant windows (entities nearby)

Implementations should consider:
    - Only extracting relationships between entities within the same window
    - Using overlapping windows to catch cross-boundary relationships
    - Deduplicating relationships found in multiple overlapping windows

### `def StreamingRelationshipExtractorInterface.extract_windowed(self, chunks: Sequence[DocumentChunk], entities: Sequence[BaseEntity], window_size: int = 2000) -> AsyncIterator[list[BaseRelationship]]`

Extract relationships from windowed chunks.

Note: This method is not async - it returns an AsyncIterator that can be
iterated with `async for`. This is the correct pattern for async generators.

Args:
    chunks: Document chunks to process
    entities: All entities found in the document (with position info)
    window_size: Size of context window for relationship extraction

Yields:
    Lists of BaseRelationship objects found in each window

## `class WindowedRelationshipExtractor(StreamingRelationshipExtractorInterface)`

Extracts relationships using sliding windows over document chunks.

This implementation wraps a standard RelationshipExtractorInterface and applies
it to overlapping windows of the document. Only entities that appear within
the same window are considered for relationship extraction.

This approach is particularly useful for:
    - LLM-based extractors with limited context windows
    - Reducing false positives by focusing on nearby entities
    - Improving performance by limiting entity combinations

Example:
    ```python
    base_extractor = MyRelationshipExtractor()
    windowed_extractor = WindowedRelationshipExtractor(
        base_extractor=base_extractor,
        window_size=2000
    )

    async for relationships in windowed_extractor.extract_windowed(
        chunks, entities, window_size=2000
    ):
        await store_relationships(relationships)
    ```

### `def WindowedRelationshipExtractor.__init__(self, base_extractor: RelationshipExtractorInterface)`

Initialize the windowed relationship extractor.

Args:
    base_extractor: The underlying relationship extractor


<span id="user-content-kgraphpromotionpy"></span>

# kgraph/promotion.py

Legacy promotion utilities.

This module is kept for backwards compatibility. The PromotionPolicy ABC
has been moved to kgschema.promotion.

## `class TodoPromotionPolicy(PromotionPolicy)`

Placeholder promotion policy that raises NotImplementedError.


<span id="user-content-kgraphqueryinitpy"></span>

# kgraph/query/__init__.py

Query interface for knowledge graph bundles.

This subpackage previously contained bundle models, which have been moved
to the standalone kgbundle package. Import from kgbundle directly.

Example:
    from kgbundle import BundleManifestV1, EntityRow, RelationshipRow


<span id="user-content-kgraphstorageinitpy"></span>

# kgraph/storage/__init__.py

Storage interfaces and implementations for the knowledge graph.


<span id="user-content-kgraphstoragememorypy"></span>

# kgraph/storage/memory.py

In-memory storage implementations for testing and development.

This module provides dictionary-based implementations of the storage interfaces
that keep all data in memory. These implementations are suitable for:

- **Unit testing**: Fast, isolated tests without external dependencies
- **Development**: Quick iteration without database setup
- **Prototyping**: Experimenting with the framework before choosing a backend
- **Small datasets**: Demos and examples with limited data

**Not recommended for production** due to:
- No persistence (data is lost when the process exits)
- No concurrency control (not safe for multi-process access)
- Memory constraints (all data must fit in RAM)
- O(n) search operations (no indexing)

For production use, implement the storage interfaces with a proper database
backend (PostgreSQL with pgvector, Neo4j, etc.).

### `def _cosine_similarity(a: Sequence[float], b: Sequence[float]) -> float`

Compute cosine similarity between two embedding vectors.

Cosine similarity measures the cosine of the angle between two vectors,
producing a value between -1 (opposite) and 1 (identical direction).
For normalized embeddings, this is equivalent to the dot product.

Args:
    a: First embedding vector.
    b: Second embedding vector (must have same dimension as a).

Returns:
    Cosine similarity score between -1 and 1. Returns 0.0 if vectors
    have different lengths, are empty, or have zero magnitude.

## `class InMemoryEntityStorage(EntityStorageInterface)`

In-memory entity storage using a dictionary keyed by entity_id.

Stores entities in a simple `dict[str, BaseEntity]` structure. All
operations are O(1) for direct lookups and O(n) for searches.

Thread safety: Not thread-safe. For concurrent access, use external
synchronization or a production database backend.

Example:
    ```python
    storage = InMemoryEntityStorage()
    await storage.add(my_entity)
    entity = await storage.get(my_entity.entity_id)
    ```

### `def InMemoryEntityStorage.__init__(self) -> None`

Initialize an empty entity storage.

## `class InMemoryRelationshipStorage(RelationshipStorageInterface)`

In-memory relationship storage using triple keys.

Stores relationships in a dictionary keyed by (subject_id, predicate, object_id)
tuples. This ensures uniqueness of triples and provides O(1) lookup for
specific relationships.

Traversal queries (get_by_subject, get_by_object) are O(n) as they scan
all relationships. For large graphs, use a database with proper indices.

Thread safety: Not thread-safe. For concurrent access, use external
synchronization or a production database backend.

Example:
    ```python
    storage = InMemoryRelationshipStorage()
    await storage.add(my_relationship)
    outgoing = await storage.get_by_subject(entity_id)
    ```

### `def InMemoryRelationshipStorage.__init__(self) -> None`

Initialize an empty relationship storage.

### `def InMemoryRelationshipStorage._make_key(self, rel: BaseRelationship) -> tuple[str, str, str]`

Create a dictionary key from a relationship's triple.

## `class InMemoryDocumentStorage(DocumentStorageInterface)`

In-memory document storage using a dictionary keyed by document_id.

Stores documents in a simple `dict[str, BaseDocument]` structure.
Document lookups by ID are O(1); lookups by source URI are O(n).

Thread safety: Not thread-safe. For concurrent access, use external
synchronization or a production database backend.

Example:
    ```python
    storage = InMemoryDocumentStorage()
    await storage.add(my_document)
    doc = await storage.get(my_document.document_id)
    ```

### `def InMemoryDocumentStorage.__init__(self) -> None`

Initialize an empty document storage.


<span id="user-content-kgschemacanonicalidpy"></span>

# kgschema/canonical_id.py

Canonical ID model for knowledge graph entities.

This module provides the CanonicalId model representing stable identifiers
from authoritative sources (UMLS, MeSH, HGNC, etc.).

## `class CanonicalId(BaseModel)`

Represents a canonical identifier from an authoritative source.

A canonical ID uniquely identifies an entity in an authoritative ontology
(e.g., UMLS, MeSH, HGNC, RxNorm, UniProt, DBPedia). This model stores
the ID, its URL (if available), and synonyms that map to this ID.

Attributes:
    id: The canonical ID string (e.g., "UMLS:C12345", "MeSH:D000570", "HGNC:1100")
    url: Optional URL to the authoritative source page for this ID
    synonyms: List of alternative names/terms that map to this canonical ID

**Fields:**

```python
id: str = Field(...)
url: Optional[str] = None
synonyms: tuple[str, ...] = Field(default_factory=tuple)
```


### `def CanonicalId.__str__(self) -> str`

String representation of the canonical ID.


<span id="user-content-kgschemadocumentpy"></span>

# kgschema/document.py

Document representation for the knowledge graph framework.

This module defines `BaseDocument`, the abstract base class for all documents
processed by the knowledge graph ingestion pipeline. Documents represent the
source material from which entities and relationships are extracted.

A document contains:
    - **Content**: The full text of the document
    - **Metadata**: Title, source URI, content type, creation timestamp
    - **Structure**: Domain-specific sections via `get_sections()`

Domain implementations subclass `BaseDocument` to add domain-specific fields
and structure. For example:
    - `JournalArticle` might add fields for authors, abstract, and citations
    - `LegalDocument` might add fields for court, case number, and parties
    - `ConferencePaper` might add fields for venue, year, and keywords

Documents are immutable (frozen Pydantic models) to ensure consistency
throughout the extraction pipeline.

## `class BaseDocument(ABC, BaseModel)`

Abstract base class for documents in the knowledge graph.

Represents a parsed document ready for entity and relationship extraction.
All documents share common fields (ID, content, metadata) while subclasses
add domain-specific structure and fields.

Documents are frozen (immutable) Pydantic models, ensuring they cannot be
modified after creation. This immutability guarantees consistency when
documents are referenced by multiple entities and relationships.

Subclasses must implement:
    - `get_document_type()`: Return the domain-specific document type
    - `get_sections()`: Return document structure as (name, content) tuples

Example:
    ```python
    class JournalArticle(BaseDocument):
        authors: tuple[str, ...] = Field(default=())
        abstract: str | None = None

        def get_document_type(self) -> str:
            return "journal_article"

        def get_sections(self) -> list[tuple[str, str]]:
            sections = []
            if self.abstract:
                sections.append(("abstract", self.abstract))
            sections.append(("body", self.content))
            return sections
    ```

**Fields:**

```python
document_id: str = Field(...)
title: str | None = None
content: str = Field(...)
content_type: str = Field(...)
source_uri: str | None = None
created_at: datetime = Field(...)
metadata: dict = Field(default_factory=dict)
```


### `def BaseDocument.get_document_type(self) -> str`

Return domain-specific document type.

Examples: 'journal_article', 'clinical_trial' for medical;
'case_opinion', 'statute' for legal; 'conference_paper' for CS.

### `def BaseDocument.get_sections(self) -> list[tuple[str, str]]`

Return document sections as (section_name, content) tuples.

Allows domain-specific document structure. For unstructured documents,
return a single section like [('body', self.content)].


<span id="user-content-kgschemadomainpy"></span>

# kgschema/domain.py

Domain schema definition for the knowledge graph framework.

A domain schema defines the vocabulary and rules for a specific knowledge
domain (medical literature, legal documents, academic CS papers, etc.).
Each domain specifies:

- **Entity types**: The kinds of entities that can exist (drugs, diseases,
  legal cases, algorithms, etc.) and their concrete class implementations.

- **Relationship types**: Valid predicates between entities (treats, cites,
  implements, etc.) and their class implementations.

- **Document types**: Source document formats the domain processes (journal
  articles, court filings, conference papers, etc.).

- **Validation rules**: Domain-specific constraints on entities and
  relationships beyond basic type checking.

- **Promotion configuration**: Thresholds for promoting provisional entities
  to canonical status, which may vary by domain based on data quality and
  external authority availability.

The domain schema serves as the central configuration point for domain-specific
behavior, allowing the core knowledge graph framework to remain domain-agnostic
while supporting specialized use cases.

Example usage:
    ```python
    class MedicalDomainSchema(DomainSchema):
        @property
        def name(self) -> str:
            return "medical"

        @property
        def entity_types(self) -> dict[str, type[BaseEntity]]:
            return {"drug": DrugEntity, "disease": DiseaseEntity, "gene": GeneEntity}

        @property
        def relationship_types(self) -> dict[str, type[BaseRelationship]]:
            return {"treats": TreatsRelationship, "causes": CausesRelationship}
        # ... etc
    ```

## `class ValidationIssue(BaseModel)`

A structured validation error with location and diagnostic information.

Provides detailed information about why validation failed, enabling
better error messages and programmatic handling of validation failures.

Attributes:
    field: The field that failed validation (e.g., "entity_type", "confidence")
    message: Human-readable description of the issue
    value: The invalid value (optional, for debugging)
    code: Machine-readable error code (optional, for programmatic handling)

**Fields:**

```python
field: str = Field(...)
message: str = Field(...)
value: str | None = None
code: str | None = None
```


## `class PredicateConstraint(BaseModel)`

Defines the valid subject and object entity types for a predicate.

**Fields:**

```python
subject_types: set[str] = Field(default_factory=set)
object_types: set[str] = Field(default_factory=set)
```


## `class Provenance(BaseModel)`

Tracks the precise location of extracted information within a document.

Used to record where entities, relationships, and other extracted data
originated, enabling traceability back to source text.

Fields:
    document_id: Unique identifier of the source document
    source_uri: Optional URI/path to the original document
    section: Name of the document section (e.g., "abstract", "methods", "results")
    paragraph: Paragraph number/index within the section (0-based)
    start_offset: Character offset where the relevant text begins
    end_offset: Character offset where the relevant text ends

**Fields:**

```python
document_id: str = Field(...)
source_uri: str | None = None
section: str | None = None
paragraph: int | None = None
start_offset: int | None = None
end_offset: int | None = None
```


## `class Evidence(BaseModel)`

**Fields:**

```python
kind: str
source_documents: tuple[str, ...] = Field(...)
primary: Provenance | None = None
mentions: tuple[Provenance, ...] = ()
notes: dict[str, object] = Field(default_factory=dict)
```


## `class DomainSchema(ABC)`

Abstract schema definition for a knowledge domain.

Each domain (medical, legal, CS papers, etc.) implements this interface
to define its vocabulary of types and validation rules. The schema is
used throughout the ingestion pipeline to:

- Validate extracted entities and relationships before storage
- Configure entity promotion thresholds
- Determine valid predicates between entity type pairs
- Deserialize domain-specific entity/relationship subclasses

Implementations should be stateless and thread-safe, as the same schema
instance may be used across multiple concurrent ingestion operations.

Required methods to implement:
    - name: Unique domain identifier
    - entity_types: Registry of entity type names to classes
    - relationship_types: Registry of predicate names to classes
    - document_types: Registry of document format names to classes
    - validate_entity: Domain-specific entity validation
    - validate_relationship: Domain-specific relationship validation

Optional methods to override:
    - promotion_config: Customize promotion thresholds
    - get_valid_predicates: Restrict predicates by entity type pair

### `def DomainSchema.name(self) -> str`

Return the unique identifier for this domain.

The domain name is used for:
    - Namespacing entities in multi-domain deployments
    - Selecting the correct deserializer for stored data
    - Logging and debugging

Returns:
    A short, lowercase identifier (e.g., 'medical', 'legal', 'cs_papers').
    Should contain only alphanumeric characters and underscores.

### `def DomainSchema.entity_types(self) -> dict[str, type[BaseEntity]]`

Return the registry of entity types for this domain.

Maps type name strings to concrete BaseEntity subclasses. The type
names are used in entity extraction and must match the values
returned by each entity's `get_entity_type()` method.

Returns:
    Dictionary mapping entity type names to their implementing classes.

Example:
    ```python
    return {
        'drug': DrugEntity,
        'disease': DiseaseEntity,
        'gene': GeneEntity,
    }
    ```

### `def DomainSchema.relationship_types(self) -> dict[str, type[BaseRelationship]]`

Return the registry of relationship types for this domain.

Maps predicate name strings to concrete BaseRelationship subclasses.
The predicate names define the vocabulary of edges in the knowledge
graph and must match the values used in relationship extraction.

Returns:
    Dictionary mapping predicate names to their implementing classes.

Example:
    ```python
    return {
        'treats': TreatsRelationship,
        'causes': CausesRelationship,
        'interacts_with': InteractionRelationship,
    }
    ```

### `def DomainSchema.predicate_constraints(self) -> dict[str, PredicateConstraint]`

Return a dictionary of predicate constraints for this domain.

This maps predicate names to a PredicateConstraint object, which
defines the valid subject and object entity types for that predicate.
These constraints are used to validate relationships during ingestion
and to filter valid predicates for a given subject-object pair.

Returns:
    Dictionary mapping predicate names (e.g., "treats") to
    PredicateConstraint instances.

Example:
    ```python
    return {
        "treats": PredicateConstraint(
            subject_types={"drug", "procedure"},
            object_types={"disease", "symptom"},
        ),
        "causes": PredicateConstraint(
            subject_types={"gene", "exposure"},
            object_types={"disease"},
        ),
    }
    ```

### `def DomainSchema.document_types(self) -> dict[str, type[BaseDocument]]`

Return the registry of document types for this domain.

Maps document format names to concrete BaseDocument subclasses.
Different document types may have different structures and metadata
fields relevant to the domain.

Returns:
    Dictionary mapping document type names to their implementing classes.

Example:
    ```python
    return {
        'journal_article': JournalArticle,
        'clinical_trial': ClinicalTrialDocument,
        'drug_label': DrugLabelDocument,
    }
    ```

### `def DomainSchema.promotion_config(self) -> PromotionConfig`

Return the configuration for promoting provisional entities.

Promotion configuration controls when provisional entities (newly
discovered mentions without canonical IDs) are promoted to canonical
status. The thresholds should be tuned based on:

- Data quality: Noisy extraction requires higher thresholds
- External authority availability: Domains with good authorities
  (UMLS, DBPedia) can use higher confidence requirements
- Entity importance: Critical domains may require more evidence

Override this property to customize thresholds for your domain.
The default configuration uses framework defaults.

Returns:
    PromotionConfig with min_usage_count, min_confidence, and
    require_embedding settings appropriate for this domain.

### `def DomainSchema.validate_entity(self, entity: BaseEntity) -> list[ValidationIssue]`

Validate an entity against domain-specific rules.

Called by the ingestion pipeline before storing an entity. Use this
to enforce constraints beyond basic type checking, such as:

- Required fields for specific entity types
- Value constraints (e.g., confidence thresholds)
- Cross-field validation (e.g., canonical entities must have IDs)

Args:
    entity: The entity to validate.

Returns:
    Empty list if the entity is valid, otherwise a list of ValidationIssue
    objects describing each validation failure. Multiple issues can be
    returned to help users fix all problems at once.

Example:
    ```python
    def validate_entity(self, entity: BaseEntity) -> list[ValidationIssue]:
        issues = []
        if entity.get_entity_type() not in self.entity_types:
            issues.append(ValidationIssue(
                field="entity_type",
                message=f"Unknown entity type: {entity.get_entity_type()}",
                value=entity.get_entity_type(),
                code="UNKNOWN_TYPE",
            ))
        return issues
    ```

Note:
    At minimum, implementations should verify that the entity's type
    (from `get_entity_type()`) is registered in `entity_types`.

### `def DomainSchema.get_valid_predicates(self, subject_type: str, object_type: str) -> list[str]`

Return predicates valid between two entity types.

Override this method to enforce type-specific relationship constraints.
For example, in a medical domain, "treats" might only be valid from
Drug to Disease, not from Disease to Drug.

The default implementation allows any predicate registered in
`relationship_types` between any entity type pair.

Args:
    subject_type: The entity type of the relationship subject.
    object_type: The entity type of the relationship object.

Returns:
    List of predicate names that are valid for this entity type pair.
    Returns an empty list if no predicates are valid.

Example:
    ```python
    def get_valid_predicates(self, subject_type: str, object_type: str) -> list[str]:
        if subject_type == "drug" and object_type == "disease":
            return ["treats", "prevents", "exacerbates"]
        if subject_type == "gene" and object_type == "disease":
            return ["associated_with", "causes"]
        return []  # No other combinations allowed
    ```

### `def DomainSchema.get_promotion_policy(self, lookup = None) -> PromotionPolicy`

Return the promotion policy for this domain.

Override this method to provide domain-specific promotion logic.
Default implementation raises NotImplementedError.

Args:
    lookup: Optional canonical ID lookup service. Domains that support
           external lookups can use this to pass the service to the policy.

### `def DomainSchema.evidence_model(self) -> type[Evidence]`

Return the domain's version of Evidence

The domain can add stuff to the evidence:
    - A predicate might be supported (or counter-argued) by lab data or test results
    - Or by whether some paper from the past was retracted or not

Returns:
    A type that is, or is a subclass of, the Evidence type

### `def DomainSchema.provenance_model(self) -> type[Provenance]`

Return the domain's version of Provenance

The domain can add stuff to the provenance, much like Evidence

Returns:
    A type that is, or is a subclass of, the Provenance type


<span id="user-content-kgschemaentitypy"></span>

# kgschema/entity.py

Entity system for the knowledge graph framework.

This module defines the core entity types for the knowledge graph:

- **BaseEntity**: Abstract base class for all domain entities (nodes in the graph)
- **EntityMention**: Raw entity extraction from a document (before resolution)
- **EntityStatus**: Enum distinguishing canonical vs provisional entities
- **PromotionConfig**: Configuration for promoting provisional → canonical

**Entity Lifecycle:**

1. **Extraction**: The entity extractor finds mentions in document text,
   producing `EntityMention` objects with text spans and confidence scores.

2. **Resolution**: The entity resolver maps mentions to `BaseEntity` instances,
   either matching existing entities or creating new provisional ones.

3. **Promotion**: Provisional entities that accumulate sufficient usage and
   confidence are promoted to canonical status with stable identifiers.

4. **Merging**: Duplicate canonical entities detected via embedding similarity
   can be merged to maintain a clean entity vocabulary.

Entities are immutable (frozen Pydantic models) to ensure consistency when
referenced by relationships and stored in multiple indices.

## `class EntityStatus(str, Enum)`

Lifecycle status of an entity in the knowledge graph.

Entities progress through a lifecycle from provisional (newly discovered)
to canonical (stable, authoritative). This status determines how the
entity is treated in queries, exports, and merge operations.

## `class PromotionConfig(BaseModel)`

Configuration for promoting provisional entities to canonical status.

Controls the thresholds that determine when a provisional entity has
accumulated enough evidence to be promoted. Different domains may
require different thresholds based on data quality and the availability
of external authority sources.

Attributes:
    min_usage_count: Minimum times the entity must appear across documents.
    min_confidence: Minimum confidence score from entity resolution.
    require_embedding: Whether an embedding vector is required for promotion.

**Fields:**

```python
min_usage_count: int = 3
min_confidence: float = 0.8
require_embedding: bool = True
```


## `class BaseEntity(ABC, BaseModel)`

Abstract base class for all domain entities (knowledge graph nodes).

Entities represent the nodes in the knowledge graph—the "things" that
relationships connect. Each entity has a unique identifier, a primary
name, optional synonyms, and domain-specific attributes.

Entities are frozen (immutable) Pydantic models. To modify an entity,
use `entity.model_copy(update={...})` to create a new instance with
updated fields.

Subclasses must implement:
    - `get_entity_type()`: Return the domain-specific type identifier

Key fields:
    - `entity_id`: Unique identifier (canonical ID or provisional UUID)
    - `status`: CANONICAL or PROVISIONAL lifecycle state
    - `name`: Primary display name
    - `synonyms`: Alternative names for matching
    - `embedding`: Semantic vector for similarity operations
    - `usage_count`: Number of document references (for promotion)
    - `confidence`: Resolution confidence score

Example:
    ```python
    class DrugEntity(BaseEntity):
        drug_class: str | None = None
        mechanism: str | None = None

        def get_entity_type(self) -> str:
            return "drug"
    ```

**Fields:**

```python
promotable: bool = True
entity_id: str = Field(...)
status: EntityStatus = EntityStatus.PROVISIONAL
name: str = Field(...)
synonyms: tuple[str, ...] = ()
embedding: tuple[float, ...] | None = None
canonical_ids: dict[str, str] = Field(default_factory=dict)
confidence: float = 1.0
usage_count: int = 0
created_at: datetime = Field(...)
source: str = Field(...)
metadata: dict = Field(default_factory=dict)
```


### `def BaseEntity.get_entity_type(self) -> str`

Return domain-specific entity type identifier.
Examples: 'drug', 'disease', 'gene' for medical domain;
'case', 'statute', 'court' for legal domain.

## `class EntityMention(BaseModel)`

A raw entity mention extracted from document text.

Represents the output of entity extraction (Pass 1) before resolution
to a canonical or provisional entity. Captures the exact text span,
its position in the document, and extraction confidence.

Entity mentions are intermediate objects that flow from the extractor
to the resolver. The resolver then maps each mention to an existing
entity or creates a new provisional entity.

Frozen (immutable) to ensure mentions can be safely passed through
the pipeline without modification.

Attributes:
    text: The exact text span that was identified as an entity.
    entity_type: Domain-specific type classification (e.g., "drug", "gene").
    start_offset: Character position where the mention begins.
    end_offset: Character position where the mention ends.
    confidence: Extraction confidence score (0.0 to 1.0).
    context: Optional surrounding text for disambiguation.
    metadata: Domain-specific extraction metadata.

Example:
    ```python
    mention = EntityMention(
        text="aspirin",
        entity_type="drug",
        start_offset=42,
        end_offset=49,
        confidence=0.95,
        context="...patients taking aspirin showed improved...",
    )
    ```

**Fields:**

```python
text: str = Field(...)
entity_type: str = Field(...)
start_offset: int = Field(...)
end_offset: int = Field(...)
confidence: float = 1.0
context: str | None = None
metadata: dict = Field(default_factory=dict)
```



<span id="user-content-kgschemainitpy"></span>

# kgschema/__init__.py

Knowledge Graph Schema - Base Models and Interfaces

This package contains only Pydantic models and ABC interfaces with no
functional code. It defines:

- Entity, relationship, and document base classes
- Domain schema interface
- Storage interfaces
- Canonical ID model
- Promotion policy interface

These are used by both kgraph (ingestion) and can be referenced by
domain implementations.


<span id="user-content-kgschemapromotionpy"></span>

# kgschema/promotion.py

Promotion policy interface for entity canonical ID assignment.

This module provides the PromotionPolicy ABC that defines how domains
assign canonical IDs to entities during promotion from provisional to
canonical status.

## `class PromotionPolicy(ABC)`

Abstract base for domain-specific entity promotion policies.

### `def PromotionPolicy.should_promote(self, entity: BaseEntity) -> bool`

Check if entity meets promotion thresholds.


<span id="user-content-kgschemarelationshippy"></span>

# kgschema/relationship.py

Relationship system for the knowledge graph framework.

This module defines `BaseRelationship`, the abstract base class for all
relationships (edges) in the knowledge graph. Relationships connect entities
via typed predicates, forming the graph structure.

Each relationship is a triple: (subject_entity, predicate, object_entity)

- **Subject**: The entity performing or originating the action
- **Predicate**: The relationship type (domain-specific vocabulary)
- **Object**: The entity receiving or being affected by the action

For example:
    - ("Aspirin", "treats", "Headache")
    - ("Paper A", "cites", "Paper B")
    - ("Court Case X", "overrules", "Court Case Y")

Relationships also track:
    - **Confidence**: How certain we are about this relationship
    - **Source documents**: Which documents support this relationship
    - **Metadata**: Domain-specific evidence and provenance

Relationships are immutable (frozen Pydantic models) and are typically
extracted during Pass 2 of the ingestion pipeline, after entities have
been resolved.

## `class BaseRelationship(ABC, BaseModel)`

Abstract base class for relationships (edges) in the knowledge graph.

Relationships connect two entities via a typed predicate, representing
facts extracted from source documents. The relationship model supports
aggregating evidence from multiple documents that assert the same fact.

Relationships are frozen (immutable) Pydantic models. To update a
relationship (e.g., to add a new source document), use
`rel.model_copy(update={...})` to create a new instance.

Subclasses must implement:
    - `get_edge_type()`: Return the domain-specific edge type category

Key fields:
    - `subject_id`: Entity ID of the relationship source (the "doer")
    - `predicate`: Relationship type from the domain vocabulary
    - `object_id`: Entity ID of the relationship target (the "receiver")
    - `confidence`: How certain we are about this relationship
    - `source_documents`: Documents that support this relationship

Example:
    ```python
    class TreatsRelationship(BaseRelationship):
        mechanism: str | None = None  # How the treatment works
        evidence_level: str = "observational"

        def get_edge_type(self) -> str:
            return "treats"
    ```

**Fields:**

```python
subject_id: str = Field(...)
predicate: str = Field(...)
object_id: str = Field(...)
confidence: float = 1.0
source_documents: tuple[str, ...] = ()
evidence: Any = None
created_at: datetime = Field(...)
last_updated: datetime | None = None
metadata: dict = Field(default_factory=dict)
```


### `def BaseRelationship.get_edge_type(self) -> str`

Return domain-specific edge type category.

Examples: 'treats', 'causes', 'interacts_with' for medical domain;
'cites', 'overrules', 'interprets' for legal domain.


<span id="user-content-kgschemastoragepy"></span>

# kgschema/storage.py

Storage interface definitions for the knowledge graph framework.

This module defines abstract interfaces for persisting knowledge graph data:
entities, relationships, and documents. These interfaces decouple the core
framework from specific storage backends, enabling:

- **In-memory storage** for testing and development
- **Relational databases** (PostgreSQL, MySQL) for ACID guarantees
- **Vector databases** (Pinecone, Weaviate, Qdrant) for embedding search
- **Graph databases** (Neo4j, ArangoDB) for relationship traversal

All interfaces are async-first to support non-blocking I/O with database
drivers like asyncpg, motor, or aioredis.

The storage layer supports key knowledge graph operations:
    - Entity lifecycle: create, read, update, delete, promote, merge
    - Relationship management: add, query by subject/object, update references
    - Document tracking: store source documents for provenance
    - Similarity search: find entities by embedding vectors

## `class EntityStorageInterface(ABC)`

Abstract interface for entity storage operations.

Entity storage is the primary persistence layer for knowledge graph nodes.
It must support both basic CRUD operations and specialized queries for
the entity lifecycle:

- **Canonical entities**: Stable entities linked to authoritative sources
  (UMLS CUIs, DBPedia URIs, etc.)
- **Provisional entities**: Newly discovered mentions awaiting promotion
  based on usage frequency and confidence thresholds

Implementations must handle:
    - Efficient lookup by ID and name/synonyms
    - Embedding-based similarity search for resolution and merge detection
    - Atomic promotion (provisional → canonical) and merge operations
    - Pagination for listing large entity collections

Thread safety: Implementations should be safe for concurrent access from
multiple async tasks.

## `class RelationshipStorageInterface(ABC)`

Abstract interface for relationship (edge) storage operations.

Relationships represent the edges in the knowledge graph, connecting
entity nodes via typed predicates. Each relationship is a triple:
(subject_entity, predicate, object_entity) with additional metadata.

Key operations:
    - **Graph traversal**: Query outgoing edges (by subject) or
      incoming edges (by object) for graph navigation
    - **Triple lookup**: Check if a specific relationship exists
    - **Reference updates**: Maintain referential integrity when
      entities are promoted or merged
    - **Provenance tracking**: Query relationships by source document

Relationships may be extracted from multiple documents, so implementations
should support aggregating evidence (confidence scores, source documents)
when the same triple is extracted repeatedly.

## `class DocumentStorageInterface(ABC)`

Abstract interface for document storage operations.

Document storage provides persistence for source documents that have been
ingested into the knowledge graph. Documents are retained for:

- **Provenance**: Track where entities and relationships originated
- **Deduplication**: Detect if a document has already been processed
- **Re-processing**: Enable re-extraction when pipeline components improve
- **Debugging**: Examine source content when validating extractions

Documents may store full content or just metadata, depending on storage
constraints and use case requirements.


<span id="user-content-kgserverdockercomposeguidemd"></span>

# kgserver/DOCKER_COMPOSE_GUIDE.md

# Docker Compose Setup - Quick Start Guide

This docker-compose configuration brings up PostgreSQL (with pgvector) and Redis for local development and testing.

## Quick Start

```bash
# Start both services
docker compose up -d

# Check status
docker compose ps

# View logs
docker compose logs -f

# Stop services
docker compose down

# Stop and remove volumes (destroys data)
```

    ...


<span id="user-content-kgserverdockerfile"></span>

# kgserver/Dockerfile

```dockerfile
# Multi-stage build for KG server
FROM python:3.13-slim AS builder

# Install uv for fast dependency management
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

WORKDIR /app

# Copy kgbundle first (local dependency)
COPY kgbundle ./kgbundle

# Copy dependency files from kgserver
COPY kgserver/pyproject.toml ./
COPY kgserver/uv.lock ./

# Install ALL dependencies using uv (not pip)
RUN apt update && \
    apt install -y git vim ripgrep
# Re-lock after path fix, then sync with relocatable venv
RUN uv venv --python /usr/local/bin/python3 --relocatable && \

    ...
```


<span id="user-content-kgserverdockersetupmd"></span>

# kgserver/DOCKER_SETUP.md

# Docker-Compose Setup Guide

This guide walks through setting up the complete docker-compose stack with persistent data
and running the ingestion pipeline.

## Prerequisites

- Docker and docker-compose installed
- `uv` installed for running Python commands
- PMC XML files in `ingest/pmc_xmls/` directory

## Step 1: Start the Docker Stack

Start all services (postgres, redis, ollama):

```bash
docker-compose up -d
```

Check that all services are running:

    ...


<span id="user-content-kgserverdocsarchitecturemd"></span>

# kgserver/docs/architecture.md

# Architecture

![Alt Text](GraphiQL_screenshot.png)

## Producer artifacts vs server bundle

This distinction is foundational to the design of the KG server and should be understood before modifying ingestion, storage, or bundle-loading code.

### Summary

This project has a deliberate separation between:

1. **Producer-side artifacts** (domain-aware, ingestion-time truth)
2. **Server bundle** (domain-neutral, serving-time contract)

The **JSON artifacts produced by ingestion are the single source of truth**. Postgres is a materialized index that can be regenerated from the bundle.

### Producer-side artifacts (single source of truth)

Producer pipelines (e.g., Sherlock, med-lit, future domains) ingest raw sources and emit rich internal artifacts such as:

    ...


<span id="user-content-kgservergraphqlvibesmd"></span>

# kgserver/GRAPHQL_VIBES.md

# Knowledge Graph GraphQL API

This document describes the **GraphQL API** for querying the knowledge graph. The API is designed to be easy to use, hard to abuse, and domain-neutral - it doesn't force premature ontology decisions.

## API Design Principles

* **Read-only**: The API currently supports queries only (no mutations).
* **Explicit pagination**: All list queries require pagination parameters to prevent unbounded result sets.
* **Canonical fields**: Only standard server schema fields are first-class; domain-specific data stays in `properties: JSON`.
* **Narrow filtering**: Filtering supports exact matches and a few safe pattern-matching helpers.
* **No graph traversal**: The API does not currently support graph traversal queries.

---

## GraphQL Schema

### Scalars

* `JSON` (for `properties`)
* `DateTime` (for timestamps, returned as ISO 8601 strings)

    ...


<span id="user-content-kgserverindexmd"></span>

# kgserver/index.md

# Flexible server for knowledge graphs

This repository contains a **domain-neutral knowledge graph server**. While it has primarily
been developed for medical literature, the same general architecture can serve other
information-dense literatures (legal, financial, academic, etc).

![subgraph](subgraph.jpg)

Links:

- [OpenAPI spec](/docs)
- [Graph visualization](/graph-viz/) -- currently focused on medical literature
- [GraphQL GUI](/graphiql/)
- [Jupyter notebook](/jupyter/)

Important architectural note:

- The server does **not** ingest raw documents directly.
- Domain-specific ingestion pipelines produce rich JSON artifacts internally.
- A finalized, validated bundle is exported and loaded by the server as-is at startup.

    ...


<span id="user-content-kgserverlintsh"></span>

# kgserver/lint.sh

```sh
#!/bin/bash -e

fixes_needed() {
    echo "Something needs fixing, trying to fix it"
    set -x
    sed -i "s/ \+$//" $(git ls-files | grep -E "\.py$")
    uv run ruff check --fix . && uv run black .
}

echo "=========================================="
echo "Running Linters and Tests"
echo "=========================================="

# Ensure uv is available
if ! command -v uv &> /dev/null; then
    echo "Error: uv not found. Please install uv first."
    echo "See: https://docs.astral.sh/uv/getting-started/installation/"
    exit 1
fi

    ...
```


<span id="user-content-kgserverlocaldevmd"></span>

# kgserver/LOCAL_DEV.md

# Local Development Guide

This guide covers running kgserver locally while using Docker for PostgreSQL.

## Quick Start (Hybrid Setup)

The most common development setup: PostgreSQL in Docker, Python server running directly.

### 1. Start PostgreSQL

```bash
cd kgserver
docker compose up -d postgres
```

This creates a database named `kgserver` (as defined in `docker-compose.yml`).

### 2. Set Environment Variables

```bash
```

    ...


<span id="user-content-kgservermainpy"></span>

# kgserver/main.py



<span id="user-content-kgservermcpgqlwrappermd"></span>

# kgserver/MCP_GQL_WRAPPER.md

# Building an MCP Wrapper for the Knowledge Graph GraphQL API

This document describes how to create a **Model Context Protocol (MCP) wrapper** that enables a Large Language Model (LLM) to query a Knowledge Graph GraphQL API. The goal is to make the knowledge graph easily accessible for LLM-based agents, providing entry points, schemas, and guidance for exploration—all exposed via a modern, asynchronous Python web service (FastAPI).

## Table of Contents

- [Overview](#overview)
- [Why Use an MCP Wrapper?](#why-use-an-mcp-wrapper)
- [Architecture](#architecture)
- [Implementation Skeleton](#implementation-skeleton)
- [API Endpoints](#api-endpoints)
- [Suggested LLM Prompts](#suggested-llm-prompts)
- [Example Async GraphQL Query Code](#example-async-graphql-query-code)
- [Security and Rate Limiting](#security-and-rate-limiting)
- [Extending for Traversal (Future)](#extending-for-traversal-future)
- [References](#references)

---

## Overview

    ...


<span id="user-content-kgservermcpserverinitpy"></span>

# kgserver/mcp_server/__init__.py

MCP (Model Context Protocol) server for Knowledge Graph GraphQL API.

This module provides an MCP server that wraps the GraphQL API, making it
accessible to AI agents like Claude or Cursor IDE.

The server can run in two modes:
1. HTTP/SSE mode: Mounted as FastAPI routes for remote access
2. STDIO mode: Standalone server for local subprocess communication


<span id="user-content-kgservermcpserverserverpy"></span>

# kgserver/mcp_server/server.py

MCP Server implementation using FastMCP.

Provides tools for querying the knowledge graph via the Model Context Protocol.

### `def _get_storage()`

Context manager for getting a storage instance with proper lifecycle management.

### `def get_entity(entity_id: str) -> dict | None`

Retrieve a specific entity by its ID.

Returns the full entity data including all metadata, identifiers, and properties.

Args:
    entity_id: The unique identifier of the entity

Returns:
    Entity dictionary with fields: entityId, entityType, name, status,
    confidence, usageCount, source, canonicalUrl, synonyms, properties.
    Returns None if entity not found.

### `def list_entities(limit: int = 100, offset: int = 0, entity_type: Optional[str] = None, name: Optional[str] = None, name_contains: Optional[str] = None, source: Optional[str] = None, status: Optional[str] = None) -> dict`

List entities with pagination and optional filtering.

This tool provides flexible querying of entities in the knowledge graph.
You can filter by type, name (exact or partial), source, and status.

Args:
    limit: Maximum number of entities to return (default: 100, max: 100)
    offset: Number of entities to skip for pagination (default: 0)
    entity_type: Filter by entity type (e.g., "Disease", "Gene", "Drug")
    name: Exact name match filter
    name_contains: Partial name match filter (case-insensitive)
    source: Filter by source (e.g., "UMLS", "HGNC", "RxNorm")
    status: Filter by status (e.g., "canonical", "provisional")

Returns:
    Dictionary with keys: items (list of entities), total (total count),
    limit, offset. Each entity has the same structure as get_entity.

### `def search_entities(query: str, entity_type: Optional[str] = None, limit: int = 10) -> list[dict]`

Search for entities by name (convenience wrapper around list_entities).

This performs a simple name-based search using the name_contains filter.
For more advanced filtering, use list_entities directly.

Args:
    query: Search query text (searches in entity names)
    entity_type: Optional entity type filter
    limit: Maximum number of results to return (default: 10, max: 100)

Returns:
    List of matching entity dictionaries

### `def get_relationship(subject_id: str, predicate: str, object_id: str) -> dict | None`

Retrieve a specific relationship by its triple (subject, predicate, object).

Returns the full relationship data including confidence, source documents, and properties.

Args:
    subject_id: The subject entity ID
    predicate: The relationship predicate/type
    object_id: The object entity ID

Returns:
    Relationship dictionary with fields: subjectId, predicate, objectId,
    confidence, sourceDocuments, properties. Returns None if relationship not found.

### `def find_relationships(subject_id: Optional[str] = None, predicate: Optional[str] = None, object_id: Optional[str] = None, limit: int = 100, offset: int = 0) -> dict`

Find relationships with pagination and optional filtering.

This tool provides flexible querying of relationships in the knowledge graph.
You can filter by subject, predicate, object, or any combination.

Args:
    limit: Maximum number of relationships to return (default: 100, max: 100)
    offset: Number of relationships to skip for pagination (default: 0)
    subject_id: Filter by subject entity ID
    predicate: Filter by relationship predicate/type
    object_id: Filter by object entity ID

Returns:
    Dictionary with keys: items (list of relationships), total (total count),
    limit, offset. Each relationship has the same structure as get_relationship.

### `def get_bundle_info() -> dict | None`

Get bundle metadata for debugging and provenance.

Returns information about the currently loaded knowledge graph bundle,
including bundle ID, domain, creation timestamp, and metadata.

Returns:
    Bundle dictionary with fields: bundleId, domain, createdAt, metadata.
    Returns None if no bundle is loaded.


<span id="user-content-kgservermkdocsyml"></span>

# kgserver/mkdocs.yml

```yaml
site_name: Domain-Agnostic Knowledge Graph Server
site_description: Documentation for the domain-agnostic knowledge graph server
repo_url: https://github.com/wware/kgraph
repo_name: wware/kgraph

theme:
  name: material

markdown_extensions:
  - pymdownx.highlight:
      anchor_linenums: true
  - pymdownx.superfences
  - pymdownx.tabbed:
      alternate_style: true
  - admonition
  - pymdownx.details
  - tables
  - toc:
      permalink: true

    ...
```


<span id="user-content-kgserverpushmainsh"></span>

# kgserver/push_main.sh

```sh
#!/bin/bash -e

fixes_needed() {
    echo "Something needs fixing, try this:"
    echo "uv run ruff check --fix . && uv run black ."
    echo 'sed -i "s/ \+$//" $(git ls-files | grep -E "\.py$")'
    exit 1
}

echo "=========================================="
echo "Running Linters and Tests"
echo "=========================================="

# Ensure uv is available
if ! command -v uv &> /dev/null; then
    echo "Error: uv not found. Please install uv first."
    echo "See: https://docs.astral.sh/uv/getting-started/installation/"
    exit 1
fi

    ...
```


<span id="user-content-kgserverquerybundleloaderpy"></span>

# kgserver/query/bundle_loader.py

Bundle loading utilities for the KG server.
Handles loading bundles from directories or ZIP files at startup.

### `def load_bundle_at_startup(engine, db_url: str) -> None`

Load a bundle at server startup if BUNDLE_PATH is set.

Environment variables:
    BUNDLE_PATH: Path to a bundle directory or ZIP file

### `def _load_from_zip(engine, db_url: str, zip_path: Path) -> None`

Extract and load a bundle from a ZIP file.

### `def _load_from_directory(engine, db_url: str, bundle_dir: Path) -> None`

Load a bundle from a directory.

### `def _find_manifest(search_dir: Path) -> Path | None`

Find manifest.json in a directory (possibly in a subdirectory).

### `def _get_docs_destination_path(asset_path: str, app_docs: Path) -> Path | None`

Determine the destination path for a documentation asset.

### `def _process_single_doc_asset(line: str, bundle_dir: Path, app_docs: Path) -> bool`

Process a single documentation asset entry.

### `def _load_doc_assets(bundle_dir: Path, manifest: BundleManifestV1) -> None`

Load documentation assets from doc_assets.jsonl into docs directory.

Reads the doc_assets.jsonl file (if present) and copies all listed assets
to the docs directory, preserving directory structure. Special handling for
mkdocs.yml which is moved to the app root.

The docs directory defaults to /app/docs (for Docker) but can be overridden
via the KGSERVER_DOCS_DIR environment variable for local development.

Note: These are human-readable documentation files (markdown, images, etc.),
NOT source documents (papers, articles) used for entity extraction.

### `def _build_mkdocs_if_present()`

Build MkDocs documentation if mkdocs.yml exists in the app root.

### `def _initialize_storage(session: Session, db_url: str) -> StorageInterface`

Initialize and return the appropriate StorageInterface.

### `def _handle_force_reload(session: Session, bundle_id: str, storage: StorageInterface) -> bool`

Handle force reload logic, returning True if bundle should be skipped.

### `def _do_load(engine, db_url: str, bundle_dir: Path, manifest_path: Path) -> None`

Actually load the bundle into storage.


<span id="user-content-kgserverquerygraphqlexamplespy"></span>

# kgserver/query/graphql_examples.py

Example GraphQL queries for the Knowledge Graph API.

These queries are displayed in the GraphiQL interface to help users get started.
When a bundle provides its own ``graphql_examples.yml``, that file replaces
the built-in examples at startup.

### `def load_examples(path: Path | None = None) -> None`

Load (or reload) example queries from a YAML file.

Args:
    path: Path to a ``graphql_examples.yml`` file.
          When *None*, the built-in default is used.

### `def get_examples() -> dict[str, str]`

Return the current example queries dict.

### `def get_default_query() -> str`

Return the current default query string.


<span id="user-content-kgserverquerygraphqlexamplesyml"></span>

# kgserver/query/graphql_examples.yml

```yaml
Get Entity by ID: |
  # Retrieve a specific entity by its ID
  query GetEntity {
    entity(id: "holmes:char:JohnWatson") {
      entityId
      name
      entityType
      synonyms
      properties
    }
  }

Search Entities: |
  # Search for entities with pagination
  query SearchEntities {
    entities(limit: 5, offset: 0) {
      items {
        entityId
        name
        entityType

    ...
```


<span id="user-content-kgserverquerygraphqlschemapy"></span>

# kgserver/query/graphql_schema.py

GraphQL schema for the Knowledge Graph API.

This schema uses proper Strawberry types for type safety and better GraphQL introspection.

## `class Entity`

Generic entity GraphQL type.

## `class Relationship`

Generic relationship GraphQL type.

## `class EntityPage`

Paginated result for entities.

## `class RelationshipPage`

Paginated result for relationships.

## `class EntityFilter`

Filter criteria for entity queries.

## `class RelationshipFilter`

Filter criteria for relationship queries.

## `class BundleInfo`

Bundle metadata for debugging and provenance.

### `def Query.entity(self, info: Info, id: str) -> Optional[Entity]`

Retrieve a single entity by its ID.

### `def Query.entities(self, info: Info, limit: int = 100, offset: int = 0, filter: Optional[EntityFilter] = None) -> EntityPage`

List entities with pagination and optional filtering.

### `def Query.relationship(self, info: Info, subject_id: str, predicate: str, object_id: str) -> Optional[Relationship]`

Retrieve a single relationship by its triple.

### `def Query.relationships(self, info: Info, limit: int = 100, offset: int = 0, filter: Optional[RelationshipFilter] = None) -> RelationshipPage`

Find relationships with pagination and optional filtering.

### `def Query.bundle(self, info: Info) -> Optional[BundleInfo]`

Get bundle metadata for debugging and provenance.


<span id="user-content-kgserverquerygraphtraversalpy"></span>

# kgserver/query/graph_traversal.py

Graph traversal logic for subgraph extraction.

Provides BFS-based traversal to extract subgraphs centered on a given entity,
returning D3.js-compatible node and edge data structures.

## `class GraphNode(BaseModel)`

D3-compatible node representation.

**Fields:**

```python
id: str = Field(...)
label: str = Field(...)
entity_type: str = Field(...)
properties: dict[str, Any] = Field(default_factory=dict)
```


## `class GraphEdge(BaseModel)`

D3-compatible edge representation.

**Fields:**

```python
source: str = Field(...)
target: str = Field(...)
label: str = Field(...)
predicate: str = Field(...)
properties: dict[str, Any] = Field(default_factory=dict)
```


## `class SubgraphResponse(BaseModel)`

Response format for graph visualization.

**Fields:**

```python
nodes: list[GraphNode] = Field(...)
edges: list[GraphEdge] = Field(...)
center_id: Optional[str] = None
hops: int = Field(...)
truncated: bool = False
total_entities: int = Field(...)
total_relationships: int = Field(...)
```


### `def _entity_to_node(entity) -> GraphNode`

Convert a storage Entity to a GraphNode.

### `def _relationship_to_edge(rel) -> GraphEdge`

Convert a storage Relationship to a GraphEdge.

### `def extract_subgraph(storage: StorageInterface, center_id: str, hops: int = 2, max_nodes: int = DEFAULT_MAX_NODES) -> SubgraphResponse`

Extract a subgraph centered on a given entity using BFS.

Args:
    storage: Storage interface for querying entities and relationships.
    center_id: The entity ID to center the subgraph on.
    hops: Number of hops (depth) to traverse from center (1-5).
    max_nodes: Maximum number of nodes to include.

Returns:
    SubgraphResponse with nodes, edges, and metadata.

### `def extract_full_graph(storage: StorageInterface, max_nodes: int = DEFAULT_MAX_NODES) -> SubgraphResponse`

Extract the entire graph (up to max_nodes).

Args:
    storage: Storage interface for querying entities and relationships.
    max_nodes: Maximum number of nodes to include.

Returns:
    SubgraphResponse with all nodes and edges (up to limits).


<span id="user-content-kgserverqueryinitpy"></span>

# kgserver/query/__init__.py

Query module for knowledge graph server.


<span id="user-content-kgserverqueryipynbcheckpointsreadme-checkpointmd"></span>

# kgserver/query/.ipynb_checkpoints/README-checkpoint.md

# Medical Knowledge Graph Query Interface

This directory contains tools for querying the medical knowledge graph in a storage-agnostic way.

## Overview

Our storage system is agnostic across:

* SQLite with sqlite-vec
* PostgreSQL with pgvector
* Neo4j

This query interface provides a unified, fluent API that works across all backends, with current support for PostgreSQL and planned support for Neo4j.

## Getting Started

### Interactive Exploration

Use the Jupyter notebook to explore queries interactively:

    ...


<span id="user-content-kgserverqueryroutersgraphapipy"></span>

# kgserver/query/routers/graph_api.py

Graph visualization API router for the Knowledge Graph Server.

Provides endpoints for extracting subgraphs suitable for D3.js force-directed
graph visualization.

## `class SearchResult(BaseModel)`

A single entity search result.

**Fields:**

```python
entity_id: str = Field(...)
name: str = Field(...)
entity_type: str = Field(...)
```


## `class SearchResponse(BaseModel)`

Response from entity search.

**Fields:**

```python
results: list[SearchResult] = Field(...)
total: int = Field(...)
query: str = Field(...)
```



<span id="user-content-kgserverqueryroutersgraphiqlcustompy"></span>

# kgserver/query/routers/graphiql_custom.py

Custom GraphiQL interface with example queries.

Serves a custom GraphiQL HTML page with a dropdown menu of example queries.

### `def create_graphiql_html(graphql_endpoint: str = '/graphql') -> str`

Create custom GraphiQL HTML with example queries dropdown.

Attributes:

    graphql_endpoint: The GraphQL endpoint URL

Returns: HTML string for the GraphiQL interface


<span id="user-content-kgserverqueryroutersrestapipy"></span>

# kgserver/query/routers/rest_api.py

REST API router for the Medical Literature Knowledge Graph.


<span id="user-content-kgserverqueryserverpy"></span>

# kgserver/query/server.py



<span id="user-content-kgserverquerystoragefactorypy"></span>

# kgserver/query/storage_factory.py

Storage factory for creating and managing storage backend instances.

### `def get_engine()`

Returns a singleton instance of the SQLAlchemy engine and db_url.

### `def get_storage() -> Generator[StorageInterface, None, None]`

FastAPI dependency that provides a storage instance with a request-scoped session.

### `def close_storage()`

Closes the engine connection.


<span id="user-content-kgserverstoragebackendsinitpy"></span>

# kgserver/storage/backends/__init__.py

Storage backend implementations.

This package contains concrete implementations of storage interfaces for
different database backends.

Available Backends:

- **sqlite**: SQLite implementation for testing and development
- **postgres**: PostgreSQL+pgvector implementation for production
- **sqlite_entity_collection**: SQLite-based entity collection

Example:

    >>> from storage.backends.sqlite import SQLiteStorage
    >>> storage = SQLiteStorage("my_database.db")

    >>> from storage.backends.postgres import PostgresStorage
    >>> storage = PostgresStorage("postgresql://user:pass@localhost/db")

For detailed backend comparison and usage, see backends/README.md.


<span id="user-content-kgserverstoragebackendspostgrespy"></span>

# kgserver/storage/backends/postgres.py

PostgreSQL implementation of the storage interface.

## `class PostgresStorage(StorageInterface)`

PostgreSQL implementation of the storage interface.

### `def PostgresStorage.load_bundle(self, bundle_manifest: BundleManifestV1, bundle_path: str) -> None`

Load a data bundle into the storage.
This is an idempotent operation. If the bundle is already loaded, it will do nothing.

### `def PostgresStorage._debug_print_sample_entities(self, entities_file: str) -> None`

Print first few entities for debugging.

### `def PostgresStorage._load_entities(self, entities_file: str) -> None`

Load entities from JSONL file.

### `def PostgresStorage._capture_entity_sample(self, entity_data: dict, entity_id: str, status: str) -> dict`

Capture a sample entity for debugging.

### `def PostgresStorage._check_canonical_url_in_props(self, entity_data: dict, entity_id: str, status: str) -> tuple[bool, dict]`

Check if canonical_url exists in properties and return it along with props.

### `def PostgresStorage._print_entity_loading_summary(self, canonical_url_count: int, canonical_entities: int, total_entities: int, sample_canonical_entity: Optional[dict], sample_entity_raw: Optional[dict], sample_with_url: Optional[dict], sample_without_url: Optional[dict]) -> None`

Print summary of entity loading with debug information.

### `def PostgresStorage._print_entity_sample(self, title: str, sample: dict) -> None`

Print a sample entity structure.

### `def PostgresStorage._load_relationships(self, relationships_file: str) -> None`

Load relationships from JSONL file.

### `def PostgresStorage._normalize_entity(self, data: dict) -> dict`

Normalize entity data, flattening metadata fields.

### `def PostgresStorage._normalize_relationship(self, data: dict) -> dict`

Normalize relationship data, mapping field names.

### `def PostgresStorage.is_bundle_loaded(self, bundle_id: str) -> bool`

Check if a bundle with the given ID is already loaded.

### `def PostgresStorage.record_bundle(self, bundle_manifest: BundleManifestV1) -> None`

Record that a bundle has been loaded.

### `def PostgresStorage.get_entity(self, entity_id: str) -> Optional[Entity]`

Get an entity by its ID.

### `def PostgresStorage.get_entities(self, limit: int = 100, offset: int = 0, entity_type: Optional[str] = None, name: Optional[str] = None, name_contains: Optional[str] = None, source: Optional[str] = None, status: Optional[str] = None) -> Sequence[Entity]`

List entities with optional filtering.

### `def PostgresStorage.count_entities(self, entity_type: Optional[str] = None, name: Optional[str] = None, name_contains: Optional[str] = None, source: Optional[str] = None, status: Optional[str] = None) -> int`

Count entities matching filter criteria.

### `def PostgresStorage.find_relationships(self, subject_id: Optional[str] = None, predicate: Optional[str] = None, object_id: Optional[str] = None, limit: Optional[int] = None, offset: Optional[int] = None) -> Sequence[Relationship]`

Find relationships matching criteria.

### `def PostgresStorage.count_relationships(self, subject_id: Optional[str] = None, predicate: Optional[str] = None, object_id: Optional[str] = None) -> int`

Count relationships matching filter criteria.

### `def PostgresStorage.get_relationship(self, subject_id: str, predicate: str, object_id: str) -> Optional[Relationship]`

Get a relationship by its canonical triple (subject_id, predicate, object_id).

### `def PostgresStorage.get_relationships(self, limit: int = 100, offset: int = 0) -> Sequence[Relationship]`

List all relationships.

### `def PostgresStorage.get_bundle_info(self)`

Get bundle metadata (latest bundle).
Returns None if no bundle is loaded.

### `def PostgresStorage.close(self) -> None`

Close connections and clean up resources.


<span id="user-content-kgserverstoragebackendsreadmemd"></span>

# kgserver/storage/backends/README.md

# Storage Backends

This directory contains concrete implementations of the storage interfaces for different database backends.

## Available Backends

### SQLite (`sqlite.py`)

SQLite implementation for development, testing, and small-scale deployments.

**Features:**
- In-memory or file-based storage
- No external dependencies
- Fast for small datasets
- Stores generic `Entity` and `Relationship` models

**Use Cases:**
- Local development
- Automated testing
- CI/CD ingests

    ...


<span id="user-content-kgserverstoragebackendssqlitepy"></span>

# kgserver/storage/backends/sqlite.py

SQLite implementation of the storage interface.

## `class SQLiteStorage(StorageInterface)`

SQLite implementation of the storage interface.

### `def SQLiteStorage.add_entity(self, entity: Entity) -> None`

Add a single entity to the storage.

### `def SQLiteStorage.add_relationship(self, relationship: Relationship) -> None`

Add a single relationship to the storage.

### `def SQLiteStorage.load_bundle(self, bundle_manifest: BundleManifestV1, bundle_path: str) -> None`

Load a data bundle into the storage.
This is an idempotent operation. If the bundle is already loaded, it will do nothing.

### `def SQLiteStorage._normalize_entity(self, data: dict) -> dict`

Normalize entity data, flattening metadata fields.

### `def SQLiteStorage._normalize_relationship(self, data: dict) -> dict`

Normalize relationship data, mapping field names.

### `def SQLiteStorage.is_bundle_loaded(self, bundle_id: str) -> bool`

Check if a bundle with the given ID is already loaded.

### `def SQLiteStorage.record_bundle(self, bundle_manifest: BundleManifestV1) -> None`

Record that a bundle has been loaded.

### `def SQLiteStorage.get_entity(self, entity_id: str) -> Optional[Entity]`

Get an entity by its ID.

### `def SQLiteStorage.get_entities(self, limit: int = 100, offset: int = 0, entity_type: Optional[str] = None, name: Optional[str] = None, name_contains: Optional[str] = None, source: Optional[str] = None, status: Optional[str] = None) -> Sequence[Entity]`

List entities with optional filtering.

### `def SQLiteStorage.count_entities(self, entity_type: Optional[str] = None, name: Optional[str] = None, name_contains: Optional[str] = None, source: Optional[str] = None, status: Optional[str] = None) -> int`

Count entities matching filter criteria.

### `def SQLiteStorage.find_relationships(self, subject_id: Optional[str] = None, predicate: Optional[str] = None, object_id: Optional[str] = None, limit: Optional[int] = None, offset: Optional[int] = None) -> Sequence[Relationship]`

Find relationships matching criteria.

### `def SQLiteStorage.count_relationships(self, subject_id: Optional[str] = None, predicate: Optional[str] = None, object_id: Optional[str] = None) -> int`

Count relationships matching filter criteria.

### `def SQLiteStorage.get_relationship(self, subject_id: str, predicate: str, object_id: str) -> Optional[Relationship]`

Get a relationship by its canonical triple (subject_id, predicate, object_id).

### `def SQLiteStorage.get_relationships(self, limit: int = 100, offset: int = 0) -> Sequence[Relationship]`

List all relationships.

### `def SQLiteStorage.get_bundle_info(self)`

Get bundle metadata (latest bundle).
Returns None if no bundle is loaded.

### `def SQLiteStorage.close(self) -> None`

Close connections and clean up resources.


<span id="user-content-kgserverstorageinitpy"></span>

# kgserver/storage/__init__.py

Storage layer for medical literature knowledge graph.

This package provides a clean abstraction for data persistence, separating
infrastructure concerns from domain logic.

Key Components:

- **interfaces**: Abstract base classes defining storage contracts
- **backends**: Concrete implementations (SQLite, PostgreSQL)
- **models**: SQLModel schemas for database persistence

Example:

    >>> from storage.interfaces import StorageInterface
    >>> from storage.backends.sqlite import SQLiteStorage
    >>>
    >>> # Create an in-memory SQLite storage
    >>> storage = SQLiteStorage(":memory:")
    >>>
    >>> # Add entities, papers, relationships
    >>> storage.entities.add_disease(disease)
    >>> storage.add_paper(paper)
    >>> storage.add_relationship(relationship)

For more information, see the README.md in this directory.


<span id="user-content-kgserverstorageinterfacespy"></span>

# kgserver/storage/interfaces.py

Storage interfaces for the Knowledge Graph Server.

## `class StorageInterface(ABC)`

Abstract interface for a knowledge graph storage backend.

### `def StorageInterface.load_bundle(self, bundle_manifest: BundleManifestV1, bundle_path: str) -> None`

Load a data bundle into the storage.
This should be an idempotent operation.

### `def StorageInterface.is_bundle_loaded(self, bundle_id: str) -> bool`

Check if a bundle with the given ID is already loaded.

### `def StorageInterface.record_bundle(self, bundle_manifest: BundleManifestV1) -> None`

Record that a bundle has been loaded.

### `def StorageInterface.get_entity(self, entity_id: str) -> Optional[Entity]`

Get an entity by its ID.

### `def StorageInterface.get_entities(self, limit: int = 100, offset: int = 0, entity_type: Optional[str] = None, name: Optional[str] = None, name_contains: Optional[str] = None, source: Optional[str] = None, status: Optional[str] = None) -> Sequence[Entity]`

List entities with optional filtering.

### `def StorageInterface.count_entities(self, entity_type: Optional[str] = None, name: Optional[str] = None, name_contains: Optional[str] = None, source: Optional[str] = None, status: Optional[str] = None) -> int`

Count entities matching filter criteria.

### `def StorageInterface.find_relationships(self, subject_id: Optional[str] = None, predicate: Optional[str] = None, object_id: Optional[str] = None, limit: Optional[int] = None, offset: Optional[int] = None) -> Sequence[Relationship]`

Find relationships matching criteria.

### `def StorageInterface.count_relationships(self, subject_id: Optional[str] = None, predicate: Optional[str] = None, object_id: Optional[str] = None) -> int`

Count relationships matching filter criteria.

### `def StorageInterface.get_relationship(self, subject_id: str, predicate: str, object_id: str) -> Optional[Relationship]`

Get a relationship by its canonical triple (subject_id, predicate, object_id).

### `def StorageInterface.get_relationships(self, limit: int = 100, offset: int = 0) -> Sequence[Relationship]`

List all relationships.

### `def StorageInterface.get_bundle_info(self)`

Get bundle metadata (latest bundle).
Returns None if no bundle is loaded.

### `def StorageInterface.close(self) -> None`

Close connections and clean up resources.


<span id="user-content-kgserverstoragemodelsbundlepy"></span>

# kgserver/storage/models/bundle.py

## `class Bundle(SQLModel)`

Represents a loaded data bundle's metadata for idempotent tracking.

**Fields:**

```python
bundle_id: str = Field(...)
domain: str
created_at: datetime
bundle_version: str
```



<span id="user-content-kgserverstoragemodelsentitypy"></span>

# kgserver/storage/models/entity.py

Generic Entity model for the Knowledge Graph Server.

## `class Entity(SQLModel)`

A generic entity in the knowledge graph.

**Fields:**

```python
entity_id: str = Field(...)
entity_type: str = Field(...)
name: Optional[str] = None
status: Optional[str] = None
confidence: Optional[float] = None
usage_count: Optional[int] = None
source: Optional[str] = None
canonical_url: Optional[str] = None
synonyms: List[str] = []
properties: dict[str, Any] = {}
```



<span id="user-content-kgserverstoragemodelsevidencepy"></span>

# kgserver/storage/models/evidence.py

## `class Evidence(SQLModel)`

**Fields:**

```python
id: Optional[int] = None
relationship_id: int = Field(...)
paper_id: int = Field(...)
evidence_type: str = Field(...)
confidence_score: Optional[float] = None
metadata_: Dict[str, Any] = Field(default_factory=dict)
created_at: datetime = Field(...)
```



<span id="user-content-kgserverstoragemodelsinitpy"></span>

# kgserver/storage/models/__init__.py

SQLModel schemas for database persistence.


<span id="user-content-kgserverstoragemodelspaperpy"></span>

# kgserver/storage/models/paper.py

## `class Paper(SQLModel)`

**Fields:**

```python
id: str = Field(...)
title: str = Field(...)
authors: Optional[str] = None
abstract: Optional[str] = None
publication_date: Optional[str] = None
journal: Optional[str] = None
doi: Optional[str] = None
pubmed_id: Optional[str] = None
entity_count: int = 0
relationship_count: int = 0
extraction_provenance_json: Optional[dict] = None
metadata_json: Optional[dict] = None
created_at: datetime = Field(default_factory=datetime.utcnow)
updated_at: datetime = Field(default_factory=datetime.utcnow)
```



<span id="user-content-kgserverstoragemodelsreadmemd"></span>

# kgserver/storage/models/README.md

# Storage Models (SQLModel Schemas)

This directory contains SQLModel persistence schemas that define the database structure for storing knowledge graphs.

## Overview

These are **Persistence Models** - flattened database representations optimized for storage and querying.

## Available Models

### Entity (`entity.py`)

Single table with primary key `entity_id`.

**Key Fields:**
```python
entity_id: str              # Primary key
entity_type: str            # Type of the entity
name: Optional[str]         # Entity name
synonyms: List[str]         # List of synonyms
```

    ...


<span id="user-content-kgserverstoragemodelsrelationshippy"></span>

# kgserver/storage/models/relationship.py

Generic Relationship model for the Knowledge Graph Server.

## `class Relationship(SQLModel)`

A generic relationship in the knowledge graph.

**Fields:**

```python
id: UUID = Field(default_factory=uuid4)
subject_id: str = Field(...)
predicate: str = Field(...)
object_id: str = Field(...)
confidence: Optional[float] = None
source_documents: List[str] = []
properties: dict[str, Any] = {}
```



<span id="user-content-kgserverstorageneo4jcompatibilitymd"></span>

# kgserver/storage/NEO4J_COMPATIBILITY.md

# Neo4j Compatibility

> **Note**: This document describes the theoretical Neo4j compatibility of the storage interfaces. **Neo4j is not currently implemented or planned for this project.** The current implementations use PostgreSQL for production and SQLite for testing. This document is preserved for architectural reference only.

## 1. Introduction

The storage interfaces are designed to be **storage-agnostic**, enabling the knowledge graph server to be backed by various persistence technologies. While the current implementations target SQLite (for testing) and PostgreSQL (for production), the abstract interface design is suitable for graph database backends like **Neo4j**.

The interfaces define operations in terms of entities and relationships rather than tables and SQL, making them naturally compatible with graph database concepts.

## 2. Current Interface Design

The server uses a single abstract `StorageInterface` defined in `storage/interfaces.py`:

### StorageInterface
Combines all storage operations into a unified abstraction.

**Key methods:**
- `load_bundle(bundle_manifest, bundle_path)`: Load a data bundle into storage.
- `get_entity(entity_id)`: Retrieve an entity by ID.

    ...


<span id="user-content-kgserverstoragereadmemd"></span>

# kgserver/storage/README.md

# Storage Layer

The storage layer provides a clean abstraction for data persistence in the knowledge graph, separating infrastructure concerns (database operations) from domain logic (data processing).

## Architecture

The storage layer is organized into three main components:

```
storage/
├── interfaces.py           # Abstract base class defining storage contract (`StorageInterface`)
├── backends/              # Concrete backend implementations
│   ├── sqlite.py          # SQLite implementation for development/testing
│   └── postgres.py        # PostgreSQL for production
└── models/                # SQLModel persistence schemas
    ├── entity.py          # Generic Entity table
    └── relationship.py    # Generic Relationship table
```

## Key Concepts

    ...


<span id="user-content-kgservertestsconftestpy"></span>

# kgserver/tests/conftest.py

Pytest configuration and shared fixtures for GraphQL tests.

### `def in_memory_storage()`

Create an in-memory SQLite storage for testing.

### `def sample_entities()`

Create sample entities for testing.

### `def sample_relationships()`

Create sample relationships for testing.

### `def populated_storage(in_memory_storage, sample_entities, sample_relationships)`

Create storage with sample data.

### `def graphql_context(populated_storage)`

Create GraphQL context with populated storage.

### `def graphql_schema()`

Create GraphQL schema for testing.

### `def sample_bundle()`

Create a sample bundle for testing.

### `def storage_with_bundle(populated_storage, sample_bundle)`

Create storage with bundle metadata.


<span id="user-content-kgserverteststestbundleloaderpy"></span>

# kgserver/tests/test_bundle_loader.py

Tests for query/bundle_loader.py bundle loading logic.

Tests cover:
- _find_manifest() function
- Bundle loading from directory
- Bundle loading from ZIP
- Bundle-specific graphql_examples.yml override
- Error handling

### `def sample_manifest_data()`

Create sample manifest data.

### `def bundle_directory(sample_manifest_data, tmp_path)`

Create a temporary bundle directory with manifest and data files.

### `def bundle_zip(bundle_directory, tmp_path)`

Create a ZIP file from bundle directory.

### `def test_engine()`

Create a test SQLAlchemy engine.

## `class TestLoadFromDirectory`

Test _load_from_directory() function.

### `def TestLoadFromDirectory.test_load_from_directory_success(self, bundle_directory, test_engine)`

Test successfully loading bundle from directory.

### `def TestLoadFromDirectory.test_load_from_directory_no_manifest(self, tmp_path, test_engine)`

Test loading from directory without manifest.

## `class TestLoadFromZip`

Test _load_from_zip() function.

### `def TestLoadFromZip.test_load_from_zip_success(self, bundle_zip, test_engine)`

Test successfully loading bundle from ZIP.

### `def TestLoadFromZip.test_load_from_zip_no_manifest(self, tmp_path, test_engine)`

Test loading ZIP without manifest.

## `class TestBundleGraphqlExamples`

Test that a bundle's graphql_examples.yml replaces the default examples.

### `def TestBundleGraphqlExamples.test_bundle_examples_override(self, bundle_directory, test_engine)`

When a bundle contains graphql_examples.yml, those examples
should replace the built-in defaults after loading.

### `def TestBundleGraphqlExamples.test_no_bundle_examples_keeps_defaults(self, bundle_directory, test_engine)`

When a bundle does NOT contain graphql_examples.yml, the built-in
defaults should remain unchanged.


<span id="user-content-kgserverteststestgraphapipy"></span>

# kgserver/tests/test_graph_api.py

Tests for graph visualization API and traversal logic.

## `class TestGraphTraversal`

Tests for BFS graph traversal logic.

### `def TestGraphTraversal.test_extract_subgraph_single_hop(self, populated_storage)`

Test extracting a subgraph with 1 hop from center.

### `def TestGraphTraversal.test_extract_subgraph_two_hops(self, populated_storage)`

Test extracting a subgraph with 2 hops.

### `def TestGraphTraversal.test_extract_subgraph_nonexistent_center(self, populated_storage)`

Test extracting subgraph with non-existent center returns empty.

### `def TestGraphTraversal.test_extract_subgraph_respects_max_nodes(self, populated_storage)`

Test that max_nodes limit is respected.

### `def TestGraphTraversal.test_extract_full_graph(self, populated_storage)`

Test extracting the full graph.

### `def TestGraphTraversal.test_graph_node_structure(self, populated_storage)`

Test that GraphNode has correct structure.

### `def TestGraphTraversal.test_graph_edge_structure(self, populated_storage)`

Test that GraphEdge has correct structure.

## `class TestGraphAPI`

Tests for graph visualization REST API.

### `def TestGraphAPI.app(self)`

Create FastAPI app with Graph API router.

### `def TestGraphAPI.file_storage(self, tmp_path, sample_entities, sample_relationships)`

Create SQLite storage for thread-safe testing.

### `def TestGraphAPI.client(self, app, file_storage)`

Create test client with storage dependency override.

### `def TestGraphAPI.test_get_subgraph_with_center(self, client)`

Test GET /api/v1/graph/subgraph with center_id.

### `def TestGraphAPI.test_get_subgraph_include_all(self, client)`

Test GET /api/v1/graph/subgraph with include_all=true.

### `def TestGraphAPI.test_get_subgraph_missing_center_id(self, client)`

Test that missing center_id returns 400 when include_all is false.

### `def TestGraphAPI.test_get_node_details(self, client)`

Test GET /api/v1/graph/node/{entity_id}.

### `def TestGraphAPI.test_get_node_details_not_found(self, client)`

Test GET /api/v1/graph/node with non-existent entity.

### `def TestGraphAPI.test_get_edge_details(self, client)`

Test GET /api/v1/graph/edge.

### `def TestGraphAPI.test_get_edge_details_not_found(self, client)`

Test GET /api/v1/graph/edge with non-existent relationship.

### `def TestGraphAPI.test_hops_parameter_validation(self, client)`

Test that hops parameter is validated.

### `def TestGraphAPI.test_max_nodes_parameter(self, client)`

Test max_nodes parameter.

### `def TestGraphAPI.test_search_entities(self, client)`

Test GET /api/v1/graph/search.

### `def TestGraphAPI.test_search_entities_no_results(self, client)`

Test search with no matching entities.

### `def TestGraphAPI.test_search_entities_with_type_filter(self, client)`

Test search with entity_type filter.


<span id="user-content-kgserverteststestgraphqlschemapy"></span>

# kgserver/tests/test_graphql_schema.py

Tests for GraphQL schema queries and types.

Tests cover:
- Entity queries (by ID, list with pagination and filtering)
- Relationship queries (by triple, list with pagination and filtering)
- Bundle introspection query
- Pagination types and metadata
- Filter functionality
- Max limit enforcement

### `def execute_query(schema, query: str, context: dict)`

Helper to execute a GraphQL query.

## `class TestEntityQueries`

Test entity-related GraphQL queries.

### `def TestEntityQueries.test_entity_by_id(self, graphql_schema, graphql_context)`

Test retrieving a single entity by ID.

### `def TestEntityQueries.test_entity_not_found(self, graphql_schema, graphql_context)`

Test querying for non-existent entity.

### `def TestEntityQueries.test_entities_pagination(self, graphql_schema, graphql_context)`

Test entities query with pagination.

### `def TestEntityQueries.test_entities_pagination_offset(self, graphql_schema, graphql_context)`

Test entities query with offset.

### `def TestEntityQueries.test_entities_filter_by_type(self, graphql_schema, graphql_context)`

Test filtering entities by entity type.

### `def TestEntityQueries.test_entities_filter_by_name(self, graphql_schema, graphql_context)`

Test filtering entities by exact name.

### `def TestEntityQueries.test_entities_filter_name_contains(self, graphql_schema, graphql_context)`

Test filtering entities by name containing string.

### `def TestEntityQueries.test_entities_filter_by_source(self, graphql_schema, graphql_context)`

Test filtering entities by source.

### `def TestEntityQueries.test_entities_filter_by_status(self, graphql_schema, graphql_context)`

Test filtering entities by status.

### `def TestEntityQueries.test_entities_filter_combined(self, graphql_schema, graphql_context)`

Test combining multiple filters.

### `def TestEntityQueries.test_entities_max_limit_enforcement(self, graphql_schema, graphql_context, monkeypatch)`

Test that max limit is enforced.

## `class TestRelationshipQueries`

Test relationship-related GraphQL queries.

### `def TestRelationshipQueries.test_relationship_by_triple(self, graphql_schema, graphql_context)`

Test retrieving a single relationship by triple.

### `def TestRelationshipQueries.test_relationship_not_found(self, graphql_schema, graphql_context)`

Test querying for non-existent relationship.

### `def TestRelationshipQueries.test_relationships_pagination(self, graphql_schema, graphql_context)`

Test relationships query with pagination.

### `def TestRelationshipQueries.test_relationships_filter_by_subject(self, graphql_schema, graphql_context)`

Test filtering relationships by subject ID.

### `def TestRelationshipQueries.test_relationships_filter_by_object(self, graphql_schema, graphql_context)`

Test filtering relationships by object ID.

### `def TestRelationshipQueries.test_relationships_filter_by_predicate(self, graphql_schema, graphql_context)`

Test filtering relationships by predicate.

### `def TestRelationshipQueries.test_relationships_filter_combined(self, graphql_schema, graphql_context)`

Test combining multiple relationship filters.

### `def TestRelationshipQueries.test_relationships_max_limit_enforcement(self, graphql_schema, graphql_context, monkeypatch)`

Test that max limit is enforced for relationships.

## `class TestBundleQuery`

Test bundle introspection query.

### `def TestBundleQuery.test_bundle_query(self, graphql_schema, storage_with_bundle)`

Test bundle introspection query.

### `def TestBundleQuery.test_bundle_query_no_bundle(self, graphql_schema, graphql_context)`

Test bundle query when no bundle is loaded.

## `class TestFieldNaming`

Test that GraphQL field names use camelCase.

### `def TestFieldNaming.test_entity_camelcase_fields(self, graphql_schema, graphql_context)`

Test that entity fields are camelCase in GraphQL.

### `def TestFieldNaming.test_relationship_camelcase_fields(self, graphql_schema, graphql_context)`

Test that relationship fields are camelCase in GraphQL.

### `def TestFieldNaming.test_relationship_no_id_field(self, graphql_schema, graphql_context)`

Test that relationship id field is not exposed in GraphQL.

## `class TestPaginationMetadata`

Test pagination metadata correctness.

### `def TestPaginationMetadata.test_entities_pagination_metadata(self, graphql_schema, graphql_context)`

Test that pagination metadata is correct.

### `def TestPaginationMetadata.test_relationships_pagination_metadata(self, graphql_schema, graphql_context)`

Test that relationship pagination metadata is correct.


<span id="user-content-kgserverteststestrestapipy"></span>

# kgserver/tests/test_rest_api.py

Tests for query/routers/rest_api.py REST API endpoints.

Tests cover:
- GET /api/v1/entities/{entity_id}
- GET /api/v1/entities
- GET /api/v1/relationships

### `def app()`

Create FastAPI app with REST API router.

### `def file_storage(tmp_path, sample_entities, sample_relationships)`

Create SQLite storage for thread-safe testing with FastAPI TestClient.

Uses tmp_path which automatically cleans up files after tests.
Uses check_same_thread=False for TestClient threading compatibility.

### `def client(app, file_storage)`

Create test client with storage dependency override.

## `class TestGetEntityById`

Test GET /api/v1/entities/{entity_id} endpoint.

### `def TestGetEntityById.test_get_existing_entity(self, client)`

Test retrieving an existing entity.

### `def TestGetEntityById.test_get_nonexistent_entity(self, client)`

Test retrieving a non-existent entity returns 404.

## `class TestListEntities`

Test GET /api/v1/entities endpoint.

### `def TestListEntities.test_list_entities_default(self, client)`

Test listing entities with default parameters.

### `def TestListEntities.test_list_entities_with_limit(self, client)`

Test listing entities with limit.

### `def TestListEntities.test_list_entities_with_offset(self, client)`

Test listing entities with offset.

### `def TestListEntities.test_list_entities_empty_result(self, client)`

Test listing entities with offset beyond available.

## `class TestFindRelationships`

Test GET /api/v1/relationships endpoint.

### `def TestFindRelationships.test_find_all_relationships(self, client)`

Test finding all relationships.

### `def TestFindRelationships.test_find_relationships_by_subject(self, client)`

Test filtering relationships by subject_id.

### `def TestFindRelationships.test_find_relationships_by_object(self, client)`

Test filtering relationships by object_id.

### `def TestFindRelationships.test_find_relationships_by_predicate(self, client)`

Test filtering relationships by predicate.

### `def TestFindRelationships.test_find_relationships_combined_filters(self, client)`

Test filtering relationships with multiple filters.

### `def TestFindRelationships.test_find_relationships_with_limit(self, client)`

Test limiting relationship results.

### `def TestFindRelationships.test_find_relationships_no_matches(self, client)`

Test finding relationships with no matches.


<span id="user-content-kgserverteststeststoragebackendspy"></span>

# kgserver/tests/test_storage_backends.py

Direct tests for storage backend implementations.

Tests cover:
- SQLiteStorage direct operations
- PostgresStorage direct operations (when available)
- Filter combinations
- Edge cases

## `class TestSQLiteStorage`

Direct tests for SQLiteStorage.

### `def TestSQLiteStorage.test_get_entity_existing(self, in_memory_storage, sample_entities)`

Test retrieving an existing entity.

### `def TestSQLiteStorage.test_get_entity_nonexistent(self, in_memory_storage)`

Test retrieving non-existent entity.

### `def TestSQLiteStorage.test_get_entities_with_filters(self, in_memory_storage, sample_entities)`

Test get_entities with various filters.

### `def TestSQLiteStorage.test_count_entities(self, in_memory_storage, sample_entities)`

Test count_entities.

### `def TestSQLiteStorage.test_find_relationships_with_filters(self, in_memory_storage, sample_entities, sample_relationships)`

Test find_relationships with filters.

### `def TestSQLiteStorage.test_count_relationships(self, in_memory_storage, sample_entities, sample_relationships)`

Test count_relationships.

### `def TestSQLiteStorage.test_get_bundle_info(self, in_memory_storage)`

Test get_bundle_info.

### `def TestSQLiteStorage.test_get_bundle_info_none(self, in_memory_storage)`

Test get_bundle_info when no bundle exists.

### `def TestSQLiteStorage.test_is_bundle_loaded(self, in_memory_storage)`

Test is_bundle_loaded.

## `class TestPostgresStorage`

Direct tests for PostgresStorage using mocked database.

### `def TestPostgresStorage.postgres_storage(self)`

Create PostgresStorage using in-memory SQLite (mocks PostgreSQL).

### `def TestPostgresStorage.test_postgres_storage_basic(self, postgres_storage, sample_entities)`

Test basic PostgresStorage operations.

### `def TestPostgresStorage.test_postgres_storage_filters(self, postgres_storage, sample_entities)`

Test PostgresStorage with filters.


<span id="user-content-kgserverteststeststoragefactorypy"></span>

# kgserver/tests/test_storage_factory.py

Tests for query/storage_factory.py storage factory logic.

Tests cover:
- get_engine() with different DATABASE_URL values
- get_storage() for PostgreSQL vs SQLite
- Error handling for unsupported schemes

## `class TestGetEngine`

Test get_engine() function.

### `def TestGetEngine.test_get_engine_with_sqlite_url(self, monkeypatch)`

Test get_engine with SQLite URL.

### `def TestGetEngine.test_get_engine_with_postgres_url(self, monkeypatch)`

Test get_engine with PostgreSQL URL.

### `def TestGetEngine.test_get_engine_defaults_to_sqlite(self, monkeypatch)`

Test that get_engine defaults to SQLite when DATABASE_URL not set.

### `def TestGetEngine.test_get_engine_singleton(self, monkeypatch)`

Test that get_engine returns the same engine instance.

## `class TestGetStorage`

Test get_storage() function.

### `def TestGetStorage.test_get_storage_sqlite(self, monkeypatch)`

Test get_storage with SQLite.

### `def TestGetStorage.test_get_storage_postgres(self, monkeypatch)`

Test get_storage with PostgreSQL.

### `def TestGetStorage.test_get_storage_unsupported_scheme(self, monkeypatch)`

Test get_storage with unsupported database scheme.

### `def TestGetStorage.test_get_storage_sqlite_file_path(self, monkeypatch)`

Test get_storage with SQLite file path.

## `class TestCloseStorage`

Test close_storage() function.

### `def TestCloseStorage.test_close_storage(self, monkeypatch)`

Test that close_storage disposes the engine.

### `def TestCloseStorage.test_close_storage_when_none(self)`

Test that close_storage handles None engine gracefully.


<span id="user-content-lambdalabsmd"></span>

# LAMBDA_LABS.md

# GPU-Accelerated AI Tinkering

Like me, you may get tired of paying subscription fees to use online LLMs.
Especially when, later, you're told that you've reached the usage limit and you
should "switch to another model" or some such nonsense. The tempation at that
point is to run a model locally using Ollama, but your local machine probably
doesn't have a GPU if you're not a gamer. Then you dream of picking up a cheap
GPU box on eBay and running it locally, and that's not a bad idea but it takes
time and money that you may not want to spend right now.

There is an alternative, services like Lambda Labs, RunPod, and others. Lambda
Labs is what I got when I threw a dart at a dartboard, so I'll be using it
here.

I'm using a LLM to translate medical papers into a graph database of entities
and relationships. I set up GPU-accelerated paper ingestion using Lambda Labs,
and got an **enormous speedup** over CPU-only. The quick turnaround made it
practical to find and fix some bugs discovered during testing.

## GPU Instance Setup

    ...


<span id="user-content-lintsh"></span>

# lint.sh

```sh
#!/bin/bash -e

fixes_needed() {
    echo "Something needs fixing, trying to fix it"
    set -x
    uv run black kgraph kgbundle kgserver
    uv run ruff check --fix kgraph kgbundle kgserver
    exit 1
}

mypy_fix_needed() {
    echo "Something mypy-ish needs fixing, You need to do that"
    exit 1
}

echo "=========================================="
echo "Running Linters and Tests"
echo "=========================================="

# Ensure uv is available

    ...
```


<span id="user-content-medlitbundledocsreadmemd"></span>

# medlit_bundle/docs/README.md

# Medical Literature Knowledge Graph Bundle

This bundle contains extracted knowledge from biomedical journal articles.

## Statistics

- Papers processed: 1
- Total entities: 31
- Total relationships: 19

## Domain

- Domain: medlit
- Entity types: disease, gene, drug, protein, symptom, procedure, biomarker, pathway
- Relationship types: treats, causes, increases_risk, associated_with, interacts_with, etc.

## Source

Papers were processed from JSON format (med-lit-schema Paper format).

    ...


<span id="user-content-medlitbundlegraphqlexamplesyml"></span>

# medlit_bundle/graphql_examples.yml

```yaml
Get Entity by ID: |
  # Retrieve a specific entity by its ID
  query GetEntity {
    entity(id: "holmes:char:JohnWatson") {
      entityId
      name
      entityType
      synonyms
      properties
    }
  }

Search Entities: |
  # Search for entities with pagination
  query SearchEntities {
    entities(limit: 5, offset: 0) {
      items {
        entityId
        name
        entityType

    ...
```


<span id="user-content-medlitschemaspecmd"></span>

# MEDLIT_SCHEMA_SPEC.md

# Work Order: MedLit as an Extension of kgschema/

## Executive Summary

**Goal:** Create `examples/medlit_schema/` as a definitions-only package demonstrating domain-specific extension of the kgraph framework for medical literature knowledge graphs.

**Key Design Decisions:**
1. ✅ **Evidence as first-class entity** (not relationship metadata) for database indexing, canonical IDs, and multi-hop traceability
2. ✅ **Canonical ID format for Evidence:** `{paper_id}:{section}:{paragraph}:{method}` enables immediate promotion
3. ✅ **Rich bibliographic model** from `med-lit-schema/`: Paper with authors, journal, study metadata, MeSH terms, extraction provenance
4. ✅ **Mandatory evidence for medical relationships:** Pydantic validation prevents evidence-free assertions
5. ✅ **Ontology integration:** UMLS, HGNC, RxNorm, UniProt for entities; ECO, OBI, STATO for evidence classification

**Traceability Requirement:** Users must be able to trace from any claim back to source papers with minimal hops and maximum clarity.

**Navigation path:** `Relationship → Evidence → TextSpan → Paper` (all first-class entities, all indexed)

## Architecture Parallel

This plan creates `examples/medlit_schema/` as a **definitions-only** package that mirrors `kgschema/` structure, but deepened for medical literature. The parallel is:

    ...


<span id="user-content-nextstepsmd"></span>

# NEXT_STEPS.md

# What are Next Steps for this thing?

Following a bunch of work, I have this little review from ChatGPT.

## Small suggestions / potential follow-ups ⚠️ (not blockers)

1.  **`ValidationIssue.value` is typed as `str | None`**. [GitHub](https://github.com/wware/kgraph/commit/7c3d5778045027f79cda39110bd09819dc05253e)  
    That’s fine for display, but you may eventually want either:
    
    -   `value: Any | None` (so callers can inspect programmatically), *or*
        
    -   keep `value` as `str` but add `raw_value_type: str | None` (or similar) for better debugging.  
        Right now you’re committing to “value is pre-stringified,” which is okay, just a design choice to be aware of.
        
2.  Consider whether you want the same structured approach for **`validate_relationship`** eventually. The docs snippet you updated only mentions `validate_entity`’s return change; `validate_relationship` appears to still be a bool in the docs list. [GitHub+1](https://github.com/wware/kgraph/commit/7c3d5778045027f79cda39110bd09819dc05253e)  
    Totally fine to do entity first, relationships later—just flagging symmetry.
    
3.  The `field` attribute is a single string. [GitHub](https://github.com/wware/kgraph/commit/7c3d5778045027f79cda39110bd09819dc05253e)  
    That works, but if you later want nested/structured locations (like Pydantic’s `loc` tuples), you might want `field_path: tuple[str, ...]` (or `list[str]`) instead. You can defer this until you feel the pain.

    ...


<span id="user-content-pushmainsh"></span>

# push_main.sh

```sh
#!/bin/bash -e

fixes_needed() {
    echo "Something needs fixing, try this:"
    echo "uv run ruff check --fix . && uv run black ."
    echo 'sed -i "s/ \+$//" $(git ls-files | grep -E "\.py$")'
    exit 1
}

echo "=========================================="
echo "Running Linters and Tests"
echo "=========================================="

# Ensure uv is available
if ! command -v uv &> /dev/null; then
    echo "Error: uv not found. Please install uv first."
    echo "See: https://docs.astral.sh/uv/getting-started/installation/"
    exit 1
fi

    ...
```


<span id="user-content-readmemd"></span>

# README.md

# kgraph

A domain-agnostic framework for building knowledge graphs from documents. Supports entity extraction, relationship mapping, and a two-pass ingestion pipeline that works across any knowledge domain (medical, legal, academic, etc.).

## Features

- **Domain-agnostic**: Define your own entity types, relationships, and validation rules
- **Two-pass ingestion**: Extract entities first, then relationships between them
- **Entity lifecycle**: Provisional entities promoted to canonical based on usage/confidence
- **Canonical ID system**: Abstractions for working with authoritative identifiers (UMLS, MeSH, HGNC, etc.)
- **Embedding support**: Semantic similarity for entity matching and duplicate detection
- **Async-first**: All storage and pipeline interfaces use async/await
- **Immutable models**: Thread-safe Pydantic models with frozen=True

## Installation

```bash
# Clone the repository
git clone https://github.com/wware/kgraph.git
cd kgraph
```

    ...


<span id="user-content-run-ingestsh"></span>

# run-ingest.sh

```sh
#!/bin/bash -xe

# PAPER="PMC12771675.xml"
# PAPER="PMC12756687.xml"
PAPER="PMC12757875.xml,PMC12784210.xml,PMC12784773.xml,PMC12788344.xml,PMC12780394.xml,PMC12757429.xml,PMC12784249.xml,PMC12764803.xml,PMC12783088.xml,PMC12775561.xml,PMC12766194.xml,PMC12750049.xml,PMC12758042.xml,PMC12780067.xml,PMC12785246.xml,PMC12785631.xml,PMC12753587.xml,PMC12754092.xml,PMC12764813.xml,PMC5487382.xml"
TIMEOUT=1000

uv run python -m examples.medlit.scripts.ingest \
  --input-dir examples/medlit/pmc_xmls \
  --input-papers $PAPER \
  --output-dir medlit_bundle \
  --ollama-timeout $TIMEOUT \
  --use-ollama --trace-all --debug

    ...
```


<span id="user-content-snapshotsemanticsv1md"></span>

# snapshot_semantics_v1.md

# Snapshot Semantics (v1 Draft)

## Purpose

This document defines the lifecycle boundaries for building a knowledge
graph snapshot. It clarifies when different identifiers become frozen
and what invariants a snapshot guarantees.

------------------------------------------------------------------------

## Identifier Categories

### 1. Intrinsic (Span/Addressing) Identifiers

These are frozen immediately during parsing and never change within a
snapshot:

-   `document_id`
-   Section / paragraph / sentence indices
-   Character offsets (`start_offset`, `end_offset`)

    ...


<span id="user-content-status20260212md"></span>

# STATUS_20260212.md

# Session status — 2026-02-12

Record of discussion and changes from this session (help.md follow-up, evidence/type enforcement, full-paper streaming extraction, Ollama/GPU).

---

## 1. Help.md to-dos

**Source:** `help.md` — next-step items from a prior review.

- **Item 1 — Type-constraint enforcement:** Enforce type constraints *before* emitting `final_relationships` and record in the relationship trace: `accepted: false`, `drop_reason: type_constraint_mismatch`.
- **Item 2 — Evidence enforcement:** Normalize evidence and subject/object names; require that *both* entity strings (or synonym matches) appear in the evidence; drop and record reason otherwise.
- **Item 3 — Doc-specific predicate filtering:** (Not implemented.) Only offer predicates valid for entity types in the document.

---

## 2. Type-constraint enforcement (item 1)

**Done:**

    ...


<span id="user-content-testsconftestpy"></span>

# tests/conftest.py

Test fixtures and minimal test domain implementation.

This module provides:
- Minimal concrete implementations of abstract base classes (SimpleEntity,
  SimpleDocument, SimpleRelationship, SimpleDomainSchema) for use in unit tests
- Mock implementations of pipeline interfaces (document parsing, entity
  extraction/resolution, relationship extraction, embedding generation)
- Pytest fixtures that instantiate in-memory storage and mock components
- Helper factory functions for creating test entities and relationships

The test domain uses a simple convention where entities are denoted by
square brackets in document text (e.g., "[aspirin]" becomes an entity).

## `class SimpleEntity(BaseEntity)`

Minimal concrete entity implementation for unit tests.

Provides a single entity type ("test_entity") with configurable type
via the entity_type field. Inherits all standard entity fields from
BaseEntity (entity_id, name, status, confidence, usage_count, etc.).

**Fields:**

```python
entity_type: str = 'test_entity'
```


### `def SimpleEntity.get_entity_type(self) -> str`

Return the entity's type identifier.

## `class SimpleRelationship(BaseRelationship)`

Minimal concrete relationship implementation for unit tests.

Uses the predicate field directly as the edge type. Supports any
predicate string, allowing flexible relationship testing without
schema constraints.

### `def SimpleRelationship.get_edge_type(self) -> str`

Return the relationship's edge type (same as predicate).

## `class SimpleDocument(BaseDocument)`

Minimal concrete document implementation for unit tests.

Represents documents as a single "body" section containing the full
content. The document type defaults to "test_document" but can be
customized for tests requiring multiple document types.

### `def SimpleDocument.get_document_type(self) -> str`

Return the document's type identifier.

### `def SimpleDocument.get_sections(self) -> list[tuple[str, str]]`

Return document sections as (section_name, content) tuples.

For SimpleDocument, returns a single "body" section with full content.

## `class SimpleDomainSchema(DomainSchema)`

Minimal domain schema defining types and validation for the test domain.

Configures:
- One entity type: "test_entity"
- Two relationship types: "related_to", "causes"
- One document type: "test_document"
- Lenient promotion config: 2 usages, 0.7 confidence, no embedding required

This schema is intentionally simple to allow straightforward testing
of domain-agnostic graph operations without real-world complexity.

### `def SimpleDomainSchema.name(self) -> str`

Return the domain name identifier.

### `def SimpleDomainSchema.entity_types(self) -> dict[str, type[BaseEntity]]`

Return mapping of entity type names to their classes.

### `def SimpleDomainSchema.relationship_types(self) -> dict[str, type[BaseRelationship]]`

Return mapping of relationship type names to their classes.

### `def SimpleDomainSchema.predicate_constraints(self) -> dict[str, PredicateConstraint]`

Define predicate constraints for the test domain.

For simplicity in testing, we'll allow all relationships to be valid
between 'test_entity' types by default, but this can be overridden
in specific tests if needed.

### `def SimpleDomainSchema.document_types(self) -> dict[str, type[BaseDocument]]`

Return mapping of document type names to their classes.

### `def SimpleDomainSchema.promotion_config(self) -> PromotionConfig`

Return configuration for promoting provisional entities to canonical.

Uses lenient thresholds suitable for testing: requires only 2 usages
and 0.7 confidence, with no embedding requirement.

### `def SimpleDomainSchema.validate_entity(self, entity: BaseEntity) -> list[ValidationIssue]`

Check if the entity's type is registered in this schema.

## `class MockEmbeddingGenerator(EmbeddingGeneratorInterface)`

Mock embedding generator producing deterministic hash-based embeddings.

Generates fixed-dimension vectors derived from the text's hash, ensuring
identical text always produces identical embeddings. Useful for testing
embedding-dependent logic (similarity search, merge detection) without
requiring a real embedding model.

Args:
    dim: Embedding vector dimension (default: 8).

### `def MockEmbeddingGenerator.dimension(self) -> int`

Return the embedding vector dimension.

## `class MockDocumentParser(DocumentParserInterface)`

Mock document parser that wraps raw bytes in a SimpleDocument.

Decodes raw content as UTF-8 text and creates a SimpleDocument with
a randomly generated document_id. Does not perform any actual parsing
or structure extraction.

## `class MockEntityExtractor(EntityExtractorInterface)`

Mock entity extractor using bracket notation for entity detection.

Extracts entities from document text by finding text enclosed in
square brackets. For example, the text "Patient took [aspirin] for
[headache]" yields two EntityMention objects for "aspirin" and
"headache", each with 0.9 confidence and "test_entity" type.

This simple convention allows tests to precisely control which
entities are extracted without needing NLP or ML components.

## `class MockEntityResolver(EntityResolverInterface)`

Mock entity resolver that links mentions to entities via name matching.

Resolution strategy:
1. Search existing storage for an entity with matching name and type
2. If found, return existing entity with 0.95 confidence
3. If not found, create a new provisional SimpleEntity with the
   mention's confidence score

This simple name-based matching is sufficient for testing entity
resolution and promotion logic without external knowledge bases.

## `class MockRelationshipExtractor(RelationshipExtractorInterface)`

Mock relationship extractor that chains adjacent entities.

Creates "related_to" relationships between consecutively ordered
entities. For a document with entities [A, B, C], produces edges
A->B and B->C, each with 0.8 confidence.

This simple linear chaining allows tests to verify relationship
storage and traversal without complex extraction logic.

### `def test_domain() -> SimpleDomainSchema`

Provide a SimpleDomainSchema instance for domain-aware tests.

The schema defines entity types, relationship types, and promotion
configuration suitable for unit testing graph operations.

### `def entity_storage() -> InMemoryEntityStorage`

Provide a fresh in-memory entity storage instance.

Each test receives an empty storage, ensuring test isolation.

### `def relationship_storage() -> InMemoryRelationshipStorage`

Provide a fresh in-memory relationship storage instance.

Each test receives an empty storage, ensuring test isolation.

### `def document_storage() -> InMemoryDocumentStorage`

Provide a fresh in-memory document storage instance.

Each test receives an empty storage, ensuring test isolation.

### `def embedding_generator() -> MockEmbeddingGenerator`

Provide a MockEmbeddingGenerator with default 8-dimensional vectors.

Generates deterministic embeddings based on text hash for reproducible
similarity comparisons in tests.

### `def document_parser() -> MockDocumentParser`

Provide a MockDocumentParser for converting raw bytes to SimpleDocument.

Decodes content as UTF-8 without performing actual parsing or
structure extraction.

### `def entity_extractor() -> MockEntityExtractor`

Provide a MockEntityExtractor using bracket notation.

Extracts entities from text enclosed in square brackets (e.g.,
"[aspirin]" becomes an entity mention).

### `def entity_resolver() -> MockEntityResolver`

Provide a MockEntityResolver for name-based entity matching.

Links mentions to existing entities by name or creates new
provisional entities when no match is found.

### `def relationship_extractor() -> MockRelationshipExtractor`

Provide a MockRelationshipExtractor for linear entity chaining.

Creates "related_to" relationships between consecutively ordered
entities in a document.

### `def make_test_entity(name: str, status: EntityStatus = EntityStatus.PROVISIONAL, entity_id: str | None = None, usage_count: int = 0, confidence: float = 1.0, embedding: tuple[float, ...] | None = None, canonical_ids: dict[str, str] | None = None) -> SimpleEntity`

Factory function to create SimpleEntity instances with sensible defaults.

Provides a concise way to create entities in tests without specifying
all required fields. Generates a random UUID if entity_id is not provided.

Args:
    name: Display name for the entity (required).
    status: Entity lifecycle status (default: PROVISIONAL).
    entity_id: Unique identifier (default: auto-generated UUID).
    usage_count: Number of document references (default: 0).
    confidence: Confidence score from extraction (default: 1.0).
    embedding: Optional semantic embedding vector.
    canonical_ids: Optional mapping of authority names to external IDs.

Returns:
    Configured SimpleEntity instance with current timestamp.

### `def make_test_relationship(subject_id: str, object_id: str, predicate: str = 'related_to', confidence: float = 1.0) -> SimpleRelationship`

Factory function to create SimpleRelationship instances with defaults.

Provides a concise way to create relationships in tests. Both subject
and object entity IDs are required; predicate and confidence have defaults.

Args:
    subject_id: Entity ID of the relationship source.
    object_id: Entity ID of the relationship target.
    predicate: Relationship type name (default: "related_to").
    confidence: Confidence score for the relationship (default: 1.0).

Returns:
    Configured SimpleRelationship instance with current timestamp.


<span id="user-content-testsinitpy"></span>

# tests/__init__.py

Tests for the kgraph knowledge graph framework.


<span id="user-content-teststestcachingpy"></span>

# tests/test_caching.py

Tests for embedding caching components.

Tests in-memory and file-based caching, as well as the cached embedding generator.

## `class TestEmbeddingCacheConfig`

Test EmbeddingCacheConfig model.

### `def TestEmbeddingCacheConfig.test_default_config(self)`

Test default cache configuration.

### `def TestEmbeddingCacheConfig.test_custom_config(self)`

Test custom cache configuration.

### `def TestEmbeddingCacheConfig.test_config_immutability(self)`

Test that config is immutable (frozen=True).

## `class TestInMemoryEmbeddingsCache`

Test InMemoryEmbeddingsCache implementation.

## `class TestFileBasedEmbeddingsCache`

Test FileBasedEmbeddingsCache implementation.

## `class TestCachedEmbeddingGenerator`

Test CachedEmbeddingGenerator wrapper.

## `class TestCachingIntegration`

Integration tests for caching components.


<span id="user-content-teststestcanonicalidpy"></span>

# tests/test_canonical_id.py

Tests for canonical ID abstractions.

This module tests the core canonical ID abstractions:
- CanonicalId model
- CanonicalIdCacheInterface and JsonFileCanonicalIdCache
- CanonicalIdLookupInterface
- Helper functions in canonical_helpers

## `class TestCanonicalId`

Tests for the CanonicalId model.

### `def TestCanonicalId.test_canonical_id_creation(self)`

CanonicalId can be created with id, url, and synonyms.

### `def TestCanonicalId.test_canonical_id_minimal(self)`

CanonicalId can be created with just an id.

### `def TestCanonicalId.test_canonical_id_frozen(self)`

CanonicalId is frozen (immutable).

### `def TestCanonicalId.test_canonical_id_str_representation(self)`

CanonicalId string representation returns the id.

## `class TestJsonFileCanonicalIdCache`

Tests for JsonFileCanonicalIdCache implementation.

### `def TestJsonFileCanonicalIdCache.test_cache_creation(self)`

Cache can be created with a file path.

### `def TestJsonFileCanonicalIdCache.test_store_and_fetch(self)`

Can store and fetch CanonicalId objects.

### `def TestJsonFileCanonicalIdCache.test_fetch_miss_returns_none(self)`

Fetching non-existent entry returns None.

### `def TestJsonFileCanonicalIdCache.test_mark_known_bad(self)`

Can mark terms as known bad and check them.

### `def TestJsonFileCanonicalIdCache.test_cache_persistence(self)`

Cache persists to disk and can be reloaded.

### `def TestJsonFileCanonicalIdCache.test_cache_metrics(self)`

Cache tracks metrics correctly.

### `def TestJsonFileCanonicalIdCache.test_cache_migration_from_old_format(self)`

Cache can migrate from old format (dict[str, str]).

### `def TestJsonFileCanonicalIdCache.test_cache_normalizes_keys(self)`

Cache normalizes keys (case-insensitive, strips whitespace).

## `class TestCanonicalHelpers`

Tests for canonical ID helper functions.

### `def TestCanonicalHelpers.test_extract_canonical_id_from_entity_with_priority(self)`

extract_canonical_id_from_entity respects priority order.

### `def TestCanonicalHelpers.test_extract_canonical_id_from_entity_no_priority(self)`

extract_canonical_id_from_entity returns first available if no priority.

### `def TestCanonicalHelpers.test_extract_canonical_id_from_entity_no_canonical_ids(self)`

extract_canonical_id_from_entity returns None if no canonical_ids.

### `def TestCanonicalHelpers.test_check_entity_id_format_prefix_match(self)`

check_entity_id_format matches prefix patterns.

### `def TestCanonicalHelpers.test_check_entity_id_format_umls_pattern(self)`

check_entity_id_format matches UMLS pattern (C + digits).

### `def TestCanonicalHelpers.test_check_entity_id_format_numeric_pattern(self)`

check_entity_id_format handles numeric patterns (HGNC, RxNorm).

### `def TestCanonicalHelpers.test_check_entity_id_format_uniprot_pattern(self)`

check_entity_id_format matches UniProt pattern (P/Q + alphanumeric).

### `def TestCanonicalHelpers.test_check_entity_id_format_no_match(self)`

check_entity_id_format returns None if no pattern matches.

### `def TestCanonicalHelpers.test_check_entity_id_format_wrong_entity_type(self)`

check_entity_id_format returns None for wrong entity type.


<span id="user-content-teststestcontextandbuilderspy"></span>

# tests/test_context_and_builders.py



<span id="user-content-teststestentitiespy"></span>

# tests/test_entities.py

Tests for entity creation, status management, and in-memory storage operations.

This module verifies:
- Entity instantiation with provisional vs canonical status
- Entity attributes: synonyms, embeddings, canonical IDs from authority sources
- Entity immutability (frozen Pydantic models)
- InMemoryEntityStorage CRUD operations (add, get, update, delete)
- Batch retrieval and counting
- Name-based lookups (case-insensitive, synonym-aware)
- Embedding-based similarity search with configurable thresholds

## `class TestEntityCreation`

Tests for creating entities with different statuses and attributes.

Verifies that entities can be created with provisional or canonical status,
store multiple canonical IDs from different authority sources (e.g., UMLS,
Wikidata), maintain synonyms for alternative names, hold semantic embeddings
for similarity comparisons, and enforce immutability via frozen Pydantic models.

### `def TestEntityCreation.test_create_provisional_entity(self) -> None`

Provisional entities are created with PROVISIONAL status and empty canonical IDs.

Provisional entities represent mentions that have been extracted from documents
but not yet validated against authoritative sources. They start with no
canonical IDs and can later be promoted to canonical status.

### `def TestEntityCreation.test_create_canonical_entity(self) -> None`

Canonical entities store validated identifiers from multiple authority sources.

Canonical entities have been validated and assigned stable IDs from
authoritative sources (e.g., test_authority, wikidata). Multiple canonical
IDs allow cross-referencing across different knowledge bases.

### `def TestEntityCreation.test_entity_with_synonyms(self) -> None`

Entities store alternative names as synonyms for improved matching.

Synonyms enable entity resolution to match different textual mentions
(e.g., "ASA" and "acetylsalicylic acid") to the same canonical entity.

### `def TestEntityCreation.test_entity_with_embedding(self) -> None`

Entities store semantic vector embeddings for similarity-based operations.

Embeddings enable semantic similarity comparisons between entities,
used for detecting potential duplicates during merge candidate detection
and for semantic search queries.

### `def TestEntityCreation.test_entity_immutability(self) -> None`

Entities are immutable (frozen Pydantic models) to ensure data integrity.

Immutability prevents accidental in-place modifications. To modify an
entity, use model_copy(update={...}) to create a new instance, then
persist the change through the storage layer.

## `class TestEntityStorage`

Tests for InMemoryEntityStorage CRUD operations and query capabilities.

Verifies add/get/update/delete operations, batch retrieval, name-based
lookups (case-insensitive with synonym support), embedding-based similarity
search with configurable thresholds, and entity counting.


<span id="user-content-teststestevidencesemanticpy"></span>

# tests/test_evidence_semantic.py

Tests for semantic evidence validation (_evidence_contains_both_entities_semantic).

## `class _MockEntity(BaseEntity)`

Minimal entity for testing.

### `def mock_embedding_generator()`

Mock embedding generator: same text -> same vector; different texts -> orthogonal.

### `def entity_with_embedding()`

Entity with pre-set embedding (so we don't need to generate).

### `def entity_without_embedding()`

Entity without embedding (will be generated via cache).


<span id="user-content-teststestevidencetraceabilitypy"></span>

# tests/test_evidence_traceability.py

Tests for evidence traceability.

### `def test_evidence_with_ids_validates()`

Test that an Evidence entity with paper_id and text_span_id validates.

### `def test_evidence_without_paper_id_fails()`

Test that an Evidence entity without a paper_id fails.

### `def test_evidence_canonical_id_format()`

Test that an Evidence entity with a canonical ID format validates.

### `def test_conceptual_navigation()`

Test the conceptual navigation from Relationship to Paper.

### `def test_textspan_is_canonical_only()`

Test that TextSpan entities are always canonical (not promotable).

### `def test_textspan_cannot_be_provisional()`

Test that TextSpan cannot be created with provisional status.

### `def test_textspan_requires_offsets()`

Test that TextSpan requires start_offset and end_offset.

### `def test_textspan_validates_offset_order()`

Test that end_offset must be greater than start_offset.

### `def test_textspan_valid_creation()`

Test that a valid TextSpan can be created.


<span id="user-content-teststestexportpy"></span>

# tests/test_export.py

Tests for exporting entities and documents to JSON files.

This module verifies:
- Entity export: Writing canonical (and optionally provisional) entities to
  a global entities.json file with all fields properly serialized
- Document export: Writing per-document JSON files containing relationships
  and provisional entities extracted from that document
- Full export: Generating both the global entities.json and per-document files
- Storage list_all methods: Pagination, status filtering, and document-based queries

### `def orchestrator(tmp_path: Path) -> IngestionOrchestrator`

Create an orchestrator for export tests.

## `class TestExportEntities`

Tests for exporting entities to a JSON file.

By default, only canonical entities are exported to the global entities.json.
Provisional entities can be included via the include_provisional flag.

## `class TestExportDocument`

Tests for exporting per-document JSON files (paper_{doc_id}.json).

Each document export includes relationships sourced from that document
and provisional entities that originated from it.

## `class TestExportAll`

Tests for full export: global entities.json plus per-document files.

The export_all method generates a complete export of the knowledge graph:
entities.json with all canonical entities, and paper_{doc_id}.json for each
ingested document.

## `class TestListAllMethods`

Tests for storage list_all methods used by export functionality.

These methods support pagination (limit/offset), status filtering for entities,
and document-based queries for relationships.


<span id="user-content-teststestgithashpy"></span>

# tests/test_git_hash.py

Tests for git hash utility in export module.

Tests the get_git_hash() function for version tracking in bundles.

## `class TestGetGitHash`

Test the get_git_hash() function.

### `def TestGetGitHash.test_returns_string_in_git_repo(self)`

Should return a string when in a git repository.

### `def TestGetGitHash.test_returns_short_hash_format(self)`

Hash should be short format (7+ characters, alphanumeric).

### `def TestGetGitHash.test_returns_none_when_git_unavailable(self)`

Should return None when git command fails.

### `def TestGetGitHash.test_returns_none_when_not_in_repo(self)`

Should return None when not in a git repository.

### `def TestGetGitHash.test_returns_none_on_timeout(self)`

Should return None when git command times out.

### `def TestGetGitHash.test_strips_whitespace_from_output(self)`

Should strip whitespace from git output.

### `def TestGetGitHash.test_uses_correct_git_command(self)`

Should call git rev-parse --short HEAD.


<span id="user-content-teststestingestionpy"></span>

# tests/test_ingestion.py

Tests for the two-pass document ingestion pipeline.

This module verifies the IngestionOrchestrator's ability to:
- Pass 1: Parse documents, extract entities, assign embeddings, resolve to
  existing entities or create new provisionals
- Pass 2: Extract relationships between entities from document content
- Store parsed documents, entities, and relationships in their respective storages
- Handle batch ingestion of multiple documents
- Validate entities and relationships against domain-specific schemas
- Detect merge candidates among canonical entities via embedding similarity

### `def orchestrator(test_domain: SimpleDomainSchema, entity_storage: InMemoryEntityStorage, relationship_storage: InMemoryRelationshipStorage, document_storage: InMemoryDocumentStorage, document_parser: MockDocumentParser, entity_extractor: MockEntityExtractor, entity_resolver: MockEntityResolver, relationship_extractor: MockRelationshipExtractor, embedding_generator: MockEmbeddingGenerator) -> IngestionOrchestrator`

Create an ingestion orchestrator with mock components.

## `class TestSingleDocumentIngestion`

Tests for ingesting a single document through the two-pass pipeline.

Verifies that ingestion extracts entities, creates relationships, stores
the parsed document, generates embeddings for new entities, and increments
usage counts when the same entity is mentioned across multiple documents.

## `class TestBatchIngestion`

Tests for ingesting multiple documents in a single batch operation.

Verifies that batch ingestion processes multiple documents, aggregates
statistics across all documents, and continues processing remaining
documents even if individual documents encounter errors.

## `class TestDomainValidation`

Tests for validating entities and relationships against domain schemas.

Each knowledge domain defines valid entity types and relationship predicates.
These tests verify that the orchestrator validates extracted data against
the configured domain schema.

## `class TestMergeCandidateDetection`

Tests for detecting potential duplicate entities via embedding similarity.

Merge candidate detection identifies canonical entities with high embedding
similarity (cosine) that may represent the same real-world concept and
should be merged. Only canonical entities with embeddings are considered.


<span id="user-content-teststestloggingpy"></span>

# tests/test_logging.py

Tests for the PprintLogger and setup_logging functionality.

This module verifies:
- PprintLogger wraps standard logging.Logger correctly
- pprint parameter defaults to True and formats complex objects
- pprint=False uses simple string conversion
- All log levels support pprint formatting
- Delegation to underlying logger methods works
- Simple strings work with both pprint options
- Pydantic models use model_dump_json() when pprint=True

## `class TestPprintLogger`

Tests for PprintLogger formatting and delegation.

### `def TestPprintLogger.test_pprint_formats_dict(self) -> None`

Test that pprint=True formats dictionaries nicely.

### `def TestPprintLogger.test_pprint_false_uses_str(self) -> None`

Test that pprint=False uses simple string conversion.

### `def TestPprintLogger.test_pprint_defaults_to_true(self) -> None`

Test that pprint parameter defaults to True.

### `def TestPprintLogger.test_simple_string_with_pprint(self) -> None`

Test that simple strings work with pprint=True.

### `def TestPprintLogger.test_all_log_levels_support_pprint(self) -> None`

Test that all log levels (debug, info, warning, error, critical) support pprint.

### `def TestPprintLogger.test_exception_logging(self) -> None`

Test that exception logging works with pprint.

### `def TestPprintLogger.test_delegates_to_underlying_logger(self) -> None`

Test that PprintLogger delegates other methods to underlying logger.

### `def TestPprintLogger.test_nested_structures_formatted(self) -> None`

Test that deeply nested structures are formatted correctly.

### `def TestPprintLogger.test_list_formatting(self) -> None`

Test that lists are formatted nicely with pprint.

### `def TestPprintLogger.test_pydantic_model_uses_model_dump_json(self) -> None`

Test that Pydantic models use model_dump_json() when pprint=True.

## `class TestModel(BaseModel)`

**Fields:**

```python
name: str
age: int
nested: dict[str, str]
```


### `def TestPprintLogger.test_pydantic_model_with_pprint_false(self) -> None`

Test that Pydantic models use str() when pprint=False.

## `class TestModel(BaseModel)`

**Fields:**

```python
name: str
```


## `class TestSetupLogging`

Tests for setup_logging function.

### `def TestSetupLogging.test_setup_logging_returns_pprint_logger(self) -> None`

Test that setup_logging returns a PprintLogger instance.

### `def TestSetupLogging.test_setup_logging_configures_handler(self) -> None`

Test that setup_logging properly configures handlers and formatters.

### `def TestSetupLogging.test_setup_logging_uses_caller_name(self) -> None`

Test that setup_logging uses the calling function's name as logger name.

### `def TestSetupLogging.test_setup_logging_does_not_duplicate_handlers(self) -> None`

Test that setup_logging doesn't add duplicate handlers on multiple calls.


<span id="user-content-teststestmedlitdomainpy"></span>

# tests/test_medlit_domain.py

Tests for the MedlitDomain.

### `def test_medlit_domain_instantiates()`

Test that MedlitDomain can be instantiated.

### `def test_medlit_domain_entity_types()`

Test that all entity types are registered.

### `def test_medlit_domain_relationship_types()`

Test that all relationship types are registered.


<span id="user-content-teststestmedlitentitiespy"></span>

# tests/test_medlit_entities.py

Tests for medlit entity validation.

### `def test_disease_with_umls_id_validates()`

Test that a Disease entity with a UMLS ID validates.

### `def test_gene_with_hgnc_id_validates()`

Test that a Gene entity with an HGNC ID validates.

### `def test_drug_with_rxnorm_id_validates()`

Test that a Drug entity with an RxNorm ID validates.

### `def test_protein_with_uniprot_id_validates()`

Test that a Protein entity with a UniProt ID validates.

### `def test_procedure_validates()`

Test that a Procedure entity validates.

### `def test_institution_validates()`

Test that an Institution entity validates.

### `def test_provisional_entity_validates()`

Test that a provisional entity (no ontology ID) validates.

### `def test_canonical_entity_without_ontology_id_fails()`

Test that a canonical entity without an ontology ID fails.

### `def test_evidence_cannot_be_provisional()`

Test that Evidence entities cannot be created with PROVISIONAL status.


<span id="user-content-teststestmedlitrelationshipspy"></span>

# tests/test_medlit_relationships.py

Tests for medlit relationship validation.

### `def test_treats_with_evidence_validates()`

Test that a Treats relationship with evidence validates.

### `def test_treats_without_evidence_fails()`

Test that a Treats relationship without evidence fails.

### `def test_bibliographic_relationship_without_evidence_validates()`

Test that a bibliographic relationship without evidence validates.

### `def test_associated_with_with_evidence_validates()`

Test that an AssociatedWith relationship with evidence validates.

### `def test_part_of_without_evidence_validates()`

Test that a PartOf relationship without evidence validates.


<span id="user-content-teststestpapermodelpy"></span>

# tests/test_paper_model.py

Tests for the Paper model.

### `def test_paper_with_full_metadata_validates()`

Test that a Paper with full metadata validates.

### `def test_papermetadata_with_study_type_validates()`

Test that PaperMetadata with a study_type validates.

### `def test_extractionprovenance_serializes_correctly()`

Test that ExtractionProvenance serializes correctly.


<span id="user-content-teststestpipelineintegrationpy"></span>

# tests/test_pipeline_integration.py

Integration test for the full kgraph ingestion pipeline.

This test verifies the complete end-to-end flow:
1. Batch document ingestion
2. Provisional entity creation
3. Entity promotion based on usage thresholds
4. Merge candidate detection via embedding similarity

### `def orchestrator(test_domain: SimpleDomainSchema, entity_storage: InMemoryEntityStorage, relationship_storage: InMemoryRelationshipStorage, document_storage: InMemoryDocumentStorage, document_parser: MockDocumentParser, entity_extractor: MockEntityExtractor, entity_resolver: MockEntityResolver, relationship_extractor: MockRelationshipExtractor, embedding_generator: MockEmbeddingGenerator) -> IngestionOrchestrator`

Create an ingestion orchestrator with mock components.

## `class TestFullPipelineIntegration`

End-to-end integration tests for the complete ingestion pipeline.

These tests verify that batch ingestion, promotion, and merge candidate
detection work together correctly in a realistic workflow.


<span id="user-content-teststestpmcchunkerpy"></span>

# tests/test_pmc_chunker.py

Tests for PMCStreamingChunker.

### `def chunker() -> PMCStreamingChunker`

PMC chunker with small window for tests.

### `def test_document_id_from_source_uri() -> None`

document_id_from_source_uri returns stem of path.


<span id="user-content-teststestpmcstreamingpy"></span>

# tests/test_pmc_streaming.py

Tests for streaming PMC XML chunker.

### `def test_iter_pmc_sections_yields_abstract_and_secs()`

iter_pmc_sections yields abstract first then body secs.

### `def test_iter_overlapping_windows_abstract_separately()`

Abstract is yielded as first window when include_abstract_separately True.

### `def test_iter_overlapping_windows_has_overlap()`

Consecutive windows overlap by roughly overlap chars.

### `def test_iter_pmc_windows_returns_iterator()`

iter_pmc_windows returns an iterator of (index, text).


<span id="user-content-teststestpromotionmergepy"></span>

# tests/test_promotion_merge.py

Tests for entity promotion (provisional to canonical) and merging duplicate entities.

This module verifies:
- Promotion: Changing provisional entities to canonical status when they meet
  usage count and confidence thresholds defined by the domain configuration
- Merge: Combining duplicate canonical entities, consolidating their synonyms,
  summing usage counts, and updating relationship references

### `def orchestrator(test_domain: SimpleDomainSchema, entity_storage: InMemoryEntityStorage, relationship_storage: InMemoryRelationshipStorage, document_storage: InMemoryDocumentStorage, document_parser: MockDocumentParser, entity_extractor: MockEntityExtractor, entity_resolver: MockEntityResolver, relationship_extractor: MockRelationshipExtractor, embedding_generator: MockEmbeddingGenerator) -> IngestionOrchestrator`

Create an ingestion orchestrator with mock components.

## `class TestEntityPromotion`

Tests for promoting provisional entities to canonical status.

Provisional entities become canonical when they meet domain-specific
thresholds for usage count and confidence. Promotion assigns a new
canonical ID and updates the storage reference.

## `class TestEntityMerging`

Tests for merging duplicate canonical entities into a single target entity.

Merging consolidates synonyms from source entities, sums usage counts,
removes source entities from storage, and updates all relationship
references from source IDs to the target ID.


<span id="user-content-teststestpromotionpy"></span>

# tests/test_promotion.py

Tests for entity promotion policy and workflow.

This test module covers:
1. PromotionPolicy base class behavior
2. Domain-specific promotion policies (Sherlock example)
3. Full promotion workflow with relationship updates
4. Entities starting as provisional with canonical_id_hint

## `class SimplePromotionPolicy(PromotionPolicy)`

Test implementation with hardcoded mappings.

## `class TestPromotionPolicyBase`

Test the base PromotionPolicy class behavior.

### `def TestPromotionPolicyBase.test_should_promote_rejects_already_canonical(self)`

should_promote returns False for entities already canonical.

### `def TestPromotionPolicyBase.test_should_promote_requires_min_usage(self)`

should_promote checks minimum usage count threshold.

### `def TestPromotionPolicyBase.test_should_promote_requires_min_confidence(self)`

should_promote checks minimum confidence threshold.

### `def TestPromotionPolicyBase.test_should_promote_checks_embedding_requirement(self)`

should_promote respects require_embedding config.

## `class TestSherlockPromotion`

Test Sherlock-specific promotion policy with DBPedia mappings.

### `def TestSherlockPromotion.test_sherlock_promotion_config_has_low_thresholds(self)`

Sherlock domain uses lower thresholds for small corpus.

### `def TestSherlockPromotion.test_get_promotion_policy_accepts_lookup_parameter(self)`

get_promotion_policy accepts lookup parameter for signature compliance.

## `class TestPromotionWorkflow`

Test complete promotion workflow with relationship updates.

## `class TestPromotionIntegration`

Test promotion in complete ingestion pipeline.


<span id="user-content-teststestrelationshipspy"></span>

# tests/test_relationships.py

Tests for relationship (edge) creation and in-memory storage operations.

This module verifies:
- Relationship instantiation with subject, predicate, and object
- Relationship attributes: metadata, source documents, immutability
- InMemoryRelationshipStorage CRUD operations (add, find, delete)
- Queries by subject, object, and triple (subject-predicate-object)
- Updating entity references when entities are merged

## `class TestRelationshipCreation`

Tests for creating relationship (edge) instances.

Relationships represent directed edges in the knowledge graph, connecting
a subject entity to an object entity via a predicate (edge type).

### `def TestRelationshipCreation.test_create_relationship(self) -> None`

Relationships have a subject_id, predicate, and object_id forming a directed edge.

### `def TestRelationshipCreation.test_relationship_with_metadata(self) -> None`

Relationships store domain-specific metadata (e.g., evidence_type, section).

### `def TestRelationshipCreation.test_relationship_with_source_documents(self) -> None`

Relationships track which documents they were extracted from via source_documents.

### `def TestRelationshipCreation.test_relationship_immutability(self) -> None`

Relationships are immutable (frozen Pydantic models) to ensure data integrity.

## `class TestRelationshipStorage`

Tests for InMemoryRelationshipStorage CRUD operations and queries.

Verifies add/find/delete operations, queries by subject or object entity,
optional predicate filtering, entity reference updates for merging, and counting.


<span id="user-content-teststestrelationshipswappy"></span>

# tests/test_relationship_swap.py

Tests for automatic subject/object swapping when LLM gets the order wrong.

## `class DrugEntity(BaseEntity)`

Drug entity for testing.

## `class DiseaseEntity(BaseEntity)`

Disease entity for testing.

## `class TreatsRelationship(BaseRelationship)`

Treats relationship for testing.

## `class TestDomainSchema(DomainSchema)`

Test domain schema with predicate constraints.

### `def TestDomainSchema.validate_entity(self, entity: BaseEntity) -> list[ValidationIssue]`

Validate entity is of a registered type.

### `def TestDomainSchema.get_promotion_policy(self, lookup = None)`

Not needed for these tests.

### `def domain()`

Create test domain schema.

### `def drug_entity()`

Create a drug entity.

### `def disease_entity()`

Create a disease entity.

### `def test_should_swap_detection()`

Test the swap detection logic in the medlit extractor.

### `def test_evidence_contains_both_entities_both_present()`

Evidence containing both subject and object is accepted.

### `def test_evidence_contains_both_entities_missing_subject()`

Evidence missing subject is rejected with evidence_missing_subject.

### `def test_evidence_contains_both_entities_empty_evidence()`

Empty evidence is rejected with evidence_empty.

### `def test_evidence_contains_both_entities_synonym_match()`

Entity synonym appearing in evidence counts as match.


<span id="user-content-teststeststreamingpy"></span>

# tests/test_streaming.py

Tests for streaming pipeline components.

Tests document chunking, streaming entity extraction, and windowed relationship
extraction capabilities.

### `def make_simple_document(document_id: str, content: str) -> SimpleDocument`

Helper to create SimpleDocument with required fields.

## `class TestDocumentChunk`

Test DocumentChunk model.

### `def TestDocumentChunk.test_chunk_creation(self)`

Test creating a document chunk.

### `def TestDocumentChunk.test_chunk_immutability(self)`

Test that chunks are immutable (frozen=True).

## `class TestChunkingConfig`

Test ChunkingConfig model.

### `def TestChunkingConfig.test_default_config(self)`

Test default chunking configuration.

### `def TestChunkingConfig.test_custom_config(self)`

Test custom chunking configuration.

### `def TestChunkingConfig.test_config_immutability(self)`

Test that config is immutable (frozen=True).

## `class TestWindowedDocumentChunker`

Test WindowedDocumentChunker implementation.

## `class TestBatchingEntityExtractor`

Test BatchingEntityExtractor implementation.

## `class TestWindowedRelationshipExtractor`

Test WindowedRelationshipExtractor implementation.

## `class TestIntegrationStreamingPipeline`

Integration tests for streaming pipeline.


<span id="user-content-vibesmd"></span>

# VIBES.md

*Design / conversation notes.*

# Generalizing literature graphs across knowledge domains

## Prompt:
1/18/2026, 10:16:42 AM

I want to reframe the medical literature project a bit, allow it to be generalized to other domains of knowledge. We are still building a graph and a graph still consists of nodes (entities) and edges (relationships). We still have a collection of entities from previous ingestion processes. We add a new thing: entities may be "canonical", that is they have been assigned canonical IDs (UMLS numbers or whatever) or they may be "provisional", meaning that we don't know yet if they should be assigned canonical IDs, for instance an entity might be a mention of some trivial thing in just one paper.

Given a batch of papers to ingest, we proceed in two passes. First pass we extract entities and assign canonical IDs where they make sense. Second pass we identify the edges (for medical, these edges are of the three types, extraction, claims, and evidence). The first pass produces a JSON serialization of the collection of entities including canonical IDs and synonyms. The second pass produces one JSON file per paper, including the paper's edges and any provisional entities unique to that paper.

This framework allows each knowledge domain (legal documents, academic CS papers) to define its own source of canonical IDs, its own schema, its own list of edge types. Any interesting query optimizations (graph theory tricks, database quirks) can be shared across domains. Where possible, cross-domain canonical IDs are preferred (such as DBPedia URIs). Including a significant chunk of DBPedia is probably a very good idea, or at least being able to pull in DBPedia entities as the ingestion progresses.

Back in the 1980s, Doug Lenat tried to do something like this with the Cyc project, but there were no LLMs then, only grad students and postdocs, so that's what he used. They never got enough scale for the project to gain much traction. Perhaps now his idea is more feasible.

What to do about entity merging? If two canonical entities turn out to be the same thing, that's a messier operation than promoting a provisional entity. Maybe track usage counts to decide when to promote, and maintain confidence scores on canonical IDs to decide when to merge. Each entity has a semantic vector, and hopefully potential merges can be flagged by near matches of cosine distance.



## Response:
1/18/2026, 10:17:20 AM

    ...


